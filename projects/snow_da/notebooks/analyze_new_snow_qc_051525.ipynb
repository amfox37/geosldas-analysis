{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import struct\n",
    "import os\n",
    "import glob\n",
    "import xarray as xr\n",
    "from datetime import datetime, timedelta\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from mapper_functions import plot_global_tight_pcm, plot_NA_tight_pcm, plot_region\n",
    "\n",
    "from helper import read_ObsFcstAna, read_tilecoord, get_tile_species_obs_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_name_1 = 'LS_DAv8_M36'\n",
    "expt_name_2 = 'LS_DAv8_M36_t10'\n",
    "\n",
    "start_date = datetime(2002, 10, 1)\n",
    "end_date = datetime(2006, 10, 1)\n",
    "\n",
    "start_date_str = start_date.strftime('%Y/%m/%d')\n",
    "end_date_str = end_date.strftime('%Y/%m/%d')\n",
    "\n",
    "ana_directory_1 = f'/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_DAv8_M36/{expt_name_1}/output/SMAP_EASEv2_M36_GLOBAL/ana/ens_avg'\n",
    "\n",
    "ana_directory_2 = f'/discover/nobackup/projects/land_da/snow_qc_expts/LS_DAv8_M36_t10/{expt_name_2}/output/SMAP_EASEv2_M36_GLOBAL/ana/ens_avg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Process each month (1-12)\n",
    "for month in range(1, 13):\n",
    "\n",
    "    # Generate the month name only\n",
    "    month_name = datetime(2000, month, 1).strftime(\"%B\")  # 2000 is arbitrary, we only want the month name\n",
    "\n",
    "    # Initialize list for expt_1\n",
    "    OFA_list = []\n",
    "    \n",
    "    # Find all directories for this month within date range\n",
    "    month_pattern = os.path.join(ana_directory_1, 'Y*', f'M{month:02d}')\n",
    "    month_dirs = sorted(glob.glob(month_pattern))\n",
    "    \n",
    "    # Process each directory if within date range\n",
    "    for month_dir in month_dirs:\n",
    "        # Extract year from path more explicitly\n",
    "        try:\n",
    "            year = int(month_dir.split('/Y')[-1][:4])\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Skipping directory with unexpected format: {month_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if directory is within date range\n",
    "        if start_date <= datetime(year, month, 1) <= end_date:\n",
    "            # Load all .bin files in directory\n",
    "            OFA_list.extend(read_ObsFcstAna(fname) for fname in sorted(glob.glob(os.path.join(month_dir, '*.ldas_ObsFcstAna.*.bin'))))\n",
    "\n",
    "    # Initialize lists to store filtered data\n",
    "    all_tilenums = []\n",
    "    all_species = []\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "    all_obs = []\n",
    "\n",
    "    # Process each element in OFA_list\n",
    "    for ofa in OFA_list:\n",
    "        # Create mask for this element\n",
    "        assim_mask = ofa['obs_assim'] == 1\n",
    "        \n",
    "        # Append filtered data\n",
    "        all_tilenums.append(ofa['obs_tilenum'][assim_mask])\n",
    "        all_species.append(ofa['obs_species'][assim_mask])\n",
    "        all_lats.append(ofa['obs_lat'][assim_mask])\n",
    "        all_lons.append(ofa['obs_lon'][assim_mask])\n",
    "        all_obs.append(ofa['obs_obs'][assim_mask])\n",
    "\n",
    "    # Create filtered dictionary with concatenated data\n",
    "    filtered_data = {\n",
    "        'obs_tilenum': np.concatenate(all_tilenums),\n",
    "        'obs_species': np.concatenate(all_species),\n",
    "        'obs_lat': np.concatenate(all_lats),\n",
    "        'obs_lon': np.concatenate(all_lons),\n",
    "        'obs_obs': np.concatenate(all_obs)\n",
    "    }\n",
    "\n",
    "    # Process filtered data\n",
    "    stats_1 = get_tile_species_obs_values(filtered_data)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Number of unique tiles: {len(stats_1['tiles'])}\")\n",
    "    print(f\"Number of species: {len(stats_1['max_values'])}\")\n",
    "    for species, max_vals in stats_1['max_values'].items():\n",
    "        print(f\"Species {species}: max value = {np.max(max_vals)}\")\n",
    "\n",
    "    # Initialize map_array with NaN values\n",
    "    map_array = np.empty((stats_1['lon'].shape[0], 3))  # Shape: (number of tiles, 3)\n",
    "    map_array.fill(np.nan)\n",
    "\n",
    "    # Fill longitude and latitude columns\n",
    "    map_array[:, 1] = stats_1['lon']  # Assuming `lon` contains longitude values\n",
    "    map_array[:, 2] = stats_1['lat']  # Assuming `lat` contains latitude values\n",
    "\n",
    "    map_array[:, 0] = stats_1['max_values'][12]\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_1} {start_date_str} - {end_date_str}:\\n Max MYD10C1 obs: {month_name}\", 'fraction', 0, 1)\n",
    "\n",
    "    map_array[:, 0] = stats_1['max_values'][13]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_1} {start_date_str} - {end_date_str}:\\n Max MOD10C1 obs {month_name}\", 'fraction', 0, 1)\n",
    "\n",
    "    map_array[:, 0] = stats_1['num_obs'][12]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_1} {start_date_str} - {end_date_str}:\\n MYD10C1 N obs: {month_name}\", '-', 0, 120)\n",
    "\n",
    "    map_array[:, 0] = stats_1['num_obs'][13]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_1} {start_date_str} - {end_date_str}:\\n MOD10C1 N obs {month_name}\", '-', 0, 120)\n",
    "\n",
    "    map_array[:, 0] = stats_1['num_obs_gt_0.9'][12]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_1} {start_date_str} - {end_date_str}:\\n MYD10C1 N obs > 0.9: {month_name}\", '-', 0, 40)\n",
    "\n",
    "    map_array[:, 0] = stats_1['num_obs_gt_0.9'][13]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_1} {start_date_str} - {end_date_str}:\\n MOD10C1 N obs > 0.9 {month_name}\", '-', 0, 40)\n",
    "\n",
    "#############################################################################\n",
    "# Initialize list for expt_2\n",
    "    OFA_list = []\n",
    "    \n",
    "    # Find all directories for this month within date range\n",
    "    month_pattern = os.path.join(ana_directory_2, 'Y*', f'M{month:02d}')\n",
    "    month_dirs = sorted(glob.glob(month_pattern))\n",
    "    \n",
    "    # Process each directory if within date range\n",
    "    for month_dir in month_dirs:\n",
    "        # Extract year from path more explicitly\n",
    "        try:\n",
    "            year = int(month_dir.split('/Y')[-1][:4])\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Skipping directory with unexpected format: {month_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if directory is within date range\n",
    "        if start_date <= datetime(year, month, 1) <= end_date:\n",
    "            # Load all .bin files in directory\n",
    "            OFA_list.extend(read_ObsFcstAna(fname) for fname in sorted(glob.glob(os.path.join(month_dir, '*.ldas_ObsFcstAna.*.bin'))))\n",
    "\n",
    "    # Initialize lists to store filtered data\n",
    "    all_tilenums = []\n",
    "    all_species = []\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "    all_obs = []\n",
    "\n",
    "    # Process each element in OFA_list\n",
    "    for ofa in OFA_list:\n",
    "        # Create mask for this element\n",
    "        assim_mask = ofa['obs_assim'] == 1\n",
    "        \n",
    "        # Append filtered data\n",
    "        all_tilenums.append(ofa['obs_tilenum'][assim_mask])\n",
    "        all_species.append(ofa['obs_species'][assim_mask])\n",
    "        all_lats.append(ofa['obs_lat'][assim_mask])\n",
    "        all_lons.append(ofa['obs_lon'][assim_mask])\n",
    "        all_obs.append(ofa['obs_obs'][assim_mask])\n",
    "\n",
    "    # Create filtered dictionary with concatenated data\n",
    "    filtered_data = {\n",
    "        'obs_tilenum': np.concatenate(all_tilenums),\n",
    "        'obs_species': np.concatenate(all_species),\n",
    "        'obs_lat': np.concatenate(all_lats),\n",
    "        'obs_lon': np.concatenate(all_lons),\n",
    "        'obs_obs': np.concatenate(all_obs)\n",
    "    }\n",
    "\n",
    "    # Process filtered data\n",
    "    stats_2 = get_tile_species_obs_values(filtered_data)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Number of unique tiles: {len(stats_2['tiles'])}\")\n",
    "    print(f\"Number of species: {len(stats_2['max_values'])}\")\n",
    "    for species, max_vals in stats_2['max_values'].items():\n",
    "        print(f\"Species {species}: max value = {np.max(max_vals)}\")\n",
    "\n",
    "    # Initialize map_array with NaN values\n",
    "    map_array = np.empty((stats_2['lon'].shape[0], 3))  # Shape: (number of tiles, 3)\n",
    "    map_array.fill(np.nan)\n",
    "\n",
    "    # Fill longitude and latitude columns\n",
    "    map_array[:, 1] = stats_2['lon']  # Assuming `lon` contains longitude values\n",
    "    map_array[:, 2] = stats_2['lat']  # Assuming `lat` contains latitude values\n",
    "\n",
    "    map_array[:, 0] = stats_2['max_values'][12]\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n Max MYD10C1 obs: {month_name}\", 'fraction', 0, 1)\n",
    "\n",
    "    map_array[:, 0] = stats_2['max_values'][13]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n Max MOD10C1 obs {month_name}\", 'fraction', 0, 1)\n",
    "\n",
    "    map_array[:, 0] = stats_2['num_obs'][12]\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n MYD10C1 N obs: {month_name}\", '-', 0, 120)\n",
    "\n",
    "    map_array[:, 0] = stats_2['num_obs'][13]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n MOD10C1 N obs {month_name}\", '-', 0, 120)\n",
    "\n",
    "    map_array[:, 0] = stats_2['num_obs_gt_0.9'][12]\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n MYD10C1 N obs > 0.9: {month_name}\", '-', 0, 40)\n",
    "\n",
    "    map_array[:, 0] = stats_2['num_obs_gt_0.9'][13]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n MOD10C1 N obs > 0.9 {month_name}\", '-', 0, 40)\n",
    "\n",
    "\n",
    "    # Find matching tiles between stats_1 and stats_2\n",
    "    # matching_tiles = set(stats_1['tiles']).intersection(set(stats_2['tiles']))\n",
    "\n",
    "    # # Initialize arrays to store differences\n",
    "    # diff_num_obs_aq = []\n",
    "    # diff_num_obs_te = []\n",
    "    # diff_max_values_aq = []\n",
    "    # diff_max_values_te = []\n",
    "    # diff_num_obs_aq_gt_0_9 = []\n",
    "    # diff_num_obs_te_gt_0_9 = []\n",
    "    # matching_lats = []\n",
    "    # matching_lons = []    \n",
    "\n",
    "    # # Calculate differences for matching tiles\n",
    "    # for tile in matching_tiles:\n",
    "    #     index_1 = np.where(stats_1['tiles'] == tile)[0][0]\n",
    "    #     index_2 = np.where(stats_2['tiles'] == tile)[0][0]\n",
    "    #     diff_num_obs_aq.append(stats_2['num_obs'][12][index_2] - stats_1['num_obs'][12][index_1])\n",
    "    #     diff_num_obs_te.append(stats_2['num_obs'][13][index_2] - stats_1['num_obs'][13][index_1])\n",
    "    #     diff_max_values_aq.append(stats_2['max_values'][12][index_2] - stats_1['max_values'][12][index_1])\n",
    "    #     diff_max_values_te.append(stats_2['max_values'][13][index_2] - stats_1['max_values'][13][index_1])\n",
    "    #     diff_num_obs_aq_gt_0_9.append(stats_2['num_obs_gt_0.9'][12][index_2] - stats_1['num_obs_gt_0.9'][12][index_1])\n",
    "    #     diff_num_obs_te_gt_0_9.append(stats_2['num_obs_gt_0.9'][13][index_2] - stats_1['num_obs_gt_0.9'][13][index_1])\n",
    "    #     # Append lat/lon for matching tiles\n",
    "    #     matching_lats.append(stats_1['lat'][index_1])\n",
    "    #     matching_lons.append(stats_1['lon'][index_1])       \n",
    "\n",
    "    # Use all tiles from stats_1\n",
    "    base_tiles = stats_1['tiles']\n",
    "    \n",
    "    # Initialize arrays to store differences\n",
    "    diff_num_obs_aq = []\n",
    "    diff_num_obs_te = []\n",
    "    diff_max_values_aq = []\n",
    "    diff_max_values_te = []\n",
    "    diff_num_obs_aq_gt_0_9 = []\n",
    "    diff_num_obs_te_gt_0_9 = []\n",
    "    matching_lats = []\n",
    "    matching_lons = []    \n",
    "\n",
    "    # Calculate differences for all tiles in stats_1\n",
    "    for tile in base_tiles:\n",
    "        index_1 = np.where(stats_1['tiles'] == tile)[0][0]\n",
    "        \n",
    "        # Find corresponding index in stats_2 or use zero values\n",
    "        if tile in stats_2['tiles']:\n",
    "            index_2 = np.where(stats_2['tiles'] == tile)[0][0]\n",
    "            stats_2_num_obs_aq = stats_2['num_obs'][12][index_2]\n",
    "            stats_2_num_obs_te = stats_2['num_obs'][13][index_2]\n",
    "            stats_2_max_values_aq = stats_2['max_values'][12][index_2]\n",
    "            stats_2_max_values_te = stats_2['max_values'][13][index_2]\n",
    "            stats_2_num_obs_gt_0_9_aq = stats_2['num_obs_gt_0.9'][12][index_2]\n",
    "            stats_2_num_obs_gt_0_9_te = stats_2['num_obs_gt_0.9'][13][index_2]\n",
    "        else:\n",
    "            stats_2_num_obs_aq = 0.\n",
    "            stats_2_num_obs_te = 0.\n",
    "            stats_2_max_values_aq = 0.\n",
    "            stats_2_max_values_te = 0.\n",
    "            stats_2_num_obs_gt_0_9_aq = 0.\n",
    "            stats_2_num_obs_gt_0_9_te = 0.\n",
    "\n",
    "        if np.isnan(stats_1['num_obs'][12][index_1]):\n",
    "            print(f\"Found NaN in stats_1['num_obs'][12] at index {index_1}\")\n",
    "        if np.isnan(stats_2_num_obs_aq):\n",
    "            print(f\"Found NaN in stats_2_num_obs_aq for tile {tile}\")\n",
    "    \n",
    "        # Calculate differences\n",
    "        diff_num_obs_aq.append(stats_2_num_obs_aq - stats_1['num_obs'][12][index_1])\n",
    "        diff_num_obs_te.append(stats_2_num_obs_te - stats_1['num_obs'][13][index_1])\n",
    "        diff_max_values_aq.append(stats_2_max_values_aq - stats_1['max_values'][12][index_1])\n",
    "        diff_max_values_te.append(stats_2_max_values_te - stats_1['max_values'][13][index_1])\n",
    "        diff_num_obs_aq_gt_0_9.append(stats_2_num_obs_gt_0_9_aq - stats_1['num_obs_gt_0.9'][12][index_1])\n",
    "        diff_num_obs_te_gt_0_9.append(stats_2_num_obs_gt_0_9_te - stats_1['num_obs_gt_0.9'][13][index_1])\n",
    "\n",
    "        if np.isnan(stats_2_num_obs_aq - stats_1['num_obs'][12][index_1]):\n",
    "            print(\"Found nan\")\n",
    "        \n",
    "        # Append lat/lon for tiles\n",
    "        matching_lats.append(stats_1['lat'][index_1])\n",
    "        matching_lons.append(stats_1['lon'][index_1]) \n",
    "\n",
    "    # Initialize map_array with NaN values\n",
    "    map_array = np.empty((len(matching_lats), 3))  # Shape: (number of matching tiles, 3)\n",
    "    map_array.fill(np.nan)\n",
    "    # Fill longitude and latitude columns\n",
    "    map_array[:, 1] = np.array(matching_lons)  # Assuming `lon` contains longitude values\n",
    "    map_array[:, 2] = np.array(matching_lats)  # Assuming `lat` contains latitude values\n",
    "\n",
    "    map_array[:, 0] = np.array(diff_num_obs_aq)\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_2} - {expt_name_1} :\\n Difference MYD10C1 N obs: {month_name}\", '-', -120, 120)\n",
    "    map_array[:, 0] = np.array(diff_num_obs_te)\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} - {expt_name_1} :\\n MOD10C1 N obs diff {month_name}\", '-', -120, 120)\n",
    "\n",
    "    map_array[:, 0] = np.array(diff_max_values_aq)\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_2} - {expt_name_1} :\\n Difference MYD10C1 max obs: {month_name}\", 'fraction', -1., 1.)\n",
    "    map_array[:, 0] = np.array(diff_max_values_te)\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} - {expt_name_1} :\\n MOD10C1 max obs diff {month_name}\", 'fraction', -1., 1.)\n",
    "\n",
    "    map_array[:, 0] = np.array(diff_num_obs_aq_gt_0_9)\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_2} - {expt_name_1} :\\n Difference MYD10C1 N obs > 0.9: {month_name}\", '-', -50, 50)\n",
    "    map_array[:, 0] = np.array(diff_num_obs_te_gt_0_9)\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} - {expt_name_1} :\\n MOD10C1 N obs > 0.9 diff {month_name}\", '-', -50, 50)      \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_directory_1 = f'/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_DAv8_M36/{expt_name_1}/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg'\n",
    "\n",
    "cat_directory_2 = f'/discover/nobackup/projects/land_da/snow_qc_expts/LS_DAv8_M36_t10/{expt_name_2}/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg'\n",
    "\n",
    "# Open the dataset for a specific file that contains lat/lon for M36 grid\n",
    "ds_latlon = xr.open_dataset('DAv7_M36.inst3_1d_lndfcstana_Nt.20150901.nc4')\n",
    "\n",
    "# Extract longitude and latitude variables\n",
    "lon = ds_latlon['lon']\n",
    "lat = ds_latlon['lat']\n",
    "\n",
    "# Determine the number of tiles based on the latitude array\n",
    "n_tile = len(lat)\n",
    "\n",
    "# Initialize an observation array with NaN values\n",
    "# The array has dimensions [n_tile, 3], where:\n",
    "# - Column 0 is reserved for future use\n",
    "# - Column 1 stores longitude values\n",
    "# - Column 2 stores latitude values\n",
    "map_array = np.empty([n_tile, 3])\n",
    "map_array.fill(np.nan)\n",
    "map_array[:, 1] = lon\n",
    "map_array[:, 2] = lat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end_date = datetime(2006, 9, 30)\n",
    "\n",
    "# Define the variables to be extracted\n",
    "variables = {\n",
    "'sm_surface': 'SFMC',\n",
    "#'sm_rootzone': 'RZMC',\n",
    "#'sm_profile': 'PRMC',\n",
    "#'precipitation_total_surface_flux': 'PRECTOTCORRLAND',\n",
    "#'vegetation_greenness_fraction': 'GRN',\n",
    "#'leaf_area_index': 'LAI',\n",
    "'snow_mass': 'SNOMASLAND',\n",
    "#'surface_temperature_of_land_incl_snow': 'TSURFLAND',\n",
    "'soil_temperature_layer_1': 'TSOIL1',\n",
    "#'snowfall_land': 'PRECSNOCORRLAND',\n",
    "'snow_depth_within_snow_covered_area_fraction_on_land': 'SNODPLAND',\n",
    "'snowpack_evaporation_latent_heat_flux_on_land': 'LHLANDSBLN',\n",
    "#'overland_runoff_including_throughflow': 'RUNSURFLAND',\n",
    "#'baseflow_flux_land': 'BASEFLOWLAND',\n",
    "'snowmelt_flux_land': 'SMLAND',\n",
    "#'total_evaporation_land': 'EVLAND',\n",
    "#'net_shortwave_flux_land': 'SWLAND',\n",
    "#'total_water_storage_land': 'TWLAND',\n",
    "'fractional_area_of_snow_on_land': 'FRLANDSNO'  # New variable added\n",
    "}\n",
    "\n",
    "# Process each month (1-12)\n",
    "for month in range(1, 13):\n",
    "\n",
    "    # Generate the month name only\n",
    "    month_name = datetime(2000, month, 1).strftime(\"%B\")  # 2000 is arbitrary, we only want the month name\n",
    "\n",
    "    # Calculate number of seconds in the month\n",
    "    num_seconds_in_month = (datetime(2000, month, 1) + relativedelta(months=1) - datetime(2000, month, 1)).total_seconds()    \n",
    "    \n",
    "    # Find all directories for this month within date range\n",
    "    month_pattern = os.path.join(cat_directory_1, 'Y*', f'M{month:02d}')\n",
    "    month_dirs = sorted(glob.glob(month_pattern))\n",
    "    \n",
    "    # Process each directory if within date range\n",
    "    for month_dir in month_dirs:\n",
    "        # Extract year from path more explicitly\n",
    "        try:\n",
    "            year = int(month_dir.split('/Y')[-1][:4])\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Skipping directory with unexpected format: {month_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if directory is within date range\n",
    "        if start_date <= datetime(year, month, 1) <= end_date:\n",
    "            # Find the files\n",
    "            files = glob.glob(os.path.join(month_dir,\"*tavg24_1d_lnd_Nt*.nc4\"))\n",
    "\n",
    "            # Load the data\n",
    "            data = xr.open_mfdataset(files, combine='nested', concat_dim=\"time\")\n",
    "\n",
    "            # Extract the variable\n",
    "            data_mean_1 = data[list(variables.values())].mean(dim='time')\n",
    "            data_max_1 = data[list(variables.values())].max(dim='time')\n",
    "\n",
    "#########################################################################\n",
    "    # Find all directories for this month within date range\n",
    "    month_pattern = os.path.join(cat_directory_2, 'Y*', f'M{month:02d}')\n",
    "    month_dirs = sorted(glob.glob(month_pattern))\n",
    "    \n",
    "    # Process each directory if within date range\n",
    "    for month_dir in month_dirs:\n",
    "        # Extract year from path more explicitly\n",
    "        try:\n",
    "            year = int(month_dir.split('/Y')[-1][:4])\n",
    "        except ValueError:\n",
    "            print(f\"Warning: Skipping directory with unexpected format: {month_dir}\")\n",
    "            continue\n",
    "        \n",
    "        # Check if directory is within date range\n",
    "        if start_date <= datetime(year, month, 1) <= end_date:\n",
    "            # Find the files\n",
    "            files = glob.glob(os.path.join(month_dir,\"*tavg24_1d_lnd_Nt*.nc4\"))\n",
    "\n",
    "            # Load the data\n",
    "            data = xr.open_mfdataset(files, combine='nested', concat_dim=\"time\")\n",
    "\n",
    "            # Extract the variable\n",
    "            data_mean_2 = data[list(variables.values())].mean(dim='time')\n",
    "            data_max_2 = data[list(variables.values())].max(dim='time')\n",
    "\n",
    "# Plot max snowcover and snowmass\n",
    "\n",
    "#    map_array[:, 0] = data_max_1['SNOMASLAND'].values  \n",
    "#    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_1} {start_date_str} - {end_date_str}:\\n Max snow mass {month_name}\", 'kg m-2', 0, 100)   \n",
    "#    map_array[:, 0] = data_max_2['SNOMASLAND'].values\n",
    "#    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n Max snow mass {month_name}\", 'kg m-2', 0, 100)\n",
    "#    map_array[:, 0] = data_max_1['FRLANDSNO'].values\n",
    "#    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_1} {start_date_str} - {end_date_str}:\\n Max snow cover fraction {month_name}\", 'fraction', 0, 1)\n",
    "#    map_array[:, 0] = data_max_2['FRLANDSNO'].values\n",
    "#    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n Max snow cover fraction {month_name}\", 'fraction', 0, 1)       \n",
    "\n",
    "# Plot mean SNOMASLAND, FRLANDSNO, SMLAND, LHLANDSBLN\n",
    "\n",
    "    map_array[:, 0] = data_mean_1['FRLANDSNO'].values\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_1} {start_date_str} - {end_date_str}:\\n Mean daily snow cover fraction: {month_name}\", 'fraction', 0, 1)\n",
    "    map_array[:, 0] = data_mean_1['SMLAND'].values * num_seconds_in_month\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_1} {start_date_str} - {end_date_str}:\\n Mean monthly snow melt: {month_name}\", 'kg m-2', 0, 400)    \n",
    "\n",
    "#    map_array[:, 0] = data_mean_2['SNOMASLAND'].values\n",
    "#    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n Snow mass mean {month_name}\", 'kg m-2') #, 0, 100)\n",
    "    map_array[:, 0] = data_mean_2['FRLANDSNO'].values\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n Mean daily snow cover fraction: {month_name}\", 'fraction', 0, 1)\n",
    "    map_array[:, 0] = data_mean_2['SMLAND'].values * num_seconds_in_month\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n Mean monthly snow melt: {month_name}\", 'kg m-2', 0, 400)\n",
    "#    map_array[:, 0] = data_mean_2['LHLANDSBLN'].values\n",
    "#    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} {start_date_str} - {end_date_str}:\\n Snow sublimation mean {month_name}\", 'W m-2') #, 0, 1)\n",
    "    \n",
    "\n",
    "# Calculate  difference between the two experiments means for SNOMASLAND, FRLANDSNO, SMLAND, LHLANDSBLN\n",
    "# Exp 2 - Exp 1 \n",
    "#    map_array[:, 0] = data_mean_2['SNOMASLAND'].values - data_mean_1['SNOMASLAND'].values\n",
    "#    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} - {expt_name_1} {start_date_str} - {end_date_str}:\\n Snow mass mean diff {month_name}\", 'kg m-2') #, -0.5, 0.5)   \n",
    "    map_array[:, 0] = data_mean_2['FRLANDSNO'].values - data_mean_1['FRLANDSNO'].values\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_2} - {expt_name_1} {start_date_str} - {end_date_str}:\\n Diff in mean daily snow cover fraction: {month_name}\", 'fraction', -0.2, 0.2)   \n",
    "    map_array[:, 0] = data_mean_2['SMLAND'].values * num_seconds_in_month - data_mean_1['SMLAND'].values * num_seconds_in_month\n",
    "    plot_global_tight_pcm(map_array, True, False, f\"{expt_name_2} - {expt_name_1} {start_date_str} - {end_date_str}:\\n Diff in mean montly snow melt: {month_name}\", 'kg m-2', -200, 200)   \n",
    "#    map_array[:, 0] = data_mean_2['LHLANDSBLN'].values - data_mean_1['LHLANDSBLN'].values\n",
    "#    plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} - {expt_name_1} {start_date_str} - {end_date_str}:\\n Snow sublimation mean diff {month_name}\", 'W m-2') #, -0.5, 0.5)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data_mean_2['FRLANDSNO'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_array[:,0] = data_mean_2['FRLANDSNO']\n",
    "plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} - {expt_name_1} :\\n MOD10C1 N obs > 0.9 diff {month_name}\", '-', -0, 1)     \n",
    "\n",
    "map_array[:,0] = data_max_2['FRLANDSNO']\n",
    "plot_global_tight_pcm(map_array, False, False, f\"{expt_name_2} - {expt_name_1} :\\n MOD10C1 N obs > 0.9 diff {month_name}\", '-', -0, 1) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = start_date\n",
    "while current_date <= end_date:\n",
    "    year_month_directory = os.path.join(ana_directory, \n",
    "                                        f\"Y{current_date.year}\", \n",
    "                                        f\"M{current_date.month:02d}\")    \n",
    "\n",
    "    OFA_list =[]\n",
    "\n",
    "    for fname in sorted([os.path.join(year_month_directory, f) for f in os.listdir(year_month_directory) if '.ldas_ObsFcstAna.' in f and f.endswith('.bin')]):\n",
    "        if os.path.isfile(fname):\n",
    "            OFA_list.append(read_ObsFcstAna(fname))\n",
    "\n",
    "    if OFA_list:\n",
    "        print(OFA_list[0].keys())\n",
    "    else:\n",
    "        print(\"OFA_list is empty.\")   \n",
    "\n",
    "    # Initialize lists to store filtered data\n",
    "    all_tilenums = []\n",
    "    all_species = []\n",
    "    all_lats = []\n",
    "    all_lons = []\n",
    "    all_obs = []\n",
    "\n",
    "    # Process each element in OFA_list\n",
    "    for ofa in OFA_list:\n",
    "        # Create mask for this element\n",
    "        assim_mask = ofa['obs_assim'] == 1\n",
    "        \n",
    "        # Append filtered data\n",
    "        all_tilenums.append(ofa['obs_tilenum'][assim_mask])\n",
    "        all_species.append(ofa['obs_species'][assim_mask])\n",
    "        all_lats.append(ofa['obs_lat'][assim_mask])\n",
    "        all_lons.append(ofa['obs_lon'][assim_mask])\n",
    "        all_obs.append(ofa['obs_obs'][assim_mask])\n",
    "\n",
    "    # Create filtered dictionary with concatenated data\n",
    "    filtered_data = {\n",
    "        'obs_tilenum': np.concatenate(all_tilenums),\n",
    "        'obs_species': np.concatenate(all_species),\n",
    "        'obs_lat': np.concatenate(all_lats),\n",
    "        'obs_lon': np.concatenate(all_lons),\n",
    "        'obs_obs': np.concatenate(all_obs)\n",
    "    }\n",
    "\n",
    "    # Process filtered data\n",
    "    stats = get_tile_species_obs_values(filtered_data)\n",
    "\n",
    "    # Print summary\n",
    "    print(f\"Number of unique tiles: {len(stats['tiles'])}\")\n",
    "    print(f\"Number of species: {len(stats['max_values'])}\")\n",
    "    for species, max_vals in stats['max_values'].items():\n",
    "        print(f\"Species {species}: max value = {np.max(max_vals)}\")\n",
    "\n",
    "    # Initialize map_array with NaN values\n",
    "    map_array = np.empty((stats['lon'].shape[0], 3))  # Shape: (number of tiles, 3)\n",
    "    map_array.fill(np.nan)\n",
    "\n",
    "    # Fill longitude and latitude columns\n",
    "    map_array[:, 1] = stats['lon']  # Assuming `lon` contains longitude values\n",
    "    map_array[:, 2] = stats['lat']  # Assuming `lat` contains latitude values\n",
    "\n",
    "    map_array[:, 0] = stats['max_values'][12]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name} {start_date_str} - {end_date_str}:\\n Max MYD10C1 obs {current_date.strftime('%B %Y')}\", 'fraction', 0, 1)\n",
    "\n",
    "    map_array[:, 0] = stats['max_values'][13]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name} {start_date_str} - {end_date_str}:\\n Max MOD10C1 obs {current_date.strftime('%B %Y')}\", 'fraction', 0, 1)\n",
    "\n",
    "    map_array[:, 0] = stats['num_obs'][12]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name} {start_date_str} - {end_date_str}:\\n MYD10C1 N obs {current_date.strftime('%B %Y')}\", '-', 0, 30)\n",
    "\n",
    "    map_array[:, 0] = stats['num_obs'][13]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name} {start_date_str} - {end_date_str}:\\n MOD10C1 N obs {current_date.strftime('%B %Y')}\", '-', 0, 30)\n",
    "\n",
    "    map_array[:, 0] = stats['num_obs_gt_0.9'][12]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name} {start_date_str} - {end_date_str}:\\n MYD10C1 N obs > 0.9 {current_date.strftime('%B %Y')}\", '-', 0, 10)\n",
    "\n",
    "    map_array[:, 0] = stats['num_obs_gt_0.9'][13]\n",
    "    plot_global_tight_pcm(map_array, False, False, f\"{expt_name} {start_date_str} - {end_date_str}:\\n MOD10C1 N obs > 0.9 {current_date.strftime('%B %Y')}\", '-', 0, 10)\n",
    "\n",
    "    # Increment the date\n",
    "    current_date += relativedelta(months=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Usage over CONUS\n",
    "lon_min = -125.0\n",
    "lon_max = -66.0\n",
    "lat_min = 24.0\n",
    "lat_max = 50.0\n",
    "    \n",
    "    # Plot the data\n",
    "plot_region(map_array, \n",
    "        lon_min, lon_max,\n",
    "        lat_min, lat_max,\n",
    "        meanflag=False,\n",
    "        saveflag=False,\n",
    "        units='SCF',\n",
    "        plot_title=f'{expt_name} {start_date_str} - {end_date_str}:\\n MODIS Snow Cover Fraction',\n",
    "        cmin=0,\n",
    "        cmax=1,\n",
    "        cmap='viridis')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_directory = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/snow_qc_expts/LS_DAv8_M36_snow_qc/test_LS_DAv8_M36_snow_qc/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg/Y2005/M07/'\n",
    "\n",
    "files = glob.glob(os.path.join(root_directory, 'test_LS_DAv8_M36_snow_qc.inst3_1d_lndfcstana_Nt.*.nc4'))\n",
    "\n",
    "# Load the data \n",
    "ds_lndfcstana = xr.open_mfdataset(files, combine='nested', concat_dim=\"time\")\n",
    "\n",
    "# Sort the dataset by the time dimension\n",
    "ds_lndfcstana = ds_lndfcstana.sortby('time')\n",
    "\n",
    "# Extract the time variable\n",
    "time_var = ds_lndfcstana['time']\n",
    "# Convert the time variable to a numpy array\n",
    "time_values = time_var.values\n",
    "\n",
    "def create_date_array(time_values, start_date):\n",
    "    \"\"\"Convert array of 3-hour increments to datetime objects\"\"\"\n",
    "    # Convert numpy integers to Python integers\n",
    "    time_ints = [int(t) for t in time_values]\n",
    "    \n",
    "    # Create datetime array\n",
    "    dates = [start_date + timedelta(hours=3*t) for t in time_ints]\n",
    "    return np.array(dates)\n",
    "\n",
    "# Create date array\n",
    "start_date = datetime(2005, 7, 1, 3)\n",
    "dates = create_date_array(time_values, start_date)\n",
    "\n",
    "# Verify conversion\n",
    "print(f\"First date: {dates[0]}\")\n",
    "print(f\"Last date: {dates[-1]}\")\n",
    "print(f\"Total timesteps: {len(dates)}\")\n",
    "\n",
    "# Print the shape of the dataset\n",
    "print(ds_lndfcstana.dims)\n",
    "# Print the names of the variables in the dataset\n",
    "print(ds_lndfcstana.data_vars)\n",
    "\n",
    "TSOIL1_FCST = ds_lndfcstana['TSOIL1_FCST']\n",
    "\n",
    "# Calculate min and max per tile\n",
    "tile_min = TSOIL1_FCST.min(dim='time') - 273.15\n",
    "tile_max = TSOIL1_FCST.max(dim='time') - 273.15\n",
    "\n",
    "# Convert to numpy arrays (compute dask arrays)\n",
    "min_vals = tile_min.compute()\n",
    "max_vals = tile_max.compute()\n",
    "\n",
    "temperature_threshold = 10.0\n",
    "\n",
    "t_thres_str = f'{temperature_threshold:.1f}C'\n",
    "\n",
    "min_vals_thres = xr.where(min_vals > temperature_threshold, 1, 0)\n",
    "max_vals_thres = xr.where(max_vals > temperature_threshold, 1, 0)\n",
    "# Convert to numpy arrays\n",
    "min_vals_thres = min_vals_thres.values\n",
    "max_vals_thres = max_vals_thres.values\n",
    "\n",
    "# Print some statistics\n",
    "print(f\"Global min: {min_vals.min():.2f}K\")\n",
    "print(f\"Global max: {max_vals.max():.2f}K\")\n",
    "print(f\"Number of tiles: {len(min_vals)}\")\n",
    "\n",
    "# Convert xarray time coordinates to datetime\n",
    "TSOIL1_FCST = TSOIL1_FCST.assign_coords(time=dates)\n",
    "\n",
    "# Resample to daily frequency and get maximum\n",
    "daily_max = TSOIL1_FCST.resample(time='D').max()\n",
    "\n",
    "# Get minimum of daily maximums for each tile\n",
    "min_of_daily_max = daily_max.min(dim='time') - 273.15\n",
    "\n",
    "min_of_daily_max_thres = xr.where(min_of_daily_max > temperature_threshold, 1, 0)\n",
    "\n",
    "# Resample to daily frequency and get mean\n",
    "daily_mean = TSOIL1_FCST.resample(time='D').mean()\n",
    "# Get minimum of daily means for each tile\n",
    "min_of_daily_mean = daily_mean.min(dim='time') - 273.15\n",
    "min_of_daily_mean_thres = xr.where(min_of_daily_mean > temperature_threshold, 1, 0)\n",
    "\n",
    "print(f\"Shape of min_of_daily_max: {min_of_daily_max.shape}\")\n",
    "print(f\"Global min of daily max: {min_of_daily_max.min().values:.2f}°C\")\n",
    "print(f\"Global max of daily max: {min_of_daily_max.max().values:.2f}°C\")\n",
    "\n",
    "\n",
    "# Get lons from first time slice\n",
    "lons = ds_lndfcstana['lon'].isel(time=0).values\n",
    "lats = ds_lndfcstana['lat'].isel(time=0).values\n",
    "\n",
    "# Create map_array for plotting\n",
    "map_array = np.empty((len(lons), 3))  # Shape: (number of tiles, 3)\n",
    "map_array.fill(np.nan)\n",
    "# Fill longitude and latitude columns\n",
    "map_array[:, 1] = lons  # Assuming `lon` contains longitude values\n",
    "map_array[:, 2] = lats  # Assuming `lat` contains latitude values\n",
    "# Fill temperature values\n",
    "map_array[:, 0] = min_vals\n",
    "# Plotting\n",
    "plot_global_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min TSOIL1_FCST 2005-07', 'C', 0, 30)\n",
    "plot_NA_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min TSOIL1_FCST 2005-07', 'C', 0, 30)\n",
    "\n",
    "map_array[:, 0] = min_vals_thres\n",
    "# Plotting\n",
    "plot_global_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min TSOIL1_FCST 2005-07', f'{t_thres_str} threshold exceeded', 0, 1)\n",
    "plot_NA_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min TSOIL1_FCST 2005-07', f'{t_thres_str} threshold exceeded', 0, 1)\n",
    "\n",
    "# Fill temperature values\n",
    "map_array[:, 0] = max_vals\n",
    "# Plotting\n",
    "plot_global_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Max TSOIL1_FCST 2005-07', 'tC', 0, 30)\n",
    "plot_NA_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Max TSOIL1_FCST 2005-07', 'C', 0, 30)\n",
    "\n",
    "map_array[:, 0] = max_vals_thres\n",
    "# Plotting\n",
    "plot_global_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Max TSOIL1_FCST 2005-07', f'{t_thres_str} threshold exceeded', 0, 1)\n",
    "plot_NA_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Max TSOIL1_FCST 2005-07', f'{t_thres_str} threshold exceeded', 0, 1)\n",
    "\n",
    "map_array[:, 0] = min_of_daily_max\n",
    "# Plotting\n",
    "plot_global_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min of daily max TSOIL1_FCST 2005-07', 'C', 0, 30)\n",
    "plot_NA_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min of daily max TSOIL1_FCST 2005-07', 'C', 0, 30)\n",
    "\n",
    "map_array[:, 0] = min_of_daily_max_thres\n",
    "# Plotting\n",
    "plot_global_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min of daily max TSOIL1_FCST 2005-07', f'{t_thres_str} threshold exceeded', 0, 1)\n",
    "plot_NA_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min of daily max TSOIL1_FCST 2005-07', f'{t_thres_str} threshold exceeded', 0, 1)\n",
    "\n",
    "map_array[:, 0] = min_of_daily_mean\n",
    "\n",
    "# Plotting\n",
    "plot_global_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min of daily mean TSOIL1_FCST 2005-07', 'C', 0, 30)\n",
    "plot_NA_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min of daily mean TSOIL1_FCST 2005-07', 'C', 0, 30)\n",
    "\n",
    "map_array[:, 0] = min_of_daily_mean_thres\n",
    "\n",
    "# Plotting\n",
    "plot_global_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min of daily mean TSOIL1_FCST 2005-07', f'{t_thres_str} threshold exceeded', 0, 1)\n",
    "plot_NA_tight_pcm(map_array, False, False, f'{expt_name} {start_date_str} - {end_date_str}:\\n Min of daily mean TSOIL1_FCST 2005-07', f'{t_thres_str} threshold exceeded', 0, 1)\n",
    "\n",
    "# Close the dataset\n",
    "ds_lndfcstana.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-diag]",
   "language": "python",
   "name": "conda-env-.conda-diag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}