{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from netCDF4 import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import sys;       sys.path.append('../util/shared/python/')\n",
    "from read_GEOSldas          import read_tilecoord, read_obs_param\n",
    "\n",
    "from mapper_functions import plot_aus_tight_pcm, plot_global_tight_pcm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "species_groups = {\n",
    "    \"MODIS\": [0, 1]\n",
    "}\n",
    "\n",
    "expt_name = '1e_LS_DAv8_M36_qc6'\n",
    "\n",
    "start_date = datetime(2002, 10, 1)\n",
    "end_date = datetime(2003, 10, 1)\n",
    "\n",
    "start_date_str = start_date.strftime('%Y/%m/%d')\n",
    "end_date_str = end_date.strftime('%Y/%m/%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the OL data files\n",
    "\n",
    "stats_file_OL = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/snow_qc_expts/1e_LS_DAv8_M36_0/' \\\n",
    "'1e_LS_OLv8_M36_0/output/SMAP_EASEv2_M36_GLOBAL/figures/1e_LS_OLv8_M36_0_temporal_stats_OL_20021001_20060930.nc4'  # tmp_stats_OL_obsfrom_DA_20021001_20061001.nc4'\n",
    "\n",
    "print('reading stats nc4 file '+stats_file_OL)\n",
    "stats_OL = {}\n",
    "with Dataset(stats_file_OL,'r') as nc:\n",
    "    for key, value in nc.variables.items():\n",
    "        stats_OL[key] = value[:].filled(np.nan)\n",
    "\n",
    "ts_stats_file_OL = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/snow_qc_expts/1e_LS_DAv8_M36_0/' \\\n",
    "'1e_LS_OLv8_M36_0/output/SMAP_EASEv2_M36_GLOBAL/figures/1e_LS_OLv8_M36_0_spatial_stats_OL_200210_200609.pkl' #  stats_LS_OLv8_M36_200210_200610.pkl'\n",
    "\n",
    "with open(ts_stats_file_OL, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "stats_dict_OL = loaded_data\n",
    "date_vec_OL = loaded_data.get('date_vec', None)  \n",
    "date_vec = date_vec_OL    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the OL data files\n",
    "\n",
    "stats_file_DO = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/snow_qc_expts/1e_LS_DAv8_M36_0/' \\\n",
    "'1e_LS_DAv8_M36_qc6/output/SMAP_EASEv2_M36_GLOBAL/figures/temporal_stats_DA_20021001_20030930.nc4 ' #1e_LS_DAv8_M36_m10_temporal_stats_DA_20021001_20060930.nc4'  # tmp_stats_OL_obsfrom_DA_20021001_20061001.nc4'\n",
    "\n",
    "print('reading stats nc4 file '+stats_file_DO)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "stats_DO = {}\n",
    "with Dataset(stats_file_OL,'r') as nc:\n",
    "    for key, value in nc.variables.items():\n",
    "        stats_DO[key] = value[:].filled(np.nan)\n",
    "\n",
    "ts_stats_file_DO = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/snow_qc_expts/1e_LS_DAv8_M36_0/' \\\n",
    "'1e_LS_DAv8_M36_qc6/output/SMAP_EASEv2_M36_GLOBAL/figures/spatial_stats_DA_200210_200309.pkl' #1e_LS_DAv8_M36_m10_spatial_stats_DA_200210_200609.pkl' #  stats_LS_OLv8_M36_200210_200610.pkl'\n",
    "\n",
    "with open(ts_stats_file_DO, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "stats_dict_DO = loaded_data\n",
    "date_vec_DO = loaded_data.get('date_vec', None)  \n",
    "date_vec = date_vec_DO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the DA data files\n",
    "stats_file_DA = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/snow_qc_expts/1e_LS_DAv8_M36_0/' \\\n",
    "'1e_LS_DAv8_M36_0/output/SMAP_EASEv2_M36_GLOBAL/figures/1e_LS_DAv8_M36_0_temporal_stats_DA_20021001_20060930.nc4'\n",
    "print('reading stats nc4 file '+stats_file_DA)\n",
    "stats_DA = {}\n",
    "with Dataset(stats_file_DA,'r') as nc:\n",
    "    for key, value in nc.variables.items():\n",
    "        stats_DA[key] = value[:].filled(np.nan)\n",
    "\n",
    "ts_stats_file_DA = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/snow_qc_expts/1e_LS_DAv8_M36_0/' \\\n",
    "'1e_LS_DAv8_M36_0/output/SMAP_EASEv2_M36_GLOBAL/figures/1e_LS_DAv8_M36_0_spatial_stats_DA_200210_200609.pkl'\n",
    "\n",
    "with open(ts_stats_file_DA, 'rb') as f:\n",
    "    loaded_data = pickle.load(f)\n",
    "stats_dict_DA = loaded_data  # The entire dict appears to be the stats\n",
    "date_vec_DA = loaded_data.get('date_vec', None)  \n",
    "date_vec = date_vec_DA   \n",
    "\n",
    "# stats_file_DA = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/snow_qc_expts/1e_LS_DAv8_M36_0/' \\\n",
    "# '1e_LS_DAv8_M36_debug/output/SMAP_EASEv2_M36_GLOBAL/figures/temporal_stats_DA_20021001_20030930.nc4 ' #1e_LS_DAv8_M36_m10_temporal_stats_DA_20021001_20060930.nc4'  # tmp_stats_OL_obsfrom_DA_20021001_20061001.nc4'\n",
    "\n",
    "# print('reading stats nc4 file '+stats_file_DA)\n",
    "# stats_DA = {}\n",
    "# with Dataset(stats_file_OL,'r') as nc:\n",
    "#     for key, value in nc.variables.items():\n",
    "#         stats_DA[key] = value[:].filled(np.nan)\n",
    "\n",
    "# ts_stats_file_DA = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/snow_qc_expts/1e_LS_DAv8_M36_0/' \\\n",
    "# '1e_LS_DAv8_M36_debug/output/SMAP_EASEv2_M36_GLOBAL/figures/spatial_stats_DA_200210_200309.pkl' #1e_LS_DAv8_M36_m10_spatial_stats_DA_200210_200609.pkl' #  stats_LS_OLv8_M36_200210_200610.pkl'\n",
    "\n",
    "# with open(ts_stats_file_DA, 'rb') as f:\n",
    "#     loaded_data = pickle.load(f)\n",
    "# stats_dict_DA = loaded_data\n",
    "# date_vec_DA = loaded_data.get('date_vec', None)  \n",
    "# date_vec = date_vec_DA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_dict_shapes(data_dict, dict_name=\"Dictionary\"):\n",
    "    print(f\"\\n{dict_name} Structure:\")\n",
    "    print(\"-\" * 50)\n",
    "    for key, value in data_dict.items():\n",
    "        if isinstance(value, np.ndarray):\n",
    "            print(f\"{key:15} shape: {value.shape}, dtype: {value.dtype}\")\n",
    "            print(f\"{' '*15} first few values: {value[:3]}\")\n",
    "        else:\n",
    "            print(f\"{key:15} type: {type(value)}\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Inspect stats_dict_OL\n",
    "print_dict_shapes(stats_dict_OL, \"stats_dict_OL\")\n",
    "print_dict_shapes(stats_dict_DA, \"stats_dict_DA\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample of final compuation of selected diagnostic metrics for OL\n",
    " \n",
    "Nmin = 20\n",
    "\n",
    "# Then computer metrics of O-F, O-A, etc. based on above computed\n",
    "N_data = stats_OL['N_data']\n",
    "O_mean = stats_OL['obs_mean']\n",
    "# mean(x-y) = E[x] - E[y]   \n",
    "OmF_mean = stats_OL['obs_mean'] - stats_OL['fcst_mean']\n",
    "OmA_mean = stats_OL['obs_mean'] - stats_OL['ana_mean']\n",
    "# var(x-y) = var(x) + var(y) - 2cov(x,y)\n",
    "# cov(x,y) = E[xy] - E[x]E[y]\n",
    "OmF_stdv  = np.sqrt(stats_OL['obs_variance'] + stats_OL['fcst_variance'] - \\\n",
    "                    2 * (stats_OL['oxf_mean'] - stats_OL['obs_mean']*stats_OL['fcst_mean']))\n",
    "                    \n",
    "OmA_stdv  = np.sqrt(stats_OL['obs_variance'] + stats_OL['ana_variance'] - \\\n",
    "                    2 * (stats_OL['oxa_mean'] - stats_OL['obs_mean']*stats_OL['ana_mean']))\n",
    "\n",
    " # \"fcstvar\" is assumed constant here for convenience. Modify if necessary\n",
    "OmF_norm_mean = OmF_mean / np.sqrt(stats_OL['obsvar_mean'] + stats_OL['fcstvar_mean']) \n",
    "OmF_norm_stdv = np.sqrt(OmF_stdv**2 / (stats_OL['obsvar_mean'] + stats_OL['fcstvar_mean']) )\n",
    "  \n",
    "# Mask out data points with insufficent observations using the Nmin threshold\n",
    "# Do NOT apply to N_data\n",
    "OmF_mean[     N_data < Nmin] = np.nan\n",
    "OmF_stdv[     N_data < Nmin] = np.nan\n",
    "OmF_norm_mean[N_data < Nmin] = np.nan\n",
    "OmF_norm_stdv[N_data < Nmin] = np.nan\n",
    "OmA_mean[     N_data < Nmin] = np.nan\n",
    "OmA_stdv[     N_data < Nmin] = np.nan\n",
    "N_data[       N_data < Nmin] = 0\n",
    "\n",
    "OmF_mean_OL = OmF_mean\n",
    "OmF_stdv_OL = OmF_stdv\n",
    "OmF_norm_mean_OL = OmF_norm_mean\n",
    "OmF_norm_stdv_OL = OmF_norm_stdv\n",
    "OmA_mean_OL = OmA_mean\n",
    "OmA_stdv_OL = OmA_stdv\n",
    "N_data_OL = N_data\n",
    "\n",
    "group_metrics_OL = {}\n",
    "\n",
    "for group, species_indices in species_groups.items():\n",
    "    group_metrics_OL[group] = {}\n",
    "    group_N_data = np.nansum(N_data[:, species_indices], axis=1)\n",
    "    \n",
    "    group_metrics_OL[group]['OmF_mean'] = np.nansum(OmF_mean[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_OL[group]['OmF_stdv'] = np.nansum(OmF_stdv[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_OL[group]['OmF_norm_mean'] = np.nansum(OmF_norm_mean[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_OL[group]['OmF_norm_stdv'] = np.nansum(OmF_norm_stdv[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_OL[group]['OmA_mean'] = np.nansum(OmA_mean[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_OL[group]['OmA_stdv'] = np.nansum(OmA_stdv[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_OL[group]['Nobs_data'] = group_N_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sample of final compuation of selected diagnostic metrics for DA\n",
    "\n",
    "# Then computer metrics of O-F, O-A, etc. based on above computed\n",
    "N_data = stats_DA['N_data']\n",
    "O_mean = stats_DA['obs_mean']\n",
    "# mean(x-y) = E[x] - E[y]\n",
    "OmF_mean = stats_DA['obs_mean'] - stats_DA['fcst_mean']\n",
    "OmA_mean = stats_DA['obs_mean'] - stats_DA['ana_mean']\n",
    "# var(x-y) = var(x) + var(y) - 2cov(x,y)\n",
    "# cov(x,y) = E[xy] - E[x]E[y]\n",
    "OmF_stdv  = np.sqrt(stats_DA['obs_variance'] + stats_DA['fcst_variance'] - \\\n",
    "                    2 * (stats_DA['oxf_mean'] - stats_DA['obs_mean']*stats_DA['fcst_mean']))\n",
    "OmA_stdv  = np.sqrt(stats_DA['obs_variance'] + stats_DA['ana_variance'] - \\\n",
    "                    2 * (stats_DA['oxa_mean'] - stats_DA['obs_mean']*stats_DA['ana_mean']))\n",
    "# \"fcstvar\" is assumed constant here for convenience. Modify if necessary\n",
    "OmF_norm_mean = OmF_mean / np.sqrt(stats_DA['obsvar_mean'] + stats_DA['fcstvar_mean'])\n",
    "OmF_norm_stdv = np.sqrt(OmF_stdv**2 / (stats_DA['obsvar_mean'] + stats_DA['fcstvar_mean']) )\n",
    "# Mask out data points with insufficent observations using the Nmin threshold\n",
    "# Do NOT apply to N_data\n",
    "OmF_mean[     N_data < Nmin] = np.nan\n",
    "OmF_stdv[     N_data < Nmin] = np.nan\n",
    "OmF_norm_mean[N_data < Nmin] = np.nan\n",
    "OmF_norm_stdv[N_data < Nmin] = np.nan\n",
    "OmA_mean[     N_data < Nmin] = np.nan\n",
    "OmA_stdv[     N_data < Nmin] = np.nan\n",
    "N_data[       N_data < Nmin] = 0\n",
    "OmF_mean_DA = OmF_mean\n",
    "OmF_stdv_DA = OmF_stdv\n",
    "OmF_norm_mean_DA = OmF_norm_mean\n",
    "OmF_norm_stdv_DA = OmF_norm_stdv\n",
    "OmA_mean_DA = OmA_mean\n",
    "OmA_stdv_DA = OmA_stdv\n",
    "N_data_DA = N_data\n",
    "\n",
    "group_metrics_DA = {}\n",
    "\n",
    "for group, species_indices in species_groups.items():\n",
    "    group_metrics_DA[group] = {}\n",
    "    group_N_data = np.nansum(N_data[:, species_indices], axis=1)\n",
    "    \n",
    "    group_metrics_DA[group]['OmF_mean'] = np.nansum(OmF_mean[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DA[group]['OmF_stdv'] = np.nansum(OmF_stdv[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DA[group]['OmF_norm_mean'] = np.nansum(OmF_norm_mean[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DA[group]['OmF_norm_stdv'] = np.nansum(OmF_norm_stdv[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DA[group]['OmA_mean'] = np.nansum(OmA_mean[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DA[group]['OmA_stdv'] = np.nansum(OmA_stdv[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DA[group]['Nobs_data'] = group_N_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Then computer metrics of O-F, O-A, etc. based on above computed\n",
    "N_data = stats_DO['N_data']\n",
    "O_mean = stats_DO['obs_mean']\n",
    "# mean(x-y) = E[x] - E[y]\n",
    "OmF_mean = stats_DO['obs_mean'] - stats_DO['fcst_mean']\n",
    "OmA_mean = stats_DO['obs_mean'] - stats_DO['ana_mean']\n",
    "# var(x-y) = var(x) + var(y) - 2cov(x,y)\n",
    "# cov(x,y) = E[xy] - E[x]E[y]\n",
    "OmF_stdv  = np.sqrt(stats_DO['obs_variance'] + stats_DO['fcst_variance'] - \\\n",
    "                    2 * (stats_DO['oxf_mean'] - stats_DO['obs_mean']*stats_DO['fcst_mean']))\n",
    "OmA_stdv  = np.sqrt(stats_DO['obs_variance'] + stats_DO['ana_variance'] - \\\n",
    "                    2 * (stats_DO['oxa_mean'] - stats_DO['obs_mean']*stats_DO['ana_mean']))\n",
    "# \"fcstvar\" is assumed constant here for convenience. Modify if necessary\n",
    "OmF_norm_mean = OmF_mean / np.sqrt(stats_DO['obsvar_mean'] + stats_DO['fcstvar_mean'])\n",
    "OmF_norm_stdv = np.sqrt(OmF_stdv**2 / (stats_DO['obsvar_mean'] + stats_DO['fcstvar_mean']) )\n",
    "# Mask out data points with insufficent observations using the Nmin threshold\n",
    "# Do NOT apply to N_data\n",
    "OmF_mean[     N_data < Nmin] = np.nan\n",
    "OmF_stdv[     N_data < Nmin] = np.nan\n",
    "OmF_norm_mean[N_data < Nmin] = np.nan\n",
    "OmF_norm_stdv[N_data < Nmin] = np.nan\n",
    "OmA_mean[     N_data < Nmin] = np.nan\n",
    "OmA_stdv[     N_data < Nmin] = np.nan\n",
    "N_data[       N_data < Nmin] = 0\n",
    "OmF_mean_DA = OmF_mean\n",
    "OmF_stdv_DA = OmF_stdv\n",
    "OmF_norm_mean_DA = OmF_norm_mean\n",
    "OmF_norm_stdv_DA = OmF_norm_stdv\n",
    "OmA_mean_DA = OmA_mean\n",
    "OmA_stdv_DA = OmA_stdv\n",
    "N_data_DA = N_data\n",
    "\n",
    "group_metrics_DO = {}\n",
    "\n",
    "for group, species_indices in species_groups.items():\n",
    "    group_metrics_DO[group] = {}\n",
    "    group_N_data = np.nansum(N_data[:, species_indices], axis=1)\n",
    "    \n",
    "    group_metrics_DO[group]['OmF_mean'] = np.nansum(OmF_mean[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DO[group]['OmF_stdv'] = np.nansum(OmF_stdv[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DO[group]['OmF_norm_mean'] = np.nansum(OmF_norm_mean[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DO[group]['OmF_norm_stdv'] = np.nansum(OmF_norm_stdv[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DO[group]['OmA_mean'] = np.nansum(OmA_mean[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DO[group]['OmA_stdv'] = np.nansum(OmA_stdv[:, species_indices] * N_data[:, species_indices], axis=1) / group_N_data\n",
    "    group_metrics_DO[group]['Nobs_data'] = group_N_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ftc = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/land_sweeper/' \\\n",
    "      'LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/rc_out/LS_OLv8_M36.ldas_tilecoord.bin'\n",
    "tc = read_tilecoord(ftc)\n",
    "n_tile = tc['N_tile']\n",
    "print(f\"Number of tiles: {n_tile}\")\n",
    "lat = tc['com_lat']\n",
    "lon = tc['com_lon']\n",
    "\n",
    "map_array = np.empty([n_tile, 3])\n",
    "map_array.fill(np.nan)\n",
    "map_array[:, 1] = lon\n",
    "map_array[:, 2] = lat\n",
    "\n",
    "ftc = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/land_sweeper/' \\\n",
    "      'LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/rc_out/M21C_test_CF0360.ldas_tilecoord.bin'\n",
    "tc = read_tilecoord(ftc)\n",
    "n_tile = tc['N_tile']\n",
    "print(f\"Number of tiles: {n_tile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_array[:, 0] = group_metrics_DA['MODIS']['Nobs_data']\n",
    "\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "\n",
    "# Plot group map\n",
    "plot_global_tight_pcm(\n",
    "    map_array, \n",
    "    False, \n",
    "    True, \n",
    "    f'{expt_name} {start_date_str} - {end_date_str}: \\n (Max: {maxval:.3g} Min: {minval:.3g})', \n",
    "    'Cnt', \n",
    "    0, \n",
    "    2000\n",
    ")\n",
    "\n",
    "map_array[:, 0] = map_array[:, 0] / 1461 # Convert to obs per day\n",
    "\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "# Plot group map\n",
    "plot_global_tight_pcm(\n",
    "    map_array, \n",
    "    False, \n",
    "    True, \n",
    "    f'{expt_name} {start_date_str} - {end_date_str}: \\n (Max: {maxval:.3g} Min: {minval:.3g})', \n",
    "    'Obs per day',\n",
    "    0,\n",
    "    1.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_key_in_dict(dictionary, old_key, new_key):\n",
    "    if old_key in dictionary:\n",
    "        dictionary[new_key] = dictionary.pop(old_key)\n",
    "\n",
    "# Replace 'Ndata' with 'N_data' in the dictionaries\n",
    "replace_key_in_dict(stats_dict_DA, 'Ndata', 'N_data')\n",
    "replace_key_in_dict(stats_dict_DO, 'Ndata', 'N_data')\n",
    "replace_key_in_dict(stats_dict_OL, 'Ndata', 'N_data')\n",
    "\n",
    "# Verify the changes\n",
    "print(\"Keys in stats_dict_DA:\", stats_dict_DA.keys())\n",
    "print(\"Keys in stats_dict_DO:\", stats_dict_DO.keys())\n",
    "print(\"Keys in stats_dict_OL:\", stats_dict_OL.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_stats_dict_to_arrays(stats_dict):\n",
    "    \"\"\"Convert dictionary of lists to numpy arrays\"\"\"\n",
    "    array_dict = {}\n",
    "    \n",
    "    for key in stats_dict.keys():\n",
    "        # Convert list to array and reshape\n",
    "        array_dict[key] = np.array(stats_dict[key])\n",
    "        \n",
    "        # Check if we need to handle missing values (-- in data)\n",
    "        if isinstance(array_dict[key][0], (list, np.ndarray)):\n",
    "            # Replace '--' with np.nan\n",
    "            temp_array = []\n",
    "            for row in array_dict[key]:\n",
    "                cleaned_row = [np.nan if x == '--' else float(x) for x in row]\n",
    "                temp_array.append(cleaned_row)\n",
    "            array_dict[key] = np.array(temp_array)\n",
    "    \n",
    "    return array_dict\n",
    "\n",
    "# Convert dictionary\n",
    "stats_dict_DA_arrays = convert_stats_dict_to_arrays(stats_dict_DA)\n",
    "stats_dict_OL_arrays = convert_stats_dict_to_arrays(stats_dict_OL)\n",
    "stats_dict_DO_arrays = convert_stats_dict_to_arrays(stats_dict_DO)\n",
    "\n",
    "# Convert date vector to datetime objects\n",
    "date_vec_DA = [datetime.strptime(date, '%Y%m') for date in date_vec_DA]\n",
    "date_vec_OL = [datetime.strptime(date, '%Y%m') for date in date_vec_OL]\n",
    "date_vec_DO = [datetime.strptime(date, '%Y%m') for date in date_vec_DO]\n",
    "\n",
    "# Print first few dates to verify\n",
    "print(\"Sample dates:\", date_vec_DA[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_arrays(arrays_dict):\n",
    "    \"\"\"Print shape and sample info for each array in dictionary\"\"\"\n",
    "    for key, arr in arrays_dict.items():\n",
    "        print(f\"\\n{key}:\")\n",
    "        print(f\"  Shape: {arr.shape}\")\n",
    "        print(f\"  dtype: {arr.dtype}\")\n",
    "        print(f\"  Sample (first 3):\\n {arr[:3]}\")\n",
    "\n",
    "# Inspect the arrays\n",
    "inspect_arrays(stats_dict_OL_arrays)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_weighted_group_stats(stats_dict, species_groups):\n",
    "    \"\"\"Calculate weighted statistics for each group\"\"\"\n",
    "\n",
    "    print(\"N_data shape:\", stats_dict['N_data'].shape)\n",
    "    n_times = len(stats_dict['N_data'])\n",
    "    \n",
    "    n_times = len(stats_dict['OmF_mean'])\n",
    "    stats = ['OmF_mean', 'OmF_stdv', 'OmA_mean', 'OmA_stdv']\n",
    "    \n",
    "    # Initialize output dictionary\n",
    "    group_stats = {}\n",
    "    for group in species_groups.keys():\n",
    "        group_stats[group] = {stat: np.zeros(n_times) for stat in stats}\n",
    "        group_stats[group]['N_data'] = np.zeros(n_times)\n",
    "    \n",
    "    # Calculate weighted stats for each timestep\n",
    "    for t in range(n_times):\n",
    "        for group, indices in species_groups.items():\n",
    "            # Get weights for this group/time\n",
    "            weights = stats_dict['N_data'][t, indices]\n",
    "            total_weight = np.sum(weights)\n",
    "            \n",
    "            if total_weight > 0:\n",
    "                # Calculate weighted statistics\n",
    "                for stat in stats:\n",
    "                    values = stats_dict[stat][t, indices]\n",
    "                    group_stats[group][stat][t] = np.average(values, weights=weights)\n",
    "                group_stats[group]['N_data'][t] = total_weight\n",
    "            else:\n",
    "                # Set to NaN if no observations\n",
    "                for stat in stats:\n",
    "                    group_stats[group][stat][t] = np.nan\n",
    "                    \n",
    "    return group_stats\n",
    "\n",
    "# Calculate group means\n",
    "group_ts_DA = calculate_weighted_group_stats(stats_dict_DA_arrays, species_groups)\n",
    "group_ts_OL = calculate_weighted_group_stats(stats_dict_OL_arrays, species_groups)\n",
    "group_ts_DO = calculate_weighted_group_stats(stats_dict_DO_arrays, species_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"length of date_vec_DA\", len(date_vec_DA))\n",
    "print(\"length of date_vec\", len(date_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual plots for each species\n",
    "group = 'MODIS'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "length = len(date_vec_DO)\n",
    "\n",
    "# Plot OmF_mean for OL and DA\n",
    "mean_OL = np.nanmean(group_ts_OL[group]['OmF_mean'][0:length])\n",
    "mean_DA = np.nanmean(group_ts_DA[group]['OmF_mean'][0:length])\n",
    "mean_DO = np.nanmean(group_ts_DO[group]['OmF_mean'][0:length])\n",
    "\n",
    "plt.plot(date_vec_DO, group_ts_OL[group]['OmF_mean'][0:length], '--', label=f'{group} OL (Mean: {mean_OL:.6f})')\n",
    "plt.plot(date_vec_DO, group_ts_DA[group]['OmF_mean'][0:length], '-', label=f'{group} DA 0 (Mean: {mean_DA:.6f})')\n",
    "plt.plot(date_vec_DO, group_ts_DO[group]['OmF_mean'][0:length], '-.', label=f'{group} DA -10 (Mean: {mean_DO:.6f})')\n",
    "\n",
    "# Add black dotted line for Y = 0\n",
    "plt.axhline(y=0, color='black', linestyle=':', linewidth=1)\n",
    "\n",
    "# Customize plot\n",
    "plt.title(f'O-F Mean for {group}: {expt_name} {start_date_str} - {end_date_str}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('O-F Mean')\n",
    "plt.legend()\n",
    "\n",
    "# Set x-ticks to every 6 months\n",
    "# Set x-ticks using datetime array\n",
    "plt.xticks(date_vec_DA[::6], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual plots for each species\n",
    "group = 'MODIS'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "length = len(date_vec_DO)\n",
    "\n",
    "# Plot OmF_mean for OL and DA\n",
    "mean_OL = np.nanmean(group_ts_OL[group]['OmF_stdv'][0:length])\n",
    "mean_DA = np.nanmean(group_ts_DA[group]['OmF_stdv'][0:length])\n",
    "mean_DO = np.nanmean(group_ts_DO[group]['OmF_stdv'][0:length])\n",
    "\n",
    "plt.plot(date_vec_DO, group_ts_OL[group]['OmF_stdv'][0:length], '--', label=f'{group} OL (Mean: {mean_OL:.6f})')\n",
    "plt.plot(date_vec_DO, group_ts_DA[group]['OmF_stdv'][0:length], '-', label=f'{group} DA minSWE (Mean: {mean_DA:.6f})')\n",
    "plt.plot(date_vec_DO, group_ts_DO[group]['OmF_stdv'][0:length], '-.', label=f'{group} DA ght (Mean: {mean_DO:.6f})')\n",
    "\n",
    "# Customize plot\n",
    "plt.title(f'O-F StdDev for {group}: {expt_name} {start_date_str} - {end_date_str}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('O-F StdDev')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Set y-axis minimum to zero\n",
    "# plt.ylim(bottom=0)\n",
    "\n",
    "# Set x-ticks using datetime array\n",
    "#plt.xticks(date_vec_DA[::6], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual plots for each species\n",
    "group = 'MODIS'\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "length = len(date_vec_DO)\n",
    "\n",
    "# Plot OmF_mean for OL and DA\n",
    "mean_OL = np.nanmean(group_ts_OL[group]['N_data'][0:length])\n",
    "mean_DA = np.nanmean(group_ts_DA[group]['N_data'][0:length])\n",
    "mean_DO = np.nanmean(group_ts_DO[group]['N_data'][0:length])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(date_vec_DO, group_ts_OL[group]['N_data'][0:length], '--', label=f'{group} OL (Mean: {mean_OL:.3f})')\n",
    "plt.plot(date_vec_DO, group_ts_DA[group]['N_data'][0:length], '-', label=f'{group} DA 0 (Mean: {mean_DA:.3f})')\n",
    "plt.plot(date_vec_DO, group_ts_DO[group]['N_data'][0:length], '-.', label=f'{group} DA -10 (Mean: {mean_DO:.3f})')\n",
    "\n",
    "# Customize plot\n",
    "plt.title(f'N_data for {group}: {expt_name} {start_date_str} - {end_date_str}')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('N_data')\n",
    "plt.legend()\n",
    "\n",
    "# Set y-axis minimum to zero\n",
    "# plt.ylim(bottom=0)\n",
    "\n",
    "# Set x-ticks using datetime array\n",
    "plt.xticks(date_vec_DA[::6], rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print total number of obs for each experiment\n",
    "total_obs_OL = np.nansum(group_ts_OL[group]['N_data'])\n",
    "total_obs_DA = np.nansum(group_ts_DA[group]['N_data'])\n",
    "print(f'Total number of observations for OL: {total_obs_OL}')\n",
    "print(f'Total number of observations for DA: {total_obs_DA}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'MODIS'\n",
    "\n",
    "# Set observation counts from stored metrics\n",
    "map_array[:, 0] = group_metrics_OL[group_name]['OmF_stdv']\n",
    "\n",
    "# Get statistics\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "# Plot group map\n",
    "plot_global_tight_pcm(\n",
    "    map_array, \n",
    "    False, \n",
    "    True, \n",
    "    f'OL OmF StdDev {group_name} \\n (Max: {maxval:.3g} Min: {minval:.3g})', \n",
    "    'fraction', \n",
    "    0., \n",
    "    0.5\n",
    ")\n",
    "\n",
    "# Set observation counts from stored metrics\n",
    "map_array[:, 0] = group_metrics_DA[group_name]['OmF_stdv']\n",
    "\n",
    "# Get statistics\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "# Plot group map\n",
    "plot_global_tight_pcm(\n",
    "    map_array, \n",
    "    False, \n",
    "    True, \n",
    "    f'DA OmF StdDev {group_name} \\n (Max: {maxval:.3g} Min: {minval:.3g})', \n",
    "    'fraction', \n",
    "    0., \n",
    "    0.5\n",
    ")\n",
    "\n",
    "# Set observation counts from stored metrics\n",
    "map_array[:, 0] = group_metrics_DA[group_name]['OmF_stdv'] - group_metrics_OL[group_name]['OmF_stdv']\n",
    "\n",
    "# Get statistics\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "# Plot group map\n",
    "plot_global_tight_pcm(\n",
    "    map_array, \n",
    "    False, \n",
    "    True, \n",
    "    f'DA - OL OmF StdDev {group_name} \\n (Max: {maxval:.3g} Min: {minval:.3g})', \n",
    "    'fraction', \n",
    "    -0.2, \n",
    "    0.2\n",
    ")\n",
    "\n",
    "# Set observation counts from stored metrics\n",
    "map_array[:, 0] = np.divide(\n",
    "    (group_metrics_DA[group_name]['OmF_stdv'] - group_metrics_OL[group_name]['OmF_stdv']),\n",
    "    group_metrics_OL[group_name]['OmF_stdv'],\n",
    "    out=np.full_like(group_metrics_OL[group_name]['OmF_stdv'], np.nan, dtype=float),\n",
    "    where=group_metrics_OL[group_name]['OmF_stdv']!=0\n",
    ") * 100\n",
    "# Get statistics\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "# Plot group map\n",
    "plot_global_tight_pcm(\n",
    "    map_array, \n",
    "    False, \n",
    "    True, \n",
    "    f'(DA - OL) / OL OmF StdDev {group_name} \\n (Max: {maxval:.3g} Min: {minval:.3g})', \n",
    "    '%', \n",
    "    -50, \n",
    "    50\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "group_name = 'MODIS'\n",
    "\n",
    "# Set observation counts from stored metrics\n",
    "map_array[:, 0] = group_metrics_OL[group_name]['OmF_mean']\n",
    "\n",
    "# Get statistics\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "# Plot group map\n",
    "plot_global_tight_pcm(\n",
    "    map_array, \n",
    "    False, \n",
    "    True, \n",
    "    f'OL OmF_mean {group_name} \\n (Max: {maxval:.3g} Min: {minval:.3g})', \n",
    "    'fraction', \n",
    "    -1., \n",
    "    1.\n",
    ")\n",
    "\n",
    "# Set observation counts from stored metrics\n",
    "map_array[:, 0] = group_metrics_DA[group_name]['OmF_mean']\n",
    "\n",
    "# Get statistics\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "# Plot group map\n",
    "plot_global_tight_pcm(\n",
    "    map_array, \n",
    "    False, \n",
    "    True, \n",
    "    f'DA OmF_mean {group_name} \\n (Max: {maxval:.3g} Min: {minval:.3g})', \n",
    "    'fraction', \n",
    "    -1., \n",
    "    1.\n",
    ")\n",
    "\n",
    "# Set observation counts from stored metrics\n",
    "map_array[:, 0] = group_metrics_DA[group_name]['OmF_mean'] - group_metrics_OL[group_name]['OmF_mean']\n",
    "\n",
    "# Get statistics\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "# Plot group map\n",
    "plot_global_tight_pcm(\n",
    "    map_array, \n",
    "    False, \n",
    "    True, \n",
    "    f'DA - OL OmF_mean {group_name} \\n (Max: {maxval:.3g} Min: {minval:.3g})', \n",
    "    'fraction', \n",
    "    -0.5, \n",
    "    0.5\n",
    ")\n",
    "\n",
    "# Set observation counts from stored metrics\n",
    "map_array[:, 0] = np.divide(\n",
    "    (group_metrics_DA[group_name]['OmF_mean'] - group_metrics_OL[group_name]['OmF_mean']),\n",
    "    group_metrics_OL[group_name]['OmF_mean'],\n",
    "    out=np.full_like(group_metrics_OL[group_name]['OmF_mean'], np.nan, dtype=float),\n",
    "    where=group_metrics_OL[group_name]['OmF_mean']!=0\n",
    ") * 100\n",
    "# Get statistics\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "# Plot group map\n",
    "plot_global_tight_pcm(\n",
    "    map_array, \n",
    "    False, \n",
    "    True, \n",
    "    f'(DA - OL) / OL OmF_mean {group_name} \\n (Max: {maxval:.3g} Min: {minval:.3g})', \n",
    "    '%', \n",
    "    -50, \n",
    "    50\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# -----------------------------\n",
    "# Set up parameters\n",
    "# -----------------------------\n",
    "x_true = 0.45                 # \"True\" soil moisture (unknown to filter)\n",
    "x_b = 0.25                    # Background state (forecast)\n",
    "P_b = 0.01                    # Background error variance\n",
    "\n",
    "y_obs = 240.0                 # Observation (brightness temperature)\n",
    "R = 4.0                       # Observation error variance (Tb in K^2)\n",
    "\n",
    "N = 40                        # Ensemble size\n",
    "\n",
    "# Nonlinear observation operator\n",
    "def H(x):\n",
    "    return 270 - 60 * x\n",
    "\n",
    "# Derivative (used by SEKF)\n",
    "def H_prime(x):\n",
    "    return -60.0  # Constant in this case\n",
    "\n",
    "# -----------------------------\n",
    "# SEKF Update\n",
    "# -----------------------------\n",
    "H_xb = H(x_b)\n",
    "K_sekf = P_b * H_prime(x_b) / (H_prime(x_b)**2 * P_b + R)\n",
    "x_sekf = x_b + K_sekf * (y_obs - H_xb)\n",
    "\n",
    "# -----------------------------\n",
    "# EnKF Update\n",
    "# -----------------------------\n",
    "# Generate ensemble around x_b\n",
    "np.random.seed(42)\n",
    "x_ens = np.random.normal(x_b, np.sqrt(P_b), N)\n",
    "y_ens = np.random.normal(y_obs, np.sqrt(R), N)\n",
    "\n",
    "# Apply nonlinear H to ensemble\n",
    "H_ens = H(x_ens)\n",
    "\n",
    "# Compute sample statistics\n",
    "x_mean = np.mean(x_ens)\n",
    "H_mean = np.mean(H_ens)\n",
    "\n",
    "# Covariances\n",
    "cov_xh = np.mean((x_ens - x_mean) * (H_ens - H_mean))\n",
    "var_h = np.var(H_ens, ddof=1)\n",
    "\n",
    "K_enkf = cov_xh / (var_h + R)\n",
    "\n",
    "# Update each ensemble member\n",
    "x_ens_update = x_ens + K_enkf * (y_ens - H_ens)\n",
    "x_enkf = np.mean(x_ens_update)\n",
    "\n",
    "# -----------------------------\n",
    "# Plot Results\n",
    "# -----------------------------\n",
    "x_vals = np.linspace(0.1, 0.6, 100)\n",
    "H_vals = H(x_vals)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x_vals, H_vals, label='H(x)', color='black')\n",
    "plt.axhline(y_obs, linestyle='--', color='gray', label='Observation (y)')\n",
    "plt.axvline(x_b, color='blue', linestyle=':', label='Background (x_b)')\n",
    "plt.axvline(x_sekf, color='green', linestyle='--', label='SEKF Analysis')\n",
    "plt.axvline(x_enkf, color='red', linestyle='--', label='EnKF Analysis')\n",
    "\n",
    "plt.scatter(x_ens, H(x_ens), color='blue', alpha=0.5, label='Ensemble (prior)')\n",
    "plt.scatter(x_ens_update, H(x_ens_update), color='red', alpha=0.5, label='Ensemble (analysis)')\n",
    "\n",
    "plt.xlabel('Soil Moisture (x)')\n",
    "plt.ylabel('Brightness Temperature H(x) [K]')\n",
    "plt.title('SEKF vs EnKF: Nonlinear Observation Operator')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Print results\n",
    "print(f\"SEKF Analysis: x = {x_sekf:.3f}\")\n",
    "print(f\"EnKF Analysis: x = {x_enkf:.3f}\")\n",
    "print(f\"Kalman Gain (SEKF): {K_sekf:.4f}\")\n",
    "print(f\"Kalman Gain (EnKF): {K_enkf:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ---------------------------------------\n",
    "# Nonlinear observation operator (complex)\n",
    "# ---------------------------------------\n",
    "def H(x):\n",
    "    return 270 - 60 * x + 10 * np.sin(3 * np.pi * x)\n",
    "\n",
    "# Finite difference approximation of H'(x)\n",
    "def finite_diff_H_prime(H, x, delta=1e-4):\n",
    "    return (H(x + delta) - H(x - delta)) / (2 * delta)\n",
    "\n",
    "# ---------------------------------------\n",
    "# SEKF parameters\n",
    "# ---------------------------------------\n",
    "x_b = 0.25       # Background state\n",
    "y_obs = 240.0    # Observation\n",
    "P = 0.01         # Background error variance\n",
    "R = 4.0          # Observation error variance\n",
    "\n",
    "# Evaluate H(x_b) and approximate H' using finite difference\n",
    "H_xb = H(x_b)\n",
    "H_prime_fd = finite_diff_H_prime(H, x_b)\n",
    "\n",
    "# Compute SEKF gain and analysis\n",
    "K_sekf = P * H_prime_fd / (H_prime_fd**2 * P + R)\n",
    "x_sekf = x_b + K_sekf * (y_obs - H_xb)\n",
    "\n",
    "# ---------------------------------------\n",
    "# Visualization\n",
    "# ---------------------------------------\n",
    "x_vals = np.linspace(x_b - 0.1, x_b + 0.1, 300)\n",
    "H_vals = H(x_vals)\n",
    "H_lin = H_xb + H_prime_fd * (x_vals - x_b)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(x_vals, H_vals, label='Nonlinear H(x)', linewidth=2)\n",
    "plt.plot(x_vals, H_lin, '--', label='Finite-Difference Linearization', linewidth=2)\n",
    "plt.axhline(y_obs, color='gray', linestyle=':', label='Observation y')\n",
    "plt.axvline(x_b, color='black', linestyle=':', label='Background x_b')\n",
    "plt.axvline(x_sekf, color='green', linestyle='--', label=f'SEKF Analysis x_a ≈ {x_sekf:.3f}')\n",
    "\n",
    "plt.scatter([x_b], [H_xb], color='black', label='H(x_b)', zorder=5)\n",
    "plt.xlabel('State x')\n",
    "plt.ylabel('Observed Value H(x)')\n",
    "plt.title('SEKF Linearization with Complex Nonlinear Operator')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print summary\n",
    "print(f\"SEKF derivative estimate: H'(x_b) ≈ {H_prime_fd:.3f}\")\n",
    "print(f\"SEKF Kalman gain: K ≈ {K_sekf:.4f}\")\n",
    "print(f\"SEKF Analysis state: x_a ≈ {x_sekf:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
