{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d26253e-9185-432c-82d5-cf4b8ce37dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.array as da\n",
    "import dask\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import os\n",
    "from dateutil.relativedelta import relativedelta\n",
    "from datetime import datetime\n",
    "\n",
    "expt_name = 'LS_DAv8_M36'\n",
    "\n",
    "start_date = datetime(2005, 1, 1)\n",
    "end_date = datetime(2006, 1, 1)\n",
    "\n",
    "start_date_str = start_date.strftime('%Y%m%d')\n",
    "end_date_str = end_date.strftime('%Y%m%d')\n",
    "\n",
    "root_directory = f'/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_DAv8_M36/{expt_name}/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg'\n",
    "\n",
    "output_file = f'{expt_name}_{start_date_str}_{end_date_str}_catch_progn_snow_incr.nc4'\n",
    "\n",
    "snow_list = []\n",
    "current_date = start_date\n",
    "files_found = 0\n",
    "\n",
    "while current_date <= end_date:\n",
    "    year_month_directory = os.path.join(root_directory, \n",
    "                                        f\"Y{current_date.year}\", \n",
    "                                        f\"M{current_date.month:02d}\")\n",
    "    for filename in sorted(os.listdir(year_month_directory)):\n",
    "        if filename.endswith('.nc4') and not filename.endswith('z.nc4') and filename.startswith(f'{expt_name}.catch_progn_incr.2'):\n",
    "            file_path = os.path.join(year_month_directory, filename)\n",
    "\n",
    "            ds = xr.open_dataset(file_path, chunks={})  # Dask-aware open\n",
    "\n",
    "            # Convert or standardize time coordinate\n",
    "            if np.issubdtype(ds['time_stamp'].dtype, np.datetime64):\n",
    "                time_coord = ds['time_stamp']\n",
    "            else:\n",
    "                try:\n",
    "                    decoded_times = [t.decode('utf-8') for t in ds['time_stamp'].values]\n",
    "                    parsed_times = np.array([np.datetime64(datetime.strptime(t[:13], \"%Y%m%d_%H%M\"), 'ns') for t in decoded_times])\n",
    "                    time_coord = xr.DataArray(parsed_times, dims='time', name='time')\n",
    "                except Exception as e:\n",
    "                    continue\n",
    "\n",
    "            wesnn1_incr = ds['WESNN1_INCR']\n",
    "            wesnn2_incr = ds['WESNN2_INCR']\n",
    "            wesnn3_incr = ds['WESNN3_INCR']\n",
    "            snow_incr = wesnn1_incr + wesnn2_incr + wesnn3_incr\n",
    "\n",
    "            snow_incr = snow_incr.assign_coords(time=time_coord)\n",
    "            snow_list.append(snow_incr)\n",
    "            files_found += 1\n",
    "\n",
    "    current_date += relativedelta(months=1)\n",
    "\n",
    "if files_found == 0:\n",
    "    raise RuntimeError(\"No valid input files found. Aborting.\")\n",
    "\n",
    "# Concatenate all time slices and save once\n",
    "snow_concat = xr.concat(snow_list, dim='time')\n",
    "out_ds = xr.Dataset({'SNOW_INCR': snow_concat})\n",
    "out_ds.to_netcdf(output_file)\n",
    "\n",
    "print(f'Finished writing merged dataset to {output_file}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d1f43a-2d82-4859-9c60-ce772dcf3dfd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-diag]",
   "language": "python",
   "name": "conda-env-.conda-diag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}