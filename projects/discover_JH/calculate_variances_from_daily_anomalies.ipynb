{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3576191f-2a29-47b6-bdc5-e1523dbd6e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Variance diagnostics from daily anomalies (OL vs DA)\n",
    "\n",
    "Products written to --outdir:\n",
    "  - variance_daily_fullperiod.nc\n",
    "  - variance_withinmonth_daily.nc\n",
    "  - variance_monthlymean_IAV.nc\n",
    "  - variance_annualmean_IAV.nc\n",
    "\n",
    "Optionally, if you *already* have monthly-mean files, you can pass\n",
    "--ol-monthly and --da-monthly to compute the monthly-mean IAV from those\n",
    "instead of resampling the daily anomalies.\n",
    "\n",
    "Example (from your notebook shell):\n",
    "  %run variance_diagnostics_from_anoms.py \\\n",
    "      --outdir ./yearly_outputs \\\n",
    "      --vars SFMC RZMC \\\n",
    "      --chunks time:365,tile:4096 \\\n",
    "      --decode-times \\\n",
    "      --use-monthly-from-daily\n",
    "\n",
    "Or, with pre-made monthly means:\n",
    "  %run variance_diagnostics_from_anoms.py \\\n",
    "      --outdir ./yearly_outputs \\\n",
    "      --vars SFMC RZMC \\\n",
    "      --ol-monthly /path/to/OL_monthly_means_*.nc \\\n",
    "      --da-monthly /path/to/DA_monthly_means_*.nc\n",
    "\"\"\"\n",
    "\n",
    "import os, glob, argparse, logging\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "# -----------------------------\n",
    "# Dask / logging\n",
    "# -----------------------------\n",
    "dask.config.set({\n",
    "    \"scheduler\": \"threads\",\n",
    "    \"array.slicing.split_large_chunks\": True,\n",
    "    \"optimization.fuse.active\": True\n",
    "})\n",
    "\n",
    "def setup_logger(verbosity: int = 1) -> logging.Logger:\n",
    "    level = logging.INFO if verbosity == 1 else (logging.DEBUG if verbosity >= 2 else logging.WARNING)\n",
    "    logging.basicConfig(level=level, format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "                        datefmt=\"%H:%M:%S\", force=True)\n",
    "    return logging.getLogger(\"variance-diags\")\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers\n",
    "# -----------------------------\n",
    "def parse_chunk_flag(s: str) -> dict:\n",
    "    out = {}\n",
    "    if not s: return out\n",
    "    for kv in s.split(\",\"):\n",
    "        if not kv.strip(): continue\n",
    "        k, v = kv.split(\":\"); out[k.strip()] = int(v.strip())\n",
    "    return out\n",
    "\n",
    "def ensure_monotonic_unique_time(ds: xr.Dataset) -> xr.Dataset:\n",
    "    \"\"\"Sort by time and drop duplicate timestamps (keeping first).\"\"\"\n",
    "    t = ds[\"time\"].values\n",
    "    # argsort → sorted indices\n",
    "    order = np.argsort(t)\n",
    "    t_sorted = t[order]\n",
    "    # unique mask on sorted values\n",
    "    keep = np.ones(t_sorted.shape[0], dtype=bool)\n",
    "    keep[1:] = t_sorted[1:] != t_sorted[:-1]\n",
    "    idx = order[keep]\n",
    "    return ds.isel(time=idx)\n",
    "\n",
    "def open_mf(pattern, chunks, decode_times=True, log=None, label=\"\"):\n",
    "    files = sorted(glob.glob(pattern))\n",
    "    if not files:\n",
    "        raise FileNotFoundError(f\"No files match pattern: {pattern}\")\n",
    "    if log: log.info(f\"[open] {label}: {len(files)} files\")\n",
    "    ds = xr.open_mfdataset(\n",
    "        files,\n",
    "        combine=\"nested\", concat_dim=\"time\",\n",
    "        engine=\"netcdf4\", parallel=False,\n",
    "        chunks=chunks,\n",
    "        data_vars=\"minimal\", coords=\"minimal\", compat=\"override\",\n",
    "        mask_and_scale=False,\n",
    "        decode_times=decode_times, decode_coords=decode_times, use_cftime=False,\n",
    "    )\n",
    "    return ds\n",
    "\n",
    "def write_nc(ds: xr.Dataset, path: str, chunks, log=None):\n",
    "    comp = dict(zlib=True, complevel=4)\n",
    "    encoding = {v: {**comp} for v in ds.data_vars}\n",
    "    # add chunk hints for 2D (time,tile) or (month,tile) or (tile,) vars\n",
    "    for v, da in ds.data_vars.items():\n",
    "        dims = da.dims\n",
    "        if len(dims) == 2 and \"tile\" in dims:\n",
    "            lead = [d for d in dims if d != \"tile\"][0]\n",
    "            lead_chunk = min(int(ds.sizes.get(lead, 1)),\n",
    "                             int(chunks.get(lead, chunks.get(\"time\", 365))))\n",
    "            tile_chunk = min(int(ds.sizes.get(\"tile\", 1)),\n",
    "                             int(chunks.get(\"tile\", 4096)))\n",
    "            encoding[v][\"chunksizes\"] = (lead_chunk, tile_chunk)\n",
    "        elif dims == (\"tile\",):\n",
    "            encoding[v][\"chunksizes\"] = (min(int(ds.sizes[\"tile\"]),\n",
    "                                             int(chunks.get(\"tile\", 4096))),)\n",
    "    if log: log.info(f\"→ Writing {path}\")\n",
    "    delayed = ds.to_netcdf(path, engine=\"h5netcdf\", encoding=encoding, compute=False)\n",
    "    with ProgressBar(): dask.compute(delayed)\n",
    "    if log: log.info(f\"✓ Wrote {path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Core computations\n",
    "# -----------------------------\n",
    "def daily_variance(ds_ol, ds_da, vars_, eps, log):\n",
    "    out = []\n",
    "    for v in vars_:\n",
    "        s2_ol = ds_ol[v].var(\"time\", skipna=True)\n",
    "        s2_da = ds_da[v].var(\"time\", skipna=True)\n",
    "        d     = s2_da - s2_ol\n",
    "        pct   = xr.where(s2_ol > eps, 100.0 * d / s2_ol, np.nan)\n",
    "        out.append(xr.Dataset({\n",
    "            f\"{v}_daily_var_OL\": s2_ol.astype(\"float32\"),\n",
    "            f\"{v}_daily_var_DA\": s2_da.astype(\"float32\"),\n",
    "            f\"{v}_daily_var_delta\": d.astype(\"float32\"),\n",
    "            f\"{v}_daily_var_pct\": pct.astype(\"float32\"),\n",
    "        }))\n",
    "    ds = xr.merge(out)\n",
    "    ds = ds.assign_coords(lat=ds_ol.lat, lon=ds_ol.lon)\n",
    "    if log: log.info(\"[daily] computed.\")\n",
    "    return ds\n",
    "\n",
    "def within_month_daily_variance(ds_ol, ds_da, vars_, eps, log):\n",
    "    # Ensure datetime time\n",
    "    if not np.issubdtype(ds_ol.time.dtype, np.datetime64):\n",
    "        ds_ol = xr.decode_cf(ds_ol)\n",
    "    if not np.issubdtype(ds_da.time.dtype, np.datetime64):\n",
    "        ds_da = xr.decode_cf(ds_da)\n",
    "\n",
    "    out = []\n",
    "    mon_ol = ds_ol[\"time\"].dt.month\n",
    "    mon_da = ds_da[\"time\"].dt.month\n",
    "\n",
    "    def _ensure_month_dim(da: xr.DataArray) -> xr.DataArray:\n",
    "        # Rename the grouping dimension (whatever it is) to \"month\"\n",
    "        # (xarray may call it \"group\" or already \"month\")\n",
    "        if \"month\" in da.dims:\n",
    "            return da\n",
    "        # pick the non-tile dim (group dim)\n",
    "        gdim = next(d for d in da.dims if d != \"tile\")\n",
    "        return da.rename({gdim: \"month\"})\n",
    "\n",
    "    for v in vars_:\n",
    "        s2m_ol = ds_ol[v].groupby(mon_ol).var(\"time\", skipna=True)\n",
    "        s2m_da = ds_da[v].groupby(mon_da).var(\"time\", skipna=True)\n",
    "        s2m_ol = _ensure_month_dim(s2m_ol)\n",
    "        s2m_da = _ensure_month_dim(s2m_da)\n",
    "\n",
    "        d   = s2m_da - s2m_ol\n",
    "        pct = xr.where(s2m_ol > eps, 100.0 * d / s2m_ol, np.nan)\n",
    "\n",
    "        out.append(xr.Dataset({\n",
    "            f\"{v}_month_dailyvar_OL\": s2m_ol.astype(\"float32\"),\n",
    "            f\"{v}_month_dailyvar_DA\": s2m_da.astype(\"float32\"),\n",
    "            f\"{v}_month_dailyvar_delta\": d.astype(\"float32\"),\n",
    "            f\"{v}_month_dailyvar_pct\": pct.astype(\"float32\"),\n",
    "        }))\n",
    "\n",
    "    ds = xr.merge(out).assign_coords(lat=ds_ol.lat, lon=ds_ol.lon)\n",
    "    if log: log.info(\"[within-month] computed.\")\n",
    "    return ds\n",
    "\n",
    "def monthlymean_variance_from_daily(ds_ol, ds_da, vars_, eps, log):\n",
    "    out = []\n",
    "    for v in vars_:\n",
    "        m_ol = ds_ol[v].resample(time=\"1MS\").mean(keep_attrs=True)\n",
    "        m_da = ds_da[v].resample(time=\"1MS\").mean(keep_attrs=True)\n",
    "        s2_ol = m_ol.var(\"time\", skipna=True)\n",
    "        s2_da = m_da.var(\"time\", skipna=True)\n",
    "        d     = s2_da - s2_ol\n",
    "        pct   = xr.where(s2_ol > eps, 100.0 * d / s2_ol, np.nan)\n",
    "        out.append(xr.Dataset({\n",
    "            f\"{v}_monthlymean_var_OL\": s2_ol.astype(\"float32\"),\n",
    "            f\"{v}_monthlymean_var_DA\": s2_da.astype(\"float32\"),\n",
    "            f\"{v}_monthlymean_var_delta\": d.astype(\"float32\"),\n",
    "            f\"{v}_monthlymean_var_pct\": pct.astype(\"float32\"),\n",
    "        }))\n",
    "    ds = xr.merge(out).assign_coords(lat=ds_ol.lat, lon=ds_ol.lon)\n",
    "    if log: log.info(\"[monthly-mean IAV] computed (from daily).\")\n",
    "    return ds\n",
    "\n",
    "def monthlymean_variance_from_monthfiles(ol_pat, da_pat, vars_, chunks, eps, log):\n",
    "    # Open monthly-mean stacks (assumed time is monthly and decoded)\n",
    "    ds_ol = open_mf(ol_pat, chunks, decode_times=True, log=log, label=\"OL monthly\")\n",
    "    ds_da = open_mf(da_pat, chunks, decode_times=True, log=log, label=\"DA monthly\")\n",
    "    ds_ol, ds_da = xr.align(ds_ol, ds_da, join=\"exact\")\n",
    "    out = []\n",
    "    for v in vars_:\n",
    "        # If these are raw monthly means (not anomalies), you can subtract an OL monthly climatology:\n",
    "        # mclim = ds_ol[v].groupby(\"time.dt.month\").mean(\"time\", skipna=True)\n",
    "        # mol_anom = ds_ol[v] - mclim.sel(month=ds_ol[\"time.dt.month\"])\n",
    "        # mda_anom = ds_da[v] - mclim.sel(month=ds_da[\"time.dt.month\"])\n",
    "        # Here we assume your monthly means are already anomaly-consistent with the daily baseline.\n",
    "        s2_ol = ds_ol[v].var(\"time\", skipna=True)\n",
    "        s2_da = ds_da[v].var(\"time\", skipna=True)\n",
    "        d     = s2_da - s2_ol\n",
    "        pct   = xr.where(s2_ol > eps, 100.0 * d / s2_ol, np.nan)\n",
    "        out.append(xr.Dataset({\n",
    "            f\"{v}_monthlymean_var_OL\": s2_ol.astype(\"float32\"),\n",
    "            f\"{v}_monthlymean_var_DA\": s2_da.astype(\"float32\"),\n",
    "            f\"{v}_monthlymean_var_delta\": d.astype(\"float32\"),\n",
    "            f\"{v}_monthlymean_var_pct\": pct.astype(\"float32\"),\n",
    "        }))\n",
    "    ds = xr.merge(out)\n",
    "    if \"lat\" in ds_ol and \"lon\" in ds_ol:\n",
    "        ds = ds.assign_coords(lat=ds_ol.lat, lon=ds_ol.lon)\n",
    "    if log: log.info(\"[monthly-mean IAV] computed (from monthly files).\")\n",
    "    return ds\n",
    "\n",
    "def annualmean_variance(ds_ol, ds_da, vars_, eps, log):\n",
    "    out = []\n",
    "    for v in vars_:\n",
    "        a_ol = ds_ol[v].resample(time=\"1Y\").mean(keep_attrs=True)\n",
    "        a_da = ds_da[v].resample(time=\"1Y\").mean(keep_attrs=True)\n",
    "        s2_ol = a_ol.var(\"time\", skipna=True)\n",
    "        s2_da = a_da.var(\"time\", skipna=True)\n",
    "        d     = s2_da - s2_ol\n",
    "        pct   = xr.where(s2_ol > eps, 100.0 * d / s2_ol, np.nan)\n",
    "        out.append(xr.Dataset({\n",
    "            f\"{v}_annualmean_var_OL\": s2_ol.astype(\"float32\"),\n",
    "            f\"{v}_annualmean_var_DA\": s2_da.astype(\"float32\"),\n",
    "            f\"{v}_annualmean_var_delta\": d.astype(\"float32\"),\n",
    "            f\"{v}_annualmean_var_pct\": pct.astype(\"float32\"),\n",
    "        }))\n",
    "    ds = xr.merge(out).assign_coords(lat=ds_ol.lat, lon=ds_ol.lon)\n",
    "    if log: log.info(\"[annual-mean IAV] computed.\")\n",
    "    return ds\n",
    "\n",
    "# -----------------------------\n",
    "# Args / main\n",
    "# -----------------------------\n",
    "def parse_args(argv=None):\n",
    "    p = argparse.ArgumentParser(description=\"Variance diagnostics from daily anomalies (OL vs DA).\")\n",
    "    p.add_argument(\"--outdir\", default=\"./yearly_outputs\")\n",
    "    p.add_argument(\"--vars\", nargs=\"+\", default=[\"SFMC\",\"RZMC\"])\n",
    "    p.add_argument(\"--chunks\", default=\"time:365,tile:4096\")\n",
    "    p.add_argument(\"--decode-times\", action=\"store_true\",\n",
    "                   help=\"Decode CF times for dailies (recommended for groupby/resample).\")\n",
    "    p.add_argument(\"--eps\", type=float, default=1e-10, help=\"Guard for percentage changes.\")\n",
    "    # Option: use pre-made monthly-mean files\n",
    "    p.add_argument(\"--ol-monthly\", default=\"\", help=\"Glob for OL monthly-mean files (optional).\")\n",
    "    p.add_argument(\"--da-monthly\", default=\"\", help=\"Glob for DA monthly-mean files (optional).\")\n",
    "    p.add_argument(\"--use-monthly-from-daily\", action=\"store_true\",\n",
    "                   help=\"Compute monthly-mean IAV by resampling daily anomalies (default).\")\n",
    "    p.add_argument(\"--verbose\", type=int, default=1)\n",
    "    return p.parse_args(argv)\n",
    "\n",
    "def main(argv=None):\n",
    "    args = parse_args(argv)\n",
    "    log  = setup_logger(args.verbose)\n",
    "    chunks = parse_chunk_flag(args.chunks)\n",
    "    os.makedirs(args.outdir, exist_ok=True)\n",
    "\n",
    "    # Input daily anomaly patterns\n",
    "    ol_daily_pat = os.path.join(args.outdir, \"OLv8_daily_anomalies_kept_*.nc\")\n",
    "    da_daily_pat = os.path.join(args.outdir, \"DAv8_daily_anomalies_kept_*.nc\")\n",
    "\n",
    "    # Output files\n",
    "    f_daily   = os.path.join(args.outdir, \"variance_daily_fullperiod.nc\")\n",
    "    f_inmonth = os.path.join(args.outdir, \"variance_withinmonth_daily.nc\")\n",
    "    f_mmiav   = os.path.join(args.outdir, \"variance_monthlymean_IAV.nc\")\n",
    "    f_aiav    = os.path.join(args.outdir, \"variance_annualmean_IAV.nc\")\n",
    "\n",
    "    # Open daily anomalies (decode times for month/year ops)\n",
    "    log.info(\"Opening daily anomalies …\")\n",
    "    ds_ol = open_mf(ol_daily_pat, chunks, decode_times=args.decode_times, log=log, label=\"OL daily anoms\")\n",
    "    ds_da = open_mf(da_daily_pat, chunks, decode_times=args.decode_times, log=log, label=\"DA daily anoms\")\n",
    "    ds_ol, ds_da = xr.align(ds_ol, ds_da, join=\"exact\")\n",
    "\n",
    "    # after aligning ds_ol, ds_da\n",
    "    if not np.issubdtype(ds_ol.time.dtype, np.datetime64):\n",
    "        ds_ol = xr.decode_cf(ds_ol)\n",
    "        ds_da = xr.decode_cf(ds_da)\n",
    "\n",
    "    # A) Daily variability (full-period)\n",
    "    if not os.path.exists(f_daily):\n",
    "        daily_ds = daily_variance(ds_ol, ds_da, args.vars, args.eps, log)\n",
    "        write_nc(daily_ds, f_daily, chunks, log)\n",
    "    else:\n",
    "        log.info(f\"[skip] {f_daily} exists\")\n",
    "\n",
    "    # B) Within-month daily variance (seasonal)\n",
    "\n",
    "    # make time strictly increasing and unique on both\n",
    "    ds_ol = ensure_monotonic_unique_time(ds_ol)\n",
    "    ds_da = ensure_monotonic_unique_time(ds_da)\n",
    "    # re-align on the common time axis exactly\n",
    "    common = np.intersect1d(ds_ol.time.values, ds_da.time.values)\n",
    "    ds_ol = ds_ol.sel(time=common)\n",
    "    ds_da = ds_da.sel(time=common)\n",
    "    \n",
    "    if not os.path.exists(f_inmonth):\n",
    "        if not args.decode_times:\n",
    "            log.info(\"Decoding CF time for within-month ops …\")\n",
    "            ds_ol = xr.decode_cf(ds_ol); ds_da = xr.decode_cf(ds_da)\n",
    "        inmonth_ds = within_month_daily_variance(ds_ol, ds_da, args.vars, args.eps, log)\n",
    "        write_nc(inmonth_ds, f_inmonth, chunks, log)\n",
    "    else:\n",
    "        log.info(f\"[skip] {f_inmonth} exists\")\n",
    "\n",
    "    # C) Monthly-mean IAV\n",
    "    if not os.path.exists(f_mmiav):\n",
    "        if args.ol_monthly and args.da_monthly:\n",
    "            mmiav_ds = monthlymean_variance_from_monthfiles(args.ol_monthly, args.da_monthly,\n",
    "                                                            args.vars, chunks, args.eps, log)\n",
    "            # Try to attach lat/lon if missing\n",
    "            if \"lat\" not in mmiav_ds.coords or \"lon\" not in mmiav_ds.coords:\n",
    "                mmiav_ds = mmiav_ds.assign_coords(lat=ds_ol.lat, lon=ds_ol.lon)\n",
    "        else:\n",
    "            if not args.decode_times:\n",
    "                log.info(\"Decoding CF time for monthly-mean resample …\")\n",
    "                ds_ol = xr.decode_cf(ds_ol); ds_da = xr.decode_cf(ds_da)\n",
    "            mmiav_ds = monthlymean_variance_from_daily(ds_ol, ds_da, args.vars, args.eps, log)\n",
    "        write_nc(mmiav_ds, f_mmiav, chunks, log)\n",
    "    else:\n",
    "        log.info(f\"[skip] {f_mmiav} exists\")\n",
    "\n",
    "    # D) Annual-mean IAV\n",
    "    if not os.path.exists(f_aiav):\n",
    "        if not args.decode_times:\n",
    "            log.info(\"Decoding CF time for annual resample …\")\n",
    "            ds_ol = xr.decode_cf(ds_ol); ds_da = xr.decode_cf(ds_da)\n",
    "        aiav_ds = annualmean_variance(ds_ol, ds_da, args.vars, args.eps, log)\n",
    "        write_nc(aiav_ds, f_aiav, chunks, log)\n",
    "    else:\n",
    "        log.info(f\"[skip] {f_aiav} exists\")\n",
    "\n",
    "    log.info(\"All variance products done.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5944ded0-dd07-4241-b812-ad5d41461d2c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-diag]",
   "language": "python",
   "name": "conda-env-.conda-diag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}