{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80542446-293a-4e74-a428-2766de17725c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "\"\"\"\n",
    "Annual-batched daily OL/DA pipeline (streaming, resume-aware, simplified & fast)\n",
    "\n",
    "Pass A (resume-aware):\n",
    "  - Build a global keep_tile mask requiring ≥ MIN_VALID_FRAC valid days for BOTH OL and DA.\n",
    "  - Skips if keep_tile.nc already exists.\n",
    "\n",
    "Pass B (resume-aware):\n",
    "  - Build global OL (CNTL) daily climatology (DOY=1..366) over kept tiles, then apply cyclic smoothing.\n",
    "  - Skips if OLv8_climatology_DOY_smooth_kept.nc already exists.\n",
    "\n",
    "Pass C (resume-aware):\n",
    "  - Write per-year OL/DA anomaly files using the global climatology and keep_tile subset.\n",
    "  - Skips any year whose outputs already exist.\n",
    "\n",
    "Outputs (in --outdir):\n",
    "  - keep_tile.nc\n",
    "  - OLv8_climatology_DOY_smooth_kept.nc\n",
    "  - OLv8_daily_anomalies_kept_YYYY.nc\n",
    "  - DAv8_daily_anomalies_kept_YYYY.nc\n",
    "\"\"\"\n",
    "\n",
    "import os, re, glob, time, argparse, logging\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import dask\n",
    "from dask.diagnostics import ProgressBar\n",
    "from dask.utils import SerializableLock\n",
    "\n",
    "# -----------------------------\n",
    "# Basic config / tunables\n",
    "# -----------------------------\n",
    "DEFAULT_READ_ENGINE  = \"netcdf4\"\n",
    "DEFAULT_WRITE_ENGINE = \"h5netcdf\"\n",
    "_DEFAULT_CHUNKS = {\"time\": 31, \"tile\": 16384}\n",
    "\n",
    "TEMP_THRESH_K   = 275.15   # 2 °C\n",
    "SNOW_EPS        = 1e-2     # 1% snow cover\n",
    "MIN_VALID_FRAC  = 0.7\n",
    "_TS_RE = re.compile(r\"\\.(\\d{8})_1200z\\.nc4$\")   # ...YYYYMMDD_1200z.nc4\n",
    "\n",
    "# Dask setup: threads play nicer with HDF5\n",
    "_H5_LOCK = SerializableLock()\n",
    "dask.config.set({\n",
    "    \"scheduler\": \"threads\",\n",
    "    \"array.slicing.split_large_chunks\": True,\n",
    "    \"optimization.fuse.active\": True\n",
    "})\n",
    "os.environ.setdefault(\"HDF5_USE_FILE_LOCKING\", \"FALSE\")\n",
    "\n",
    "# -----------------------------\n",
    "# Logging\n",
    "# -----------------------------\n",
    "def setup_logger(verbosity: int = 1) -> logging.Logger:\n",
    "    level = logging.INFO if verbosity == 1 else (logging.DEBUG if verbosity >= 2 else logging.WARNING)\n",
    "    logging.basicConfig(level=level, format=\"%(asctime)s | %(levelname)s | %(message)s\",\n",
    "                        datefmt=\"%H:%M:%S\", force=True)\n",
    "    return logging.getLogger(\"daily-da-yearly\")\n",
    "\n",
    "def stamp(log, msg): log.info(msg)\n",
    "\n",
    "# -----------------------------\n",
    "# Helpers: discovery & opening\n",
    "# -----------------------------\n",
    "def _parse_ts(bname: str):\n",
    "    m = _TS_RE.search(bname)\n",
    "    if not m: return None\n",
    "    ymd = m.group(1)\n",
    "    return np.datetime64(f\"{ymd[:4]}-{ymd[4:6]}-{ymd[6:8]}T12:00\")\n",
    "\n",
    "def collect_daily_files(root_dir: str, file_prefix: str, start_date, end_date, log=None):\n",
    "    pattern = os.path.join(root_dir, \"**\", f\"{file_prefix}.tavg24_1d_lnd_Nt.*_1200z.nc4\")\n",
    "    hits = glob.glob(pattern, recursive=True)\n",
    "    if not hits: raise FileNotFoundError(f\"No *_1200z.nc4 under {root_dir} for {file_prefix}\")\n",
    "    start = np.datetime64(str(start_date), \"ns\"); end = np.datetime64(str(end_date), \"ns\")\n",
    "    files, times = [], []\n",
    "    for p in hits:\n",
    "        ts = _parse_ts(os.path.basename(p))\n",
    "        if ts is not None and start <= ts <= end:\n",
    "            files.append(p); times.append(ts)\n",
    "    if not files: raise FileNotFoundError(f\"No daily files for {file_prefix} within [{start_date}..{end_date}]\")\n",
    "    order = np.argsort(np.asarray(times))\n",
    "    files = [files[i] for i in order]\n",
    "    times = np.asarray(times, dtype=\"datetime64[ns]\")[order]\n",
    "    if log:\n",
    "        stamp(log, f\"[{file_prefix}] in-range files: {len(files)} / hits: {len(hits)}\")\n",
    "        stamp(log, f\"[{file_prefix}] time span: {str(times[0])} … {str(times[-1])}\")\n",
    "    return files, times\n",
    "\n",
    "def split_by_year(files, times):\n",
    "    years = np.array([int(str(t)[:4]) for t in times])\n",
    "    out = {}\n",
    "    for y in np.unique(years):\n",
    "        idx = np.where(years == y)[0]\n",
    "        out[y] = ([files[i] for i in idx], times[idx])\n",
    "    return out\n",
    "\n",
    "def speed_open_mfdataset(files, varnames, engine=\"netcdf4\", chunks=None, log=None):\n",
    "    if chunks is None: chunks = _DEFAULT_CHUNKS\n",
    "    want = set(varnames)\n",
    "    def _pre(ds):\n",
    "        keep = [v for v in ds.variables if v in want]\n",
    "        if not keep: raise KeyError(f\"Requested vars missing. Asked: {varnames}\")\n",
    "        return ds[keep]\n",
    "    if log: stamp(log, f\"[open] {len(files)} files …\")\n",
    "    t0 = time.perf_counter()\n",
    "    ds = xr.open_mfdataset(files, combine=\"nested\", concat_dim=\"time\", preprocess=_pre,\n",
    "                           engine=engine, parallel=False, lock=_H5_LOCK, chunks=chunks,\n",
    "                           data_vars=\"minimal\", coords=\"minimal\", compat=\"override\",\n",
    "                           mask_and_scale=False, decode_times=False, decode_coords=False,\n",
    "                           use_cftime=False)\n",
    "    if log: stamp(log, f\"[open] done in {time.perf_counter()-t0:.1f}s\")\n",
    "    return ds\n",
    "\n",
    "def _open_first_for_latlon(path: str, engine: str = \"netcdf4\"):\n",
    "    with xr.open_dataset(path, engine=engine, chunks={}) as ds0:\n",
    "        lat = ds0[\"lat\"].values; lon = ds0[\"lon\"].values\n",
    "    return lat, lon\n",
    "\n",
    "def batched(seq, n):\n",
    "    for i in range(0, len(seq), n):\n",
    "        yield seq[i:i+n], slice(i, min(i+n, len(seq)))\n",
    "\n",
    "# -----------------------------\n",
    "# Masking & DOY\n",
    "# -----------------------------\n",
    "def apply_frozen_snow_mask(sm_da, tsoil, frsnow, temp_thresh=TEMP_THRESH_K, snow_eps=SNOW_EPS):\n",
    "    return sm_da.where(~((tsoil < temp_thresh) | (frsnow > snow_eps)))\n",
    "\n",
    "def mask_vars(ds: xr.Dataset, anom_vars, temp_k, snow_eps) -> xr.Dataset:\n",
    "    out = xr.Dataset(coords=ds.coords)\n",
    "    for v in anom_vars:\n",
    "        sm = ds[v]\n",
    "        tsoil  = ds[\"TSOIL1\"] if \"TSOIL1\" in ds else xr.full_like(sm, np.nan)\n",
    "        frsnow = ds[\"FRLANDSNO\"] if \"FRLANDSNO\" in ds else xr.full_like(sm, np.nan)\n",
    "        out[v] = apply_frozen_snow_mask(sm, tsoil, frsnow, temp_k, snow_eps)\n",
    "    return out\n",
    "\n",
    "def doy_index(da_time):\n",
    "    return xr.where(da_time.dt.dayofyear == 366, 365, da_time.dt.dayofyear)\n",
    "\n",
    "# -----------------------------\n",
    "# Write util (handles time/tile and dayofyear/tile)\n",
    "# -----------------------------\n",
    "def write_nc(ds: xr.Dataset, path: str, engine: str, chunks=None, log=None):\n",
    "    if chunks is None: chunks = _DEFAULT_CHUNKS\n",
    "    comp = dict(zlib=True, complevel=4)\n",
    "    encoding = {}\n",
    "    for v in ds.data_vars:\n",
    "        dims = ds[v].dims\n",
    "        if len(dims) == 2 and dims[-1] == \"tile\":\n",
    "            # Respect chunk hints if present\n",
    "            d0 = dims[0]\n",
    "            d0_len = int(ds.sizes.get(d0, 1))\n",
    "            tile_len = int(ds.sizes.get(\"tile\", 1))\n",
    "            d0_chunk = min(d0_len, int(chunks.get(d0, chunks.get(\"time\", 31))))\n",
    "            tile_chunk = min(tile_len, int(chunks.get(\"tile\", 16384)))\n",
    "            encoding[v] = {**comp, \"chunksizes\": (d0_chunk, tile_chunk)}\n",
    "    if log: stamp(log, f\"→ Writing {path}\")\n",
    "    delayed = ds.to_netcdf(path, engine=engine, encoding=encoding, compute=False)\n",
    "    with ProgressBar(): dask.compute(delayed)\n",
    "    if log: stamp(log, f\"✓ Wrote {path}\")\n",
    "\n",
    "# -----------------------------\n",
    "# Arg parsing\n",
    "# -----------------------------\n",
    "def parse_chunk_flag(s: str) -> dict:\n",
    "    out = {}\n",
    "    if not s: return out\n",
    "    for kv in s.split(\",\"):\n",
    "        if not kv.strip(): continue\n",
    "        k, v = kv.split(\":\"); out[k.strip()] = int(v.strip())\n",
    "    return out\n",
    "\n",
    "def parse_args(argv=None):\n",
    "    p = argparse.ArgumentParser(description=\"Annual-batched OL/DA anomalies with global keep_tile and climatology (streaming, resume-aware).\")\n",
    "    # Roots/prefixes\n",
    "    p.add_argument(\"--ol-root\", default=\"/discover/nobackup/projects/land_da/Experiment_archive/M21C_land_sweeper_OLv8_M36/LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg\")\n",
    "    p.add_argument(\"--ol-prefix\", default=\"LS_OLv8_M36\")\n",
    "    p.add_argument(\"--da-root\", default=\"/discover/nobackup/projects/land_da/M21C_land_sweeper/LS_DAv8_M36_v2/LS_DAv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/ens_avg\")\n",
    "    p.add_argument(\"--da-prefix\", default=\"LS_DAv8_M36\")\n",
    "    # Vars\n",
    "    p.add_argument(\"--vars\", nargs=\"+\", default=[\"SFMC\",\"RZMC\",\"PRECTOTCORRLAND\",\"FRLANDSNO\",\"TSOIL1\"])\n",
    "    p.add_argument(\"--anom-vars\", nargs=\"+\", default=[\"SFMC\",\"RZMC\"])\n",
    "    # Time & I/O\n",
    "    p.add_argument(\"--start-date\", default=\"2000-01-01\")\n",
    "    p.add_argument(\"--end-date\",   default=\"2024-12-31\")\n",
    "    p.add_argument(\"--read-engine\", choices=[\"h5netcdf\",\"netcdf4\"], default=DEFAULT_READ_ENGINE)\n",
    "    p.add_argument(\"--write-engine\", choices=[\"h5netcdf\",\"netcdf4\"], default=DEFAULT_WRITE_ENGINE)\n",
    "    p.add_argument(\"--chunks\", default=\"time:31,tile:8192\")\n",
    "    p.add_argument(\"--outdir\", default=\"./yearly_outputs\")\n",
    "    # Mask/clim\n",
    "    p.add_argument(\"--temp-K\", type=float, default=TEMP_THRESH_K)\n",
    "    p.add_argument(\"--snow-eps\", type=float, default=SNOW_EPS)\n",
    "    p.add_argument(\"--min-valid-frac\", type=float, default=MIN_VALID_FRAC)\n",
    "    p.add_argument(\"--clim-window\", type=int, default=31)\n",
    "    # Batching\n",
    "    p.add_argument(\"--batch-days\", type=int, default=60, help=\"Days per file batch in streaming steps (45–90 good).\")\n",
    "    # Verbosity\n",
    "    p.add_argument(\"--verbose\", type=int, default=1)\n",
    "    return p.parse_args(argv)\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "def main(argv=None):\n",
    "    args = parse_args(argv)\n",
    "    log = setup_logger(args.verbose)\n",
    "    chunks = parse_chunk_flag(args.chunks)\n",
    "    os.makedirs(args.outdir, exist_ok=True)\n",
    "\n",
    "    keep_path = os.path.join(args.outdir, \"keep_tile.nc\")\n",
    "    clim_path = os.path.join(args.outdir, \"OLv8_climatology_DOY_smooth_kept.nc\")\n",
    "\n",
    "    stamp(log, \"=== Pipeline start ===\")\n",
    "    files_ol, times_ol = collect_daily_files(args.ol_root, args.ol_prefix, args.start_date, args.end_date, log)\n",
    "    files_da, times_da = collect_daily_files(args.da_root, args.da_prefix, args.start_date, args.end_date, log)\n",
    "    yearmap_ol = split_by_year(files_ol, times_ol)\n",
    "    yearmap_da = split_by_year(files_da, times_da)\n",
    "    years = sorted(set(yearmap_ol.keys()) & set(yearmap_da.keys()))\n",
    "    stamp(log, f\"Years with both OL & DA: {years[0]}–{years[-1]} ({len(years)} years)\")\n",
    "\n",
    "    lat, lon = _open_first_for_latlon(yearmap_ol[years[0]][0][0], args.read_engine)\n",
    "\n",
    "    # Peek to get ntiles\n",
    "    one_file = [yearmap_ol[years[0]][0][0]]\n",
    "    tmp = speed_open_mfdataset(one_file, args.vars, engine=args.read_engine, chunks=chunks, log=log)\n",
    "    ntiles = int(tmp.sizes[\"tile\"])\n",
    "    del tmp\n",
    "\n",
    "    # -------------------------\n",
    "    # PASS A: keep_tile (resume-aware)\n",
    "    # -------------------------\n",
    "    stamp(log, \"=== PASS A: valid-day keep mask ===\")\n",
    "    BATCH = max(1, int(args.batch_days))\n",
    "\n",
    "    if os.path.exists(keep_path):\n",
    "        stamp(log, f\"[A] Found existing keep mask → {keep_path} (skipping PASS A)\")\n",
    "        kds = xr.load_dataset(keep_path)\n",
    "        keep_tile = kds[\"keep_tile\"].values.astype(bool)\n",
    "        keep_idx  = np.where(keep_tile)[0]\n",
    "        n_keep = int(keep_tile.sum())\n",
    "        stamp(log, f\"[A] keep tiles: {n_keep}/{len(keep_tile)} (loaded)\")\n",
    "    else:\n",
    "        def accumulate_valid_for_filebatch(files, times):\n",
    "            ds = speed_open_mfdataset(files, args.vars + [\"TSOIL1\",\"FRLANDSNO\"], engine=args.read_engine, chunks=chunks, log=log)\n",
    "            ds = ds.assign_coords({\"time\": (\"time\", times), \"lat\": (\"tile\", lat), \"lon\": (\"tile\", lon)})\n",
    "            val = None\n",
    "            for v in args.anom_vars:\n",
    "                sm = ds[v]\n",
    "                tsoil = ds[\"TSOIL1\"] if \"TSOIL1\" in ds else xr.full_like(sm, np.nan)\n",
    "                frsn  = ds[\"FRLANDSNO\"] if \"FRLANDSNO\" in ds else xr.full_like(sm, np.nan)\n",
    "                ok    = apply_frozen_snow_mask(sm, tsoil, frsn, args.temp_K, args.snow_eps).notnull()\n",
    "                val   = ok if val is None else (val & ok)\n",
    "            cnt = val.astype(\"int8\").sum(\"time\", dtype=\"int64\")\n",
    "            with ProgressBar():\n",
    "                return cnt.compute().values, int(val.sizes[\"time\"])\n",
    "\n",
    "        accum_valid_ol = np.zeros((ntiles,), dtype=\"int64\")\n",
    "        accum_valid_da = np.zeros((ntiles,), dtype=\"int64\")\n",
    "        accum_days_ol = 0; accum_days_da = 0\n",
    "\n",
    "        for y in years:\n",
    "            f_ol, t_ol = yearmap_ol[y]; f_da, t_da = yearmap_da[y]\n",
    "            for fbatch, s in batched(f_ol, BATCH):\n",
    "                tbatch = t_ol[s]; cnt, days = accumulate_valid_for_filebatch(fbatch, tbatch)\n",
    "                accum_valid_ol += cnt; accum_days_ol += days\n",
    "            for fbatch, s in batched(f_da, BATCH):\n",
    "                tbatch = t_da[s]; cnt, days = accumulate_valid_for_filebatch(fbatch, tbatch)\n",
    "                accum_valid_da += cnt; accum_days_da += days\n",
    "            stamp(log, f\"[A] {y} done.\")\n",
    "\n",
    "        frac_ol = accum_valid_ol / max(1, accum_days_ol)\n",
    "        frac_da = accum_valid_da / max(1, accum_days_da)\n",
    "        keep_tile = (frac_ol >= args.min_valid_frac) & (frac_da >= args.min_valid_frac)\n",
    "        keep_idx = np.where(keep_tile)[0]\n",
    "        n_keep = int(keep_tile.sum())\n",
    "        stamp(log, f\"[A] keep tiles: {n_keep}/{ntiles}\")\n",
    "\n",
    "        keep_ds = xr.Dataset(dict(keep_tile=xr.DataArray(\n",
    "            keep_tile, dims=(\"tile\",),\n",
    "            coords={\"tile\": np.arange(ntiles), \"lat\": (\"tile\", lat), \"lon\": (\"tile\", lon)}\n",
    "        )))\n",
    "        write_nc(keep_ds, keep_path, args.write_engine, {\"tile\": _DEFAULT_CHUNKS[\"tile\"]}, log)\n",
    "\n",
    "    # -------------------------\n",
    "    # PASS B: climatology on kept tiles (resume-aware)\n",
    "    # -------------------------\n",
    "    stamp(log, \"=== PASS B: OL climatology (DOY) ===\")\n",
    "    DOY = 366\n",
    "    BATCH = max(1, int(args.batch_days))\n",
    "\n",
    "    if os.path.exists(clim_path):\n",
    "        stamp(log, f\"[B] Found existing climatology → {clim_path} (skipping PASS B)\")\n",
    "        clim_ds = xr.load_dataset(clim_path)\n",
    "    else:\n",
    "        clim_sums = {v: np.zeros((DOY, len(keep_idx)), dtype=np.float64) for v in args.anom_vars}\n",
    "        clim_cnts = {v: np.zeros((DOY, len(keep_idx)), dtype=np.int64)   for v in args.anom_vars}\n",
    "\n",
    "        def accumulate_doy_for_batch(files_batch, times_batch):\n",
    "            ds = speed_open_mfdataset(files_batch, args.vars, engine=args.read_engine, chunks=chunks, log=log)\n",
    "            ds = ds.assign_coords({\"time\": (\"time\", times_batch), \"lat\": (\"tile\", lat), \"lon\": (\"tile\", lon)})\n",
    "            olm = mask_vars(ds, args.anom_vars, args.temp_K, args.snow_eps).isel(tile=keep_idx)\n",
    "            # Group with 366→365 mapping (so Feb 29 merges with DOY 365), but we keep length 366 in reindex\n",
    "            doy = xr.where(olm.time.dt.dayofyear == 366, 365, olm.time.dt.dayofyear)\n",
    "            full = xr.DataArray(np.arange(1, 367), dims=\"dayofyear\", name=\"dayofyear\")\n",
    "            sums_b, cnts_b = {}, {}\n",
    "            for v in args.anom_vars:\n",
    "                g = olm[v].groupby(doy)\n",
    "                with ProgressBar():\n",
    "                    s = g.sum(\"time\", skipna=True).reindex(dayofyear=full, fill_value=0.0).compute()\n",
    "                    c = g.count(\"time\").reindex(dayofyear=full, fill_value=0).compute()\n",
    "                sums_b[v] = s.values  # (366, n_keep)\n",
    "                cnts_b[v] = c.values\n",
    "            return sums_b, cnts_b\n",
    "\n",
    "        for y in years:\n",
    "            f_ol, t_ol = yearmap_ol[y]\n",
    "            stamp(log, f\"[B] {y}\")\n",
    "            for fbatch, s in batched(f_ol, BATCH):\n",
    "                tbatch = t_ol[s]\n",
    "                sums_b, cnts_b = accumulate_doy_for_batch(fbatch, tbatch)\n",
    "                for v in args.anom_vars:\n",
    "                    clim_sums[v] += sums_b[v]\n",
    "                    clim_cnts[v] += cnts_b[v]\n",
    "\n",
    "        # If DOY=366 never observed, copy 365 over so length stays 366 and continuous\n",
    "        for v in args.anom_vars:\n",
    "            zero_366 = clim_cnts[v][-1, :] == 0\n",
    "            clim_sums[v][-1, zero_366] = clim_sums[v][-2, zero_366]\n",
    "            clim_cnts[v][-1, zero_366] = clim_cnts[v][-2, zero_366]\n",
    "\n",
    "        # Compute mean and apply circular smoothing that preserves length (366)\n",
    "        clim_vars = {}\n",
    "        for v in args.anom_vars:\n",
    "            count = clim_cnts[v]\n",
    "            summ  = clim_sums[v]\n",
    "            with np.errstate(invalid=\"ignore\", divide=\"ignore\"):\n",
    "                clim = summ / np.maximum(count, 1)\n",
    "\n",
    "            def cyclic_smooth_same_len(arr2d, window: int):\n",
    "                w = max(1, int(window))\n",
    "                if w == 1: return arr2d\n",
    "                left = w // 2\n",
    "                right = (w - 1) // 2\n",
    "                pad = np.pad(arr2d, ((left, right), (0,0)), mode=\"wrap\")\n",
    "                kernel = np.ones(w, dtype=np.float64) / w\n",
    "                out = np.apply_along_axis(lambda m: np.convolve(m, kernel, mode=\"valid\"), 0, pad)\n",
    "                return out  # (366, n_keep)\n",
    "\n",
    "            clim = cyclic_smooth_same_len(clim, args.clim_window)\n",
    "            clim_vars[f\"{v}_clim\"] = clim\n",
    "\n",
    "        coords = dict(dayofyear=np.arange(1, DOY+1),\n",
    "                      tile=(\"tile\", keep_idx),\n",
    "                      lat=(\"tile\", lat[keep_idx]),\n",
    "                      lon=(\"tile\", lon[keep_idx]))\n",
    "        clim_ds = xr.Dataset({k: ((\"dayofyear\",\"tile\"), v) for k, v in clim_vars.items()}, coords=coords)\n",
    "        clim_ds.attrs.update(baseline=\"CNTL masked DOY mean (global, smoothed)\",\n",
    "                             smoothing=f\"cyclic {args.clim_window}-day\")\n",
    "        write_nc(clim_ds, clim_path, args.write_engine, {\"dayofyear\": DOY, \"tile\": _DEFAULT_CHUNKS[\"tile\"]}, log)\n",
    "\n",
    "    # -------------------------\n",
    "    # PASS C: per-year anomalies (resume-aware)\n",
    "    # -------------------------\n",
    "    stamp(log, \"=== PASS C: yearly anomalies (OL & DA) ===\")\n",
    "    for y in years:\n",
    "        out_ol_y = os.path.join(args.outdir, f\"OLv8_daily_anomalies_kept_{y}.nc\")\n",
    "        out_da_y = os.path.join(args.outdir, f\"DAv8_daily_anomalies_kept_{y}.nc\")\n",
    "\n",
    "        if os.path.exists(out_ol_y) and os.path.exists(out_da_y):\n",
    "            stamp(log, f\"[C] {y}: outputs exist → skipping\")\n",
    "            continue\n",
    "\n",
    "        stamp(log, f\"[C] {y}\")\n",
    "        f_ol, t_ol = yearmap_ol[y]; f_da, t_da = yearmap_da[y]\n",
    "\n",
    "        ds_ol_y = speed_open_mfdataset(f_ol, args.vars, engine=args.read_engine, chunks=chunks, log=log)\n",
    "        ds_ol_y = ds_ol_y.assign_coords({\"time\": (\"time\", t_ol), \"lat\": (\"tile\", lat), \"lon\": (\"tile\", lon)}).isel(tile=clim_ds.tile.values)\n",
    "        ds_da_y = speed_open_mfdataset(f_da, args.vars, engine=args.read_engine, chunks=chunks, log=log)\n",
    "        ds_da_y = ds_da_y.assign_coords({\"time\": (\"time\", t_da), \"lat\": (\"tile\", lat), \"lon\": (\"tile\", lon)}).isel(tile=clim_ds.tile.values)\n",
    "\n",
    "        olm = mask_vars(ds_ol_y, args.anom_vars, args.temp_K, args.snow_eps)\n",
    "        dam = mask_vars(ds_da_y, args.anom_vars, args.temp_K, args.snow_eps)\n",
    "\n",
    "        doy = doy_index(olm.time)  # 366→365 mapping for Feb 29\n",
    "        an_ol = xr.Dataset(coords=dict(time=olm.time, tile=clim_ds.tile, lat=clim_ds.lat, lon=clim_ds.lon))\n",
    "        an_da = xr.Dataset(coords=dict(time=dam.time, tile=clim_ds.tile, lat=clim_ds.lat, lon=clim_ds.lon))\n",
    "        for v in args.anom_vars:\n",
    "            base = clim_ds[f\"{v}_clim\"].sel(dayofyear=doy).transpose(\"time\",\"tile\")\n",
    "            an_ol[v] = olm[v] - base\n",
    "            an_da[v] = dam[v] - base\n",
    "\n",
    "        # Write only missing outputs (resume-aware)\n",
    "        if not os.path.exists(out_ol_y):\n",
    "            write_nc(an_ol, out_ol_y, args.write_engine, chunks, log)\n",
    "        if not os.path.exists(out_da_y):\n",
    "            write_nc(an_da, out_da_y, args.write_engine, chunks, log)\n",
    "\n",
    "        del ds_ol_y, ds_da_y, olm, dam, an_ol, an_da\n",
    "\n",
    "    stamp(log, \"=== Done ===\")\n",
    "    stamp(log, f\" keep_tile: {keep_path}\")\n",
    "    stamp(log, f\" climatology: {clim_path}\")\n",
    "    stamp(log, f\" anomalies: {args.outdir}/OLv8_daily_anomalies_kept_YYYY.nc & DAv8_daily_anomalies_kept_YYYY.nc\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main([])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c898db7-9b7f-47fb-a49d-d9e218a54038",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-diag]",
   "language": "python",
   "name": "conda-env-.conda-diag-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}