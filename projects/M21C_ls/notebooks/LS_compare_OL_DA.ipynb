{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "\n",
    "# ---------- paths ----------\n",
    "ol_path = \"/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/land_sweeper/LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/OLv8_land_variables_2000_2024_compressed.nc\"\n",
    "da_path = \"/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/M21C_land_sweeper/LS_DAv8_M36_v2/LS_DAv8_M36/output/SMAP_EASEv2_M36_GLOBAL/cat/DAv8_land_variables_2000_2024_compressed.nc\"\n",
    "\n",
    "# ---------- load ----------\n",
    "ds_ol = xr.decode_cf(xr.open_dataset(ol_path))\n",
    "ds_da = xr.decode_cf(xr.open_dataset(da_path))\n",
    "\n",
    "# ---------- align months robustly with PeriodIndex ----------\n",
    "idx_ol = pd.to_datetime(ds_ol['time'].values).to_period('M')\n",
    "idx_da = pd.to_datetime(ds_da['time'].values).to_period('M')\n",
    "\n",
    "# optional: deduplicate months per side (keep first month occurrence)\n",
    "_, i_ol = np.unique(idx_ol.astype(str), return_index=True)\n",
    "_, i_da = np.unique(idx_da.astype(str), return_index=True)\n",
    "ds_ol = ds_ol.isel(time=np.sort(i_ol))\n",
    "ds_da = ds_da.isel(time=np.sort(i_da))\n",
    "idx_ol = pd.to_datetime(ds_ol['time'].values).to_period('M')\n",
    "idx_da = pd.to_datetime(ds_da['time'].values).to_period('M')\n",
    "\n",
    "# intersect months\n",
    "commonM = np.intersect1d(idx_ol.astype(str), idx_da.astype(str))\n",
    "assert commonM.size > 0, \"No overlapping months between OL and DA.\"\n",
    "\n",
    "m_ol = np.isin(idx_ol.astype(str), commonM)\n",
    "m_da = np.isin(idx_da.astype(str), commonM)\n",
    "\n",
    "ol = ds_ol['SFMC'].isel(time=np.where(m_ol)[0]).astype('float64')\n",
    "da = ds_da['SFMC'].isel(time=np.where(m_da)[0]).astype('float64')\n",
    "\n",
    "# assign identical end-of-month timestamps\n",
    "common_ts = pd.PeriodIndex(commonM, freq='M').to_timestamp('M')\n",
    "ol = ol.assign_coords(time=common_ts)\n",
    "da = da.assign_coords(time=common_ts)\n",
    "\n",
    "# ---------- optional: restrict to cells finite in both across all months ----------\n",
    "common_mask = np.isfinite(ol).all('time') & np.isfinite(da).all('time')\n",
    "ol = ol.where(common_mask)\n",
    "da = da.where(common_mask)\n",
    "\n",
    "# ---------- fixed climatology baseline (2001–2020) ----------\n",
    "clim_base = slice('2001-01-01','2020-12-31')\n",
    "ol_clim = ol.sel(time=clim_base).groupby('time.month').mean('time', skipna=True)\n",
    "da_clim = da.sel(time=clim_base).groupby('time.month').mean('time', skipna=True)\n",
    "\n",
    "# anomalies\n",
    "ol_anom = (ol.groupby('time.month') - ol_clim)\n",
    "da_anom = (da.groupby('time.month') - da_clim)\n",
    "\n",
    "# ---------- metrics ----------\n",
    "# mean difference\n",
    "mean_diff = (da.mean('time') - ol.mean('time')).astype('float64')\n",
    "mean_diff.name = \"mean_diff\"\n",
    "mean_diff.attrs.update(long_name=\"DA minus OL mean SFMC\", units=\"m3 m-3\")\n",
    "\n",
    "# guard against zero std\n",
    "std_ol = ol_anom.std('time')\n",
    "std_da = da_anom.std('time')\n",
    "safe_std = (std_ol > 0) & (std_da > 0)\n",
    "\n",
    "# variance ratio\n",
    "vr = (std_da / std_ol).where(safe_std)\n",
    "vr.name = \"variance_ratio\"\n",
    "vr.attrs.update(long_name=\"Std(DA_anom) / Std(OL_anom)\", units=\"1\")\n",
    "\n",
    "# percent change in amplitude\n",
    "pct_amp_change = ((vr - 1.0) * 100.0).where(np.isfinite(vr))\n",
    "pct_amp_change.name = \"pct_amp_change\"\n",
    "pct_amp_change.attrs.update(long_name=\"Percent change in anomaly std (DA vs OL)\", units=\"%\")\n",
    "\n",
    "# anomaly correlation\n",
    "cov = (da_anom * ol_anom).mean('time')\n",
    "den = np.sqrt((da_anom**2).mean('time') * (ol_anom**2).mean('time'))\n",
    "anomR = (cov / den).where(np.isfinite(den) & (den>0))\n",
    "anomR.name = \"anom_correlation\"\n",
    "anomR.attrs.update(long_name=\"Anomaly correlation (DA vs OL)\", units=\"1\")\n",
    "\n",
    "# ubRMSE on anomalies\n",
    "ubRMSE = np.sqrt(((da_anom - ol_anom)**2).mean('time'))\n",
    "ubRMSE = ubRMSE.where(np.isfinite(ubRMSE))\n",
    "ubRMSE.name = \"ubRMSE_anom\"\n",
    "ubRMSE.attrs.update(long_name=\"Unbiased RMSE of anomalies (DA vs OL)\", units=\"m3 m-3\")\n",
    "\n",
    "# normalized absolute difference\n",
    "nad = (np.abs(da - ol).mean('time') / std_ol).where(std_ol > 0)\n",
    "nad.name = \"normalized_abs_diff\"\n",
    "nad.attrs.update(long_name=\"Mean |DA-OL| normalized by Std(OL_anom)\", units=\"1\")\n",
    "\n",
    "# trend difference (simple OLS slope in m3/m3 per year)\n",
    "t_years = (ol['time'].dt.year + (ol['time'].dt.month - 0.5)/12).astype('float64')\n",
    "\n",
    "def slope_along_time(a, t=t_years):\n",
    "    t0 = t - t.mean().item()\n",
    "    num = (a * t0).sum('time')\n",
    "    den = (t0**2).sum('time')\n",
    "    return num / den\n",
    "\n",
    "beta_ol = slope_along_time(ol)\n",
    "beta_da = slope_along_time(da)\n",
    "dtrend  = (beta_da - beta_ol)\n",
    "dtrend.name = \"delta_trend\"\n",
    "dtrend.attrs.update(long_name=\"Trend difference: slope_DA - slope_OL\", units=\"m3 m-3 yr-1\")\n",
    "\n",
    "# lag-1 autocorrelation difference on anomalies\n",
    "def lag1_acorr(a):\n",
    "    a0 = a.isel(time=slice(0, -1))\n",
    "    a1 = a.isel(time=slice(1, None))\n",
    "    num = (a0 * a1).mean('time')\n",
    "    den = np.sqrt((a0**2).mean('time') * (a1**2).mean('time'))\n",
    "    return num / den\n",
    "\n",
    "rho1_ol = lag1_acorr(ol_anom)\n",
    "rho1_da = lag1_acorr(da_anom)\n",
    "drho1 = (rho1_da - rho1_ol)\n",
    "drho1.name = \"delta_rho1\"\n",
    "drho1.attrs.update(long_name=\"Lag-1 autocorrelation difference (DA-OL) on anomalies\", units=\"1\")\n",
    "\n",
    "# ---------- lat/lon coords if present ----------\n",
    "lat = ds_ol.coords.get('lat', None)\n",
    "lon = ds_ol.coords.get('lon', None)\n",
    "coords = {}\n",
    "if lat is not None and lon is not None:\n",
    "    coords[\"lat\"] = ((\"y\",\"x\"), lat.values, {\"standard_name\":\"latitude\",\"units\":\"degrees_north\"})\n",
    "    coords[\"lon\"] = ((\"y\",\"x\"), lon.values, {\"standard_name\":\"longitude\",\"units\":\"degrees_east\"})\n",
    "\n",
    "import datetime as dt\n",
    "\n",
    "# Figure out dims (tile vs y,x)\n",
    "dims = mean_diff.dims  # expect ('tile',) here\n",
    "\n",
    "coords = {}\n",
    "if 'tile' in dims and 'lat' in ds_ol.coords and 'lon' in ds_ol.coords:\n",
    "    # 1-D coords over 'tile'\n",
    "    coords['lat'] = ('tile', ds_ol['lat'].values,\n",
    "                     {'standard_name':'latitude','units':'degrees_north'})\n",
    "    coords['lon'] = ('tile', ds_ol['lon'].values,\n",
    "                     {'standard_name':'longitude','units':'degrees_east'})\n",
    "\n",
    "# Build the output dataset on the *tile* dimension\n",
    "ds_metrics = xr.Dataset(\n",
    "    data_vars={\n",
    "        \"mean_diff\":            mean_diff.astype(\"float32\"),\n",
    "        \"variance_ratio\":       vr.astype(\"float32\"),\n",
    "        \"pct_amp_change\":       (((vr - 1.0) * 100.0).where(np.isfinite(vr))).astype(\"float32\"),\n",
    "        \"anom_correlation\":     anomR.astype(\"float32\"),\n",
    "        \"ubRMSE_anom\":          ubRMSE.astype(\"float32\"),\n",
    "        \"normalized_abs_diff\":  nad.astype(\"float32\"),\n",
    "        \"delta_trend\":          dtrend.astype(\"float32\"),\n",
    "        \"delta_rho1\":           drho1.astype(\"float32\"),\n",
    "    },\n",
    "    coords=coords,\n",
    "    attrs={\n",
    "        \"title\": \"DA vs OL monthly SFMC comparison (tile space, M36 land tiles)\",\n",
    "        \"source\": \"GEOS-LDAS OLv8 and DAv8 monthly SFMC (time x tile)\",\n",
    "        \"climatology\": \"Anomalies relative to each run’s 2001–2020 monthly climatology\",\n",
    "        \"created_utc\": f\"{dt.datetime.utcnow():%Y-%m-%dT%H:%MZ}\",\n",
    "        \"conventions\": \"CF-1.8\",\n",
    "        \"notes\": \"Metrics computed per tile; lat/lon are 1-D tile coordinates.\",\n",
    "    },\n",
    ")\n",
    "\n",
    "# Compression\n",
    "enc = {v: {\"zlib\": True, \"complevel\": 4, \"_FillValue\": np.float32(np.nan)}\n",
    "       for v in ds_metrics.data_vars}\n",
    "if \"lat\" in ds_metrics.coords: enc[\"lat\"] = {\"zlib\": True, \"complevel\": 4}\n",
    "if \"lon\" in ds_metrics.coords: enc[\"lon\"] = {\"zlib\": True, \"complevel\": 4}\n",
    "\n",
    "out_nc = \"DA_vs_OL_SFMC_metrics_TILE.nc\"\n",
    "ds_metrics.to_netcdf(out_nc, format=\"NETCDF4\", encoding=enc)\n",
    "print(f\"Wrote {out_nc} (tile-space)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ======== HUMAN-READABLE SUMMARY + HOTSPOTS (prints + CSVs) ========\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def _flatten_with_coords(da):\n",
    "    \"\"\"Return 1D arrays of values, lat, lon from either (tile) or (y,x) DataArray.\"\"\"\n",
    "    if (\"y\" in da.dims) and (\"x\" in da.dims):\n",
    "        vals = da.values.ravel()\n",
    "        latv = ds_metrics[\"lat\"].values.ravel() if \"lat\" in ds_metrics.coords else np.full_like(vals, np.nan, dtype=float)\n",
    "        lonv = ds_metrics[\"lon\"].values.ravel() if \"lon\" in ds_metrics.coords else np.full_like(vals, np.nan, dtype=float)\n",
    "    elif (\"tile\" in da.dims):\n",
    "        vals = da.values\n",
    "        latv = ds_metrics[\"lat\"].values if \"lat\" in ds_metrics.coords else np.full_like(vals, np.nan, dtype=float)\n",
    "        lonv = ds_metrics[\"lon\"].values if \"lon\" in ds_metrics.coords else np.full_like(vals, np.nan, dtype=float)\n",
    "    else:\n",
    "        raise ValueError(\"Unexpected dims; need (y,x) or (tile).\")\n",
    "    return vals, latv, lonv\n",
    "\n",
    "def _nanmean(a): \n",
    "    return float(np.nanmean(a)) if np.size(a) else np.nan\n",
    "def _nanmedian(a): \n",
    "    return float(np.nanmedian(a)) if np.size(a) else np.nan\n",
    "def _nanpct(a, cond):\n",
    "    a = np.asarray(a); cond = np.asarray(cond)\n",
    "    ok = np.isfinite(a) & np.isfinite(cond)\n",
    "    if ok.any():\n",
    "        return float(100.0 * cond[ok].mean())\n",
    "    return np.nan\n",
    "\n",
    "# Pull arrays for summary\n",
    "md   = ds_metrics[\"mean_diff\"]\n",
    "vr   = ds_metrics[\"variance_ratio\"]\n",
    "pct  = ds_metrics[\"pct_amp_change\"]\n",
    "ar   = ds_metrics[\"anom_correlation\"]\n",
    "ub   = ds_metrics[\"ubRMSE_anom\"]\n",
    "nadv = ds_metrics[\"normalized_abs_diff\"]\n",
    "dtr  = ds_metrics[\"delta_trend\"]\n",
    "dr1  = ds_metrics[\"delta_rho1\"]\n",
    "\n",
    "# Flatten for simple stats\n",
    "md_v, lat_v, lon_v = _flatten_with_coords(md)\n",
    "vr_v, _, _         = _flatten_with_coords(vr)\n",
    "pct_v, _, _        = _flatten_with_coords(pct)\n",
    "ar_v, _, _         = _flatten_with_coords(ar)\n",
    "ub_v, _, _         = _flatten_with_coords(ub)\n",
    "nad_v, _, _        = _flatten_with_coords(nadv)\n",
    "dtr_v, _, _        = _flatten_with_coords(dtr)\n",
    "dr1_v, _, _        = _flatten_with_coords(dr1)\n",
    "\n",
    "# Thresholds you can tweak\n",
    "eps_mean   = 0.005      # m3/m3 \"no-meaningful-mean-change\"\n",
    "vr_tol     = 0.10       # ±10% amplitude change\n",
    "r_good     = 0.90       # anomaly correlation good\n",
    "ub_lim     = 0.02       # m3/m3 ubRMSE \"small\"\n",
    "trend_tol  = 5e-4       # m3/m3/yr\n",
    "rho1_tol   = 0.05       # lag-1 change\n",
    "\n",
    "summary = {\n",
    "    \"mean(|DA-OL|) [m3/m3]\":        _nanmean(np.abs(md_v)),\n",
    "    \"median(|DA-OL|) [m3/m3]\":      _nanmedian(np.abs(md_v)),\n",
    "    \"mean(variance_ratio)\":         _nanmean(vr_v),\n",
    "    \"median(variance_ratio)\":       _nanmedian(vr_v),\n",
    "    \"mean(pct_amp_change) [%]\":     _nanmean(pct_v),\n",
    "    \"median(pct_amp_change) [%]\":   _nanmedian(pct_v),\n",
    "    \"mean(anom_correlation)\":       _nanmean(ar_v),\n",
    "    \"median(anom_correlation)\":     _nanmedian(ar_v),\n",
    "    \"mean(ubRMSE_anom) [m3/m3]\":    _nanmean(ub_v),\n",
    "    \"mean(NAD)\":                    _nanmean(nad_v),\n",
    "    \"mean(delta_trend) [m3/m3/yr]\": _nanmean(dtr_v),\n",
    "    \"mean(delta_rho1)\":             _nanmean(dr1_v),\n",
    "    \"% |mean_diff| <= 0.005\":       _nanpct(md_v, np.abs(md_v) <= eps_mean),\n",
    "    \"% |VR-1| <= 0.1\":              _nanpct(vr_v, np.abs(vr_v - 1.0) <= vr_tol),\n",
    "    \"% anomR >= 0.9\":               _nanpct(ar_v, ar_v >= r_good),\n",
    "    \"% ubRMSE < 0.02\":              _nanpct(ub_v, ub_v < ub_lim),\n",
    "    \"% |Δtrend| <= 5e-4\":           _nanpct(dtr_v, np.abs(dtr_v) <= trend_tol),\n",
    "    \"% |Δrho1| <= 0.05\":            _nanpct(dr1_v, np.abs(dr1_v) <= rho1_tol),\n",
    "}\n",
    "\n",
    "# Print a neat text summary\n",
    "print(\"\\n=== DA vs OL SFMC: Text Summary ===\")\n",
    "for k, v in summary.items():\n",
    "    print(f\"{k:>30s}: {v:.4g}\")\n",
    "\n",
    "# Build a small hotspots table (top changes)\n",
    "hotspots = []\n",
    "# Top |mean diff|\n",
    "idx_md = np.argsort(-np.abs(md_v))[:20]\n",
    "for i in idx_md:\n",
    "    hotspots.append((\"abs_mean_diff\", float(md_v[i]), float(vr_v[i]), float(ar_v[i]),\n",
    "                     float(ub_v[i]), float(pct_v[i]), float(dtr_v[i]), float(dr1_v[i]),\n",
    "                     float(lat_v[i]), float(lon_v[i])))\n",
    "\n",
    "# Worst anomaly correlation\n",
    "idx_ar = np.argsort(ar_v)[:20]\n",
    "for i in idx_ar:\n",
    "    hotspots.append((\"low_anomR\", float(md_v[i]), float(vr_v[i]), float(ar_v[i]),\n",
    "                     float(ub_v[i]), float(pct_v[i]), float(dtr_v[i]), float(dr1_v[i]),\n",
    "                     float(lat_v[i]), float(lon_v[i])))\n",
    "\n",
    "# Largest % amplitude change\n",
    "idx_pct = np.argsort(-np.abs(pct_v))[:20]\n",
    "for i in idx_pct:\n",
    "    hotspots.append((\"large_pct_amp_change\", float(md_v[i]), float(vr_v[i]), float(ar_v[i]),\n",
    "                     float(ub_v[i]), float(pct_v[i]), float(dtr_v[i]), float(dr1_v[i]),\n",
    "                     float(lat_v[i]), float(lon_v[i])))\n",
    "\n",
    "hot_df = pd.DataFrame(hotspots, columns=[\n",
    "    \"reason\",\"mean_diff\",\"variance_ratio\",\"anom_correlation\",\"ubRMSE_anom\",\n",
    "    \"pct_amp_change\",\"delta_trend\",\"delta_rho1\",\"lat\",\"lon\"\n",
    "])\n",
    "\n",
    "# Save CSVs\n",
    "pd.DataFrame([summary]).to_csv(\"DA_vs_OL_SFMC_global_summary.csv\", index=False)\n",
    "hot_df.to_csv(\"DA_vs_OL_SFMC_hotspots.csv\", index=False)\n",
    "\n",
    "print(\"\\nWrote:\")\n",
    "print(\" - DA_vs_OL_SFMC_global_summary.csv  (one-line global metrics)\")\n",
    "print(\" - DA_vs_OL_SFMC_hotspots.csv        (top changes with lat/lon)\")\n",
    "print(\"Done.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import struct\n",
    "\n",
    "def read_tilecoord(fname):\n",
    "    \"\"\"Read GEOS-LDAS tilecoord Fortran binary (little-endian).\"\"\"\n",
    "    int_precision = 'i'\n",
    "    float_precision = 'f'\n",
    "    machfmt = '<'\n",
    "    tile_coord = {}\n",
    "    with open(fname, 'rb') as ifp:\n",
    "        _ = struct.unpack(f'{machfmt}i', ifp.read(4))[0]\n",
    "        tile_coord['N_tile'] = struct.unpack(f'{machfmt}i', ifp.read(4))[0]\n",
    "        _ = struct.unpack(f'{machfmt}i', ifp.read(4))[0]\n",
    "        Nt = tile_coord['N_tile']\n",
    "        fields = ['tile_id','typ','pfaf','com_lon','com_lat','min_lon','max_lon',\n",
    "                  'min_lat','max_lat','i_indg','j_indg','frac_cell','frac_pfaf',\n",
    "                  'area','elev']\n",
    "        for field in fields:\n",
    "            _ = struct.unpack(f'{machfmt}i', ifp.read(4))[0]\n",
    "            dtype = int_precision if field in ['tile_id','typ','pfaf','i_indg','j_indg'] else float_precision\n",
    "            arr = np.frombuffer(ifp.read(Nt*4), dtype=f'{machfmt}{dtype}')\n",
    "            arr = arr.astype(np.float64 if dtype=='f' else np.int32)\n",
    "            tile_coord[field] = arr\n",
    "            _ = struct.unpack(f'{machfmt}i', ifp.read(4))[0]\n",
    "    return tile_coord\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geospatial_plotting import plot_region, REGION_BOUNDS\n",
    "\n",
    "ftc = '/Users/amfox/Desktop/GEOSldas_diagnostics/test_data/land_sweeper/LS_OLv8_M36/output/SMAP_EASEv2_M36_GLOBAL/rc_out/LS_OLv8_M36.ldas_tilecoord.bin'\n",
    "tc = read_tilecoord(ftc)\n",
    "print(f\"N_tile = {tc['N_tile']}\")\n",
    "\n",
    "n_tile = tc['N_tile']\n",
    "lat = tc['com_lat']\n",
    "lon = tc['com_lon']\n",
    "\n",
    "map_array = np.empty([n_tile, 3])\n",
    "map_array.fill(np.nan)\n",
    "map_array[:, 1] = lon\n",
    "map_array[:, 2] = lat\n",
    "\n",
    "# Keep your existing per-group plot behavior\n",
    "map_array[:, 0] = ds_metrics[\"anom_correlation\"].values\n",
    "\n",
    "maxval = np.nanmax(map_array[:, 0])\n",
    "minval = np.nanmin(map_array[:, 0])\n",
    "\n",
    "# Plot group map\n",
    "fig, ax = plot_region(\n",
    "    map_array,\n",
    "    region_bounds=REGION_BOUNDS['global'],\n",
    "    meanflag=True,\n",
    "    plot_title=(f'DAv8_M36 - OLv8_M36\\n'\n",
    "                f'(Max: {maxval:.3g} Min: {minval:.3g})'),\n",
    "    units='Normalized Abs Diff',\n",
    "    cmin=0.2,\n",
    "    cmax=1.0,\n",
    "\n",
    ")\n",
    "fig.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "regrid",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
