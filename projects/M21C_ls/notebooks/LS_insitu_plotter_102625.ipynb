{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.io as sio\n",
    "import mat73"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment name for output plots\n",
    "expt_name = 'LS' \n",
    "\n",
    "# List of experiment names\n",
    "experiment_names = ['LS_OLv8_M36','LS_DAv8_M36']\n",
    "\n",
    "# insitu_tag plus details of timestep and number of years analysed in matlab file name\n",
    "insitu_tag = '_SCAN_SM_1d_c1234smv_25yr'\n",
    "\n",
    "# Create MATLAB file names based on experiment names\n",
    "matlab_files = [f'../test_data/M21C_land_sweeper/Evaluation/InSitu/output/{name}'+ insitu_tag +'_stats.mat' for name in experiment_names]\n",
    "\n",
    "# Read the first MATLAB file to get the shape of Bias\n",
    "first_file = matlab_files[0]\n",
    "mat_contents = sio.loadmat(first_file)\n",
    "shape = mat_contents['Bias'].shape\n",
    "\n",
    "# Create empty arrays with the shape of Bias and number of experiments\n",
    "num_exp = len(matlab_files)\n",
    "Bias = np.zeros(shape + (num_exp,))\n",
    "BiasLO = np.zeros(shape + (num_exp,))\n",
    "BiasUP = np.zeros(shape + (num_exp,))\n",
    "RMSE = np.zeros(shape + (num_exp,))\n",
    "RMSELO = np.zeros(shape + (num_exp,))\n",
    "RMSEUP = np.zeros(shape + (num_exp,))\n",
    "R = np.zeros(shape + (num_exp,))\n",
    "RLO = np.zeros(shape + (num_exp,))\n",
    "RUP = np.zeros(shape + (num_exp,))\n",
    "absBias = np.zeros(shape + (num_exp,))\n",
    "absBiasLO = np.zeros(shape + (num_exp,))\n",
    "absBiasUP = np.zeros(shape + (num_exp,))\n",
    "anomR = np.zeros(shape + (num_exp,))\n",
    "anomRLO = np.zeros(shape + (num_exp,))\n",
    "anomRUP = np.zeros(shape + (num_exp,))\n",
    "ubRMSE = np.zeros(shape + (num_exp,))\n",
    "ubRMSELO = np.zeros(shape + (num_exp,))\n",
    "ubRMSEUP = np.zeros(shape + (num_exp,))\n",
    "\n",
    "# Read data from the MATLAB files\n",
    "for i, file in enumerate(matlab_files):\n",
    "    mat_contents = sio.loadmat(file)\n",
    "    Bias[:, :, i] = mat_contents['Bias']\n",
    "    BiasLO[:, :, i] = mat_contents['BiasLO']\n",
    "    BiasUP[:, :, i] = mat_contents['BiasUP']\n",
    "    RMSE[:, :, i] = mat_contents['RMSE']\n",
    "    RMSELO[:, :, i] = mat_contents['RMSELO']\n",
    "    RMSEUP[:, :, i] = mat_contents['RMSEUP']\n",
    "    R[:, :, i] = mat_contents['R']\n",
    "    RLO[:, :, i] = mat_contents['RLO']\n",
    "    RUP[:, :, i] = mat_contents['RUP']\n",
    "    absBias[:, :, i] = mat_contents['absBias']\n",
    "    absBiasLO[:, :, i] = mat_contents['absBiasLO']\n",
    "    absBiasUP[:, :, i] = mat_contents['absBiasUP']\n",
    "    anomR[:, :, i] = mat_contents['anomR']\n",
    "    anomRLO[:, :, i] = mat_contents['anomRLO']\n",
    "    anomRUP[:, :, i] = mat_contents['anomRUP']\n",
    "    ubRMSE[:, :, i] = mat_contents['ubRMSE']\n",
    "    ubRMSELO[:, :, i] = mat_contents['ubRMSELO']\n",
    "    ubRMSEUP[:, :, i] = mat_contents['ubRMSEUP']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get info about ubRMSE\n",
    "print(ubRMSE.shape)\n",
    "print(ubRMSE.dtype)\n",
    "\n",
    "num_sites = ubRMSE.shape[0]\n",
    "print(\"num_sites: \", num_sites)\n",
    "\n",
    "num_depths = ubRMSE.shape[1]\n",
    "print(\"num_depths: \", num_depths)\n",
    "\n",
    "num_expts = ubRMSE.shape[2]\n",
    "print(\"num_expts: \", num_expts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate R_mean, Bias, RMSE, and ubRMSE from R\n",
    "R_mean = np.around(np.nanmean(R, axis=0), decimals=2)\n",
    "print('R_mean: ', R_mean)\n",
    "R_std = np.around(np.nanstd(R, axis=0), decimals=3)\n",
    "num_sites_sr = np.sum(~np.isnan(R), axis=0)\n",
    "R_CI_LO = np.around(np.nanmean(RLO, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "print('R_CI_LO: ', R_CI_LO)\n",
    "R_CI_UP = np.around(np.nanmean(RUP, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "print('R_CI_UP: ', R_CI_UP)\n",
    "R_CI = np.array([-R_CI_LO, R_CI_UP])\n",
    "print('R_CI: ', R_CI)\n",
    "print('R_CI[:, 0, :]: ', R_CI[:, 0, :])\n",
    "\n",
    "anomR_mean = np.around(np.nanmean(anomR, axis=0), decimals=2)\n",
    "anomR_std = np.around(np.nanstd(anomR, axis=0), decimals=3)\n",
    "num_sites_sr = np.sum(~np.isnan(anomR), axis=0)\n",
    "anomR_CI_LO = np.around(np.nanmean(anomRLO, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "anomR_CI_UP = np.around(np.nanmean(anomRUP, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "anomR_CI = np.array([-anomR_CI_LO, anomR_CI_UP])\n",
    "print('anomR_mean: ', anomR_mean)\n",
    "\n",
    "Bias_mean = np.around(np.nanmean(Bias, axis=0), decimals=3)\n",
    "Bias_std = np.around(np.nanstd(Bias, axis=0), decimals=3)\n",
    "num_sites_sr = np.sum(~np.isnan(Bias), axis=0)\n",
    "Bias_CI_LO = np.around(np.nanmean(BiasLO, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "Bias_CI_UP = np.around(np.nanmean(BiasUP, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "Bias_CI = np.array([-Bias_CI_LO, Bias_CI_UP])\n",
    "\n",
    "absBias_mean = np.around(np.nanmean(absBias, axis=0), decimals=3)\n",
    "absBias_std = np.around(np.nanstd(absBias, axis=0), decimals=3)\n",
    "num_sites_sr = np.sum(~np.isnan(absBias), axis=0)\n",
    "absBias_CI_LO = np.around(np.nanmean(absBiasLO, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "absBias_CI_UP = np.around(np.nanmean(absBiasUP, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "absBias_CI = np.array([-absBias_CI_LO, absBias_CI_UP])\n",
    "\n",
    "RMSE_mean = np.around(np.nanmean(RMSE, axis=0), decimals=3)\n",
    "RMSE_std = np.around(np.nanstd(RMSE, axis=0), decimals=3)\n",
    "num_sites_sr = np.sum(~np.isnan(RMSE), axis=0)\n",
    "RMSE_CI_LO = np.around(np.nanmean(RMSELO, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "RMSE_CI_UP = np.around(np.nanmean(RMSEUP, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "RMSE_CI = np.array([-RMSE_CI_LO, RMSE_CI_UP])\n",
    "\n",
    "ubRMSE_mean = np.around(np.nanmean(ubRMSE, axis=0), decimals=3)\n",
    "ubRMSE_std = np.around(np.nanstd(ubRMSE, axis=0), decimals=3)\n",
    "num_sites_sr = np.sum(~np.isnan(ubRMSE), axis=0)\n",
    "ubRMSE_CI_LO = np.around(np.nanmean(ubRMSELO, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "ubRMSE_CI_UP = np.around(np.nanmean(ubRMSEUP, axis=0) / np.sqrt(num_sites_sr), decimals=4)\n",
    "ubRMSE_CI = np.array([-ubRMSE_CI_LO, ubRMSE_CI_UP])\n",
    "\n",
    "print('ubRMSE_mean: ', ubRMSE_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expt_labels = [\"LS_OL\", \"LS_DA\"]\n",
    "\n",
    "ind = np.arange(num_expts)\n",
    "\n",
    "title_fontsize = 20\n",
    "label_fontsize = 20\n",
    "y_tick_label_fontsize = 18\n",
    "\n",
    "# Create a figure with a 2x3 subplot grid\n",
    "fig, axs = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# Subplot 1: Surface R_mean\n",
    "axs[0, 0].bar(ind, R_mean[0, :num_expts], color=plt.rcParams['axes.prop_cycle'].by_key()['color'][:num_expts])\n",
    "axs[0, 0].errorbar(ind, R_mean[0, :num_expts], yerr=R_CI[:, 0, :num_expts], fmt='none', ecolor='grey', capsize=2)\n",
    "# axs[0, 0].set_xlabel('Experiments')\n",
    "axs[0, 0].set_ylabel(r'$R$ (-)', fontsize=label_fontsize)\n",
    "axs[0, 0].set_ylim(0.5, 0.7)\n",
    "axs[0, 0].set_yticks(np.arange(0.5, 0.7, 0.05))\n",
    "axs[0, 0].set_yticklabels([0.5, 0.55, 0.6, 0.65], fontsize=y_tick_label_fontsize)\n",
    "axs[0, 0].set_axisbelow(True)\n",
    "axs[0, 0].grid(axis='y', color='lightgrey')\n",
    "axs[0, 0].set_title(r'Surface $R$ (mean)', fontsize=title_fontsize)\n",
    "axs[0, 0].set_xticks(ind)\n",
    "#axs[0, 0].set_xticklabels(expt_labels[:num_expts], rotation=25, fontsize=14)\n",
    "axs[0, 0].set_xticklabels('', fontsize=1)\n",
    "\n",
    "# Subplot 2: anomR_mean\n",
    "axs[0, 1].bar(ind, anomR_mean[0, :num_expts], color=plt.rcParams['axes.prop_cycle'].by_key()['color'][:num_expts])\n",
    "axs[0, 1].errorbar(ind, anomR_mean[0, :num_expts], yerr=anomR_CI[:, 0, :num_expts], fmt='none', ecolor='grey', capsize=2)\n",
    "#axs[0, 1].set_xlabel('Experiments')\n",
    "axs[0, 1].set_ylabel('anomR (-)', fontsize=label_fontsize)\n",
    "axs[0, 1].set_ylim(0.5, 0.7)\n",
    "axs[0, 1].set_yticks(np.arange(0.5, 0.7, 0.05))\n",
    "axs[0, 1].set_yticklabels([0.5, 0.55, 0.6, 0.65], fontsize=y_tick_label_fontsize)\n",
    "axs[0, 1].set_axisbelow(True)\n",
    "axs[0, 1].grid(axis='y', color='lightgrey')\n",
    "axs[0, 1].set_title('Surface anomR (mean)', fontsize=title_fontsize)\n",
    "axs[0, 1].set_xticks(ind)\n",
    "# axs[0, 1].set_xticklabels(expt_labels[:num_expts], rotation=25, fontsize=14)\n",
    "axs[0, 1].set_xticklabels('', fontsize=1)\n",
    "\n",
    "# Subplot 3: Surface ubRMSE_mean\n",
    "axs[0, 2].bar(ind, ubRMSE_mean[0, :num_expts], color=plt.rcParams['axes.prop_cycle'].by_key()['color'][:num_expts])\n",
    "axs[0, 2].errorbar(ind, ubRMSE_mean[0, :num_expts], yerr=ubRMSE_CI[:, 0, :num_expts], fmt='none', ecolor='grey', capsize=2)\n",
    "# axs[0, 1].set_xlabel('Experiments')\n",
    "axs[0, 2].set_ylabel('ubRMSD ($m^3 \\, m^{-3}$)', fontsize=label_fontsize)\n",
    "axs[0, 2].set_ylim(0.03, 0.06)\n",
    "axs[0, 2].set_yticks(np.arange(0.03, 0.06, 0.005))\n",
    "axs[0, 2].set_yticklabels([0.03, ' ', 0.04, ' ', 0.05, ' '], fontsize=y_tick_label_fontsize)\n",
    "axs[0, 2].set_axisbelow(True)\n",
    "axs[0, 2].grid(axis='y', color='lightgrey')\n",
    "axs[0, 2].set_title('Surface ubRMSD (mean)', fontsize=title_fontsize)\n",
    "axs[0, 2].set_xticks(ind)\n",
    "# axs[0, 2].set_xticklabels(expt_labels[:num_expts], rotation=25, fontsize=14)\n",
    "axs[0, 2].set_xticklabels('', fontsize=1)\n",
    "\n",
    "# Subplot 4: Root zone R_mean\n",
    "axs[1, 0].bar(ind, R_mean[1, :num_expts], color=plt.rcParams['axes.prop_cycle'].by_key()['color'][:num_expts])\n",
    "axs[1, 0].errorbar(ind, R_mean[1, :num_expts], yerr=R_CI[:, 1, :num_expts], fmt='none', ecolor='grey', capsize=2)\n",
    "#axs[1, 0].set_xlabel('Experiments', fontsize=14)\n",
    "axs[1, 0].set_ylabel(r'$R$ (-)', fontsize=label_fontsize)\n",
    "axs[1, 0].set_ylim(0.5, 0.7)\n",
    "axs[1, 0].set_yticks(np.arange(0.5, 0.7, 0.05))\n",
    "axs[1, 0].set_yticklabels([0.5, 0.55, 0.6, 0.65], fontsize=y_tick_label_fontsize)\n",
    "axs[1, 0].set_axisbelow(True)\n",
    "axs[1, 0].grid(axis='y', color='lightgrey')\n",
    "axs[1, 0].set_title(r'Rootzone $R$ (mean)', fontsize=title_fontsize)\n",
    "axs[1, 0].set_xticks(ind)\n",
    "axs[1, 0].set_xticklabels(expt_labels[:num_expts], rotation=35, fontsize=label_fontsize)\n",
    "\n",
    "# Subplot 5: anomR_mean\n",
    "axs[1, 1].bar(ind, anomR_mean[1, :num_expts], color=plt.rcParams['axes.prop_cycle'].by_key()['color'][:num_expts])\n",
    "axs[1, 1].errorbar(ind, anomR_mean[1, :num_expts], yerr=anomR_CI[:, 1, :num_expts], fmt='none', ecolor='grey', capsize=2)\n",
    "#axs[1, 1].set_xlabel('Experiments', fontsize=14)\n",
    "axs[1, 1].set_ylabel('anomR (-)', fontsize=label_fontsize)\n",
    "axs[1, 1].set_ylim(0.5, 0.7)\n",
    "axs[1, 1].set_yticks(np.arange(0.5, 0.7, 0.05))\n",
    "axs[1, 1].set_yticklabels([0.5, 0.55, 0.6, 0.65], fontsize=y_tick_label_fontsize)\n",
    "axs[1, 1].set_axisbelow(True)\n",
    "axs[1, 1].grid(axis='y', color='lightgrey')\n",
    "axs[1, 1].set_title('Rootzone anomR (mean)', fontsize=title_fontsize)\n",
    "axs[1, 1].set_xticks(ind)\n",
    "axs[1, 1].set_xticklabels(expt_labels[:num_expts], rotation=35, fontsize=label_fontsize)\n",
    "\n",
    "# Subplot 6: Root zone ubRMSE_mean\n",
    "axs[1, 2].bar(ind, ubRMSE_mean[1, :num_expts], color=plt.rcParams['axes.prop_cycle'].by_key()['color'][:num_expts])\n",
    "axs[1, 2].errorbar(ind, ubRMSE_mean[1, :num_expts], yerr=ubRMSE_CI[:, 1, :num_expts], fmt='none', ecolor='grey', capsize=2)\n",
    "#axs[1, 2].set_xlabel('Experiments', fontsize=14)\n",
    "axs[1, 2].set_ylabel('ubRMSD ($m^3 \\, m^{-3}$)', fontsize=label_fontsize)\n",
    "axs[1, 2].set_ylim(0.03, 0.06)\n",
    "axs[1, 2].set_yticks(np.arange(0.03, 0.06, 0.005))\n",
    "axs[1, 2].set_yticklabels([0.03, ' ', 0.04, ' ', 0.05, ' '], fontsize=y_tick_label_fontsize)\n",
    "axs[1, 2].set_axisbelow(True)\n",
    "axs[1, 2].grid(axis='y', color='lightgrey')\n",
    "axs[1, 2].set_title('Rootzone ubRMSD (mean)', fontsize=title_fontsize)\n",
    "axs[1, 2].set_xticks(ind)\n",
    "axs[1, 2].set_xticklabels(expt_labels[:num_expts], rotation=35, fontsize=label_fontsize)\n",
    "\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.savefig(expt_name + '_surf_rz_stats.png')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rs_file = '../test_data/M21C_land_sweeper/Evaluation/InSitu/output/LS_OLv8_M36_SCAN_SM_1d_c1234smv_25yr_raw_timeseries.mat'\n",
    "mat_contents = sio.loadmat(m_rs_file)\n",
    "\n",
    "# List of variables and their dimensions in the MATLAB file\n",
    "print(sio.whosmat(m_rs_file))\n",
    "\n",
    "vars = [k for k in mat_contents.keys() if not k.startswith('__')]\n",
    "print('Variables in MAT file:')\n",
    "for name in vars:\n",
    "    val = mat_contents[name]\n",
    "    shp = getattr(val, 'shape', None)\n",
    "    print(f\"{name}: type={type(val).__name__}, shape={shp}\")\n",
    "\n",
    "# ...existing code...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rs_file = '../test_data/M21C_land_sweeper/Evaluation/InSitu/output/LS_OLv8_M36_SCAN_SM_1d_c1234smv_25yr_raw_timeseries.mat'\n",
    "\n",
    "def unwrap(x):\n",
    "    while isinstance(x, np.ndarray) and x.size == 1:\n",
    "        x = x.ravel()[0]\n",
    "    return x\n",
    "\n",
    "\n",
    "# load mat (squeeze_me to remove singleton dims, struct_as_record for easier struct access)\n",
    "mat = sio.loadmat(m_rs_file)\n",
    "\n",
    "INSITU_sm = mat.get('INSITU_sm')\n",
    "INSITU_id = mat.get('INSITU_id')\n",
    "LDAS_sm_OL = mat.get('LDAS_sm_org')\n",
    "\n",
    "print(\"INSITU_sm type/shape:\", type(INSITU_sm), None if INSITU_sm is None else INSITU_sm.shape)\n",
    "print(\"INSITU_id type/shape:\", type(INSITU_id), None if INSITU_id is None else np.atleast_1d(INSITU_id).shape)\n",
    "\n",
    "\n",
    "dt_array = mat['date_time_vec']  # existing loaded variable\n",
    "n = dt_array.size\n",
    "\n",
    "timestamps = []\n",
    "for i in range(n):\n",
    "    e = unwrap(dt_array.ravel()[i])\n",
    "    names = e.dtype.names\n",
    "    # safe extraction with defaults\n",
    "    y = int(np.squeeze(e['year'])) if 'year' in names else 0\n",
    "    mo = int(np.squeeze(e['month'])) if 'month' in names else 1\n",
    "    d = int(np.squeeze(e['day'])) if 'day' in names else 1\n",
    "    hr = int(np.squeeze(e['hour'])) if 'hour' in names else 0\n",
    "    mn = int(np.squeeze(e['min'])) if 'min' in names else 0\n",
    "    sc = int(np.squeeze(e['sec'])) if 'sec' in names else 0\n",
    "    timestamps.append(datetime(y, mo, d, hr, mn, sc))\n",
    "\n",
    "# example: first 5 timestamps\n",
    "print(timestamps[:5])\n",
    "\n",
    "print(\"number of timestamps:\", len(timestamps))\n",
    "if len(timestamps) > 0:\n",
    "    print(\"first timestamp:\", timestamps[0], \"last timestamp:\", timestamps[-1])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.dates import DateFormatter, AutoDateLocator\n",
    "import numpy as np\n",
    "\n",
    "assert 'INSITU_sm' in globals(), \"INSITU_sm not loaded\"\n",
    "assert 'timestamps' in globals(), \"timestamps not available\"\n",
    "\n",
    "n_time = INSITU_sm.shape[0]\n",
    "n_sites_plot = min(5, INSITU_sm.shape[2])\n",
    "timestamps_arr = np.array(timestamps)  # list of datetimes -> works with matplotlib\n",
    "\n",
    "# prepare ids if available\n",
    "ids = []\n",
    "if 'INSITU_id' in globals():\n",
    "    ids_arr = np.atleast_1d(INSITU_id).ravel()\n",
    "    for a in ids_arr[:n_sites_plot]:\n",
    "        v = a\n",
    "        if isinstance(v, np.ndarray): v = np.squeeze(v)\n",
    "        if isinstance(v, bytes): v = v.decode('utf-8')\n",
    "        ids.append(str(v))\n",
    "else:\n",
    "    ids = [str(i) for i in range(n_sites_plot)]\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(n_sites_plot):\n",
    "    series = INSITU_sm[:, 0, i].astype(float)  # first SM value\n",
    "    plt.plot(timestamps_arr, series, label=ids[i])\n",
    "\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('SM (first value)')\n",
    "plt.title('First SM value for first 5 sites')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "ax = plt.gca()\n",
    "ax.xaxis.set_major_locator(AutoDateLocator())\n",
    "ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "plt.xticks(rotation=30, ha='right')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "n_times, _, n_sites = INSITU_sm.shape\n",
    "\n",
    "ids = []\n",
    "if 'INSITU_id' in globals():\n",
    "    ids_arr = np.atleast_1d(INSITU_id).ravel()\n",
    "    for a in ids_arr[:n_sites]:\n",
    "        v = a\n",
    "        if isinstance(v, np.ndarray): v = np.squeeze(v)\n",
    "        if isinstance(v, bytes): v = v.decode('utf-8')\n",
    "        ids.append(str(v))\n",
    "else:\n",
    "    ids = [str(i) for i in range(n_sites)]\n",
    "\n",
    "assert 'timestamps' in globals(), \"timestamps not available\"\n",
    "\n",
    "# compute valid mask per depth\n",
    "valid_d0 = ~np.isnan(INSITU_sm[:, 0, :])             # shape (ntime, n_sites)\n",
    "valid_d1 = ~np.isnan(INSITU_sm[:, 1, :]) if INSITU_sm.shape[1] > 1 else np.zeros((n_times, n_sites), dtype=bool)\n",
    "\n",
    "# prepare storage\n",
    "first_dates_d0 = np.empty(n_sites, dtype=object)\n",
    "last_dates_d0  = np.empty(n_sites, dtype=object)\n",
    "first_idx_d0   = -np.ones(n_sites, dtype=int)\n",
    "last_idx_d0    = -np.ones(n_sites, dtype=int)\n",
    "\n",
    "first_dates_d1 = np.empty(n_sites, dtype=object)\n",
    "last_dates_d1  = np.empty(n_sites, dtype=object)\n",
    "first_idx_d1   = -np.ones(n_sites, dtype=int)\n",
    "last_idx_d1    = -np.ones(n_sites, dtype=int)\n",
    "\n",
    "for j in range(n_sites):\n",
    "    idxs0 = np.where(valid_d0[:, j])[0]\n",
    "    if idxs0.size == 0:\n",
    "        first_dates_d0[j] = None; last_dates_d0[j] = None\n",
    "        first_idx_d0[j] = -1; last_idx_d0[j] = -1\n",
    "    else:\n",
    "        first_idx_d0[j] = int(idxs0[0]); last_idx_d0[j] = int(idxs0[-1])\n",
    "        first_dates_d0[j] = timestamps[first_idx_d0[j]]; last_dates_d0[j] = timestamps[last_idx_d0[j]]\n",
    "\n",
    "    idxs1 = np.where(valid_d1[:, j])[0]\n",
    "    if idxs1.size == 0:\n",
    "        first_dates_d1[j] = None; last_dates_d1[j] = None\n",
    "        first_idx_d1[j] = -1; last_idx_d1[j] = -1\n",
    "    else:\n",
    "        first_idx_d1[j] = int(idxs1[0]); last_idx_d1[j] = int(idxs1[-1])\n",
    "        first_dates_d1[j] = timestamps[first_idx_d1[j]]; last_dates_d1[j] = timestamps[last_idx_d1[j]]\n",
    "\n",
    "# print concise summary\n",
    "for j in range(n_sites):\n",
    "    idstr = ids[j] if j < len(ids) else str(j)\n",
    "    d0 = (\"no obs\" if first_dates_d0[j] is None else f\"{first_dates_d0[j]} -> {last_dates_d0[j]}\")\n",
    "    d1 = (\"no obs\" if first_dates_d1[j] is None else f\"{first_dates_d1[j]} -> {last_dates_d1[j]}\")\n",
    "    print(f\"site {j} ({idstr}): depth1 = {d0}; depth2 = {d1}\")\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "import numpy as np\n",
    "\n",
    "# 1) ensure inputs present and aligned\n",
    "assert 'timestamps' in globals(), \"timestamps not available\"\n",
    "assert 'INSITU_sm' in globals(), \"INSITU_sm not loaded\"\n",
    "timestamps_arr = np.array(timestamps)\n",
    "n_times, n_depths, n_sites = INSITU_sm.shape\n",
    "assert timestamps_arr.shape[0] == n_times, \"timestamps length != INSITU_sm time dimension\"\n",
    "\n",
    "# 2) date-only array for grouping by calendar day\n",
    "dates_only = np.array([ts.date() for ts in timestamps_arr])\n",
    "\n",
    "# 3) storage for results\n",
    "days_counts = np.zeros((n_depths, n_sites), dtype=int)\n",
    "unique_days = [[None for _ in range(n_sites)] for _ in range(n_depths)]\n",
    "\n",
    "# 4) loop depths/sites: mask non-NaN times, get unique dates, count\n",
    "for d in range(n_depths):\n",
    "    for j in range(n_sites):\n",
    "        valid_mask = ~np.isnan(INSITU_sm[:, d, j])            # times with obs at this depth/site\n",
    "        if not np.any(valid_mask):\n",
    "            unique_days[d][j] = np.array([], dtype=object)\n",
    "            days_counts[d, j] = 0\n",
    "        else:\n",
    "            days = np.unique(dates_only[valid_mask])         # unique calendar days with at least one obs\n",
    "            unique_days[d][j] = days\n",
    "            days_counts[d, j] = days.size\n",
    "\n",
    "# 5) example summary for first 10 sites\n",
    "for j in range(min(10, n_sites)):\n",
    "    idstr = ids[j] if j < len(ids) else str(j)\n",
    "    counts_str = ', '.join(str(int(days_counts[d, j])) for d in range(n_depths))\n",
    "    print(f\"site {j} ({idstr}): days per depth = [{counts_str}]\")\n",
    "# ...existing code...\n",
    "\n",
    "# list the 20 sites with the most days of observations at depth 0\n",
    "n_top = 10  \n",
    "depth = 0\n",
    "top_indices = np.argsort(-days_counts[depth, :])[:n_top]  # negative for descending sort\n",
    "print(f\"Top {n_top} sites with most days of observations at depth {depth}:\")\n",
    "for rank, j in enumerate(top_indices, start=1):\n",
    "    idstr = ids[j] if j < len(ids) else str(j)\n",
    "    count = int(days_counts[depth, j])\n",
    "    first_date = unique_days[depth][j][0] if count > 0 else None\n",
    "    last_date = unique_days[depth][j][-1] if count > 0 else None\n",
    "    print(f\"{rank:2d}. site {j} ({idstr}): {count} days from {first_date} to {last_date}\")\n",
    "\n",
    "# For each site want to print how many days of obs between 2000-06-01 and 2007-06-01 and then\n",
    "# between 2007-06-01 and 2015-04-01, and then between 2015-04-01 and 2024-06-31\n",
    "\n",
    "for rank, j in enumerate(top_indices, start=1):\n",
    "    idstr = ids[j] if j < len(ids) else str(j)\n",
    "    days = unique_days[depth][j]\n",
    "    count1 = np.sum((days >= datetime(2000, 6, 1).date()) & (days < datetime(2007, 6, 1).date()))\n",
    "    count2 = np.sum((days >= datetime(2007, 6, 1).date()) & (days < datetime(2015, 4, 1).date()))\n",
    "    count3 = np.sum((days >= datetime(2015, 4, 1).date()) & (days <= datetime(2024, 6, 30).date()))\n",
    "    print(f\"{rank:2d}. site {j} ({idstr}): {count1} days from 2000-06-01 to 2007-06-01; \"\n",
    "          f\"{count2} days from 2007-06-01 to 2015-04-01; {count3} days from 2015-04-01 to 2024-06-30\")\n",
    "\n",
    "# Plot a time series of SM at these sites\n",
    "\n",
    "for rank, j in enumerate(top_indices, start=1):\n",
    "    series = INSITU_sm[:, depth, j].astype(float)  # SM at selected depth\n",
    "    plt.figure(figsize=(18, 6))\n",
    "    plt.plot(timestamps_arr, series, label=f\"{rank}. {ids[j] if j < len(ids) else str(j)}\")\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('SM')\n",
    "    plt.title(f'SM time series site {j} ({idstr}) at depth {depth}')\n",
    "    plt.legend(fontsize='small', ncol=2)\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()    \n",
    "\n",
    "# Repeat for depth 0\n",
    "depth = 1\n",
    "top_indices = np.argsort(-days_counts[depth, :])[:n_top]  # negative for descending sort\n",
    "print(f\"Top {n_top} sites with most days of observations at depth {depth}:\")\n",
    "for rank, j in enumerate(top_indices, start=1):\n",
    "    idstr = ids[j] if j < len(ids) else str(j)\n",
    "    count = int(days_counts[depth, j])\n",
    "    first_date = unique_days[depth][j][0] if count > 0 else None\n",
    "    last_date = unique_days[depth][j][-1] if count > 0 else None\n",
    "    print(f\"{rank:2d}. site {j} ({idstr}): {count} days from {first_date} to {last_date}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import numpy as np\n",
    "\n",
    "# --- config ---\n",
    "depth = 0                  # 0=surface, 1=rootzone (adjust as needed)\n",
    "top_n_total = 30\n",
    "periods = [\n",
    "    (\"P1\", datetime(2000, 6, 1).date(), datetime(2007, 6, 1).date()),   # [start, end)\n",
    "    (\"P2\", datetime(2007, 6, 1).date(), datetime(2015, 4, 1).date()),\n",
    "    (\"P3\", datetime(2015, 4, 1).date(), datetime(2024, 6, 30).date()),  # inclusive end for your use case\n",
    "]\n",
    "\n",
    "# --- compute counts per site, per period ---\n",
    "counts_by_site = []  # (site_idx, site_id, counts[3], total, balance_std)\n",
    "\n",
    "for j in range(n_sites):\n",
    "    days = unique_days[depth][j]\n",
    "    if days is None or len(days) == 0:\n",
    "        continue\n",
    "\n",
    "    c = np.array([\n",
    "        np.sum((days >= p[1]) & (days < p[2])) if i < 2\n",
    "        else np.sum((days >= p[1]) & (days <= p[2]))  # P3 inclusive end\n",
    "        for i, p in enumerate(periods)\n",
    "    ], dtype=int)\n",
    "\n",
    "    total = int(c.sum())\n",
    "    balance = float(np.std(c.astype(float)))\n",
    "    sid = ids[j] if j < len(ids) else str(j)\n",
    "    counts_by_site.append((j, sid, c, total, balance))\n",
    "\n",
    "# --- pick top N by total coverage ---\n",
    "top_by_total = sorted(counts_by_site, key=lambda x: -x[3])[:top_n_total]\n",
    "\n",
    "# --- within those, sort by balance (ascending) ---\n",
    "top_sorted_by_balance = sorted(top_by_total, key=lambda x: x[4])\n",
    "\n",
    "# --- print ---\n",
    "print(f\"Top {len(top_sorted_by_balance)} sites by TOTAL coverage, sorted by BALANCE among the top:\")\n",
    "for rank, (j, sid, c, total, balance) in enumerate(top_sorted_by_balance, start=1):\n",
    "    print(f\"{rank:2d}. site {j:4d} ({sid}): \"\n",
    "          f\"{periods[0][0]}={c[0]}, {periods[1][0]}={c[1]}, {periods[2][0]}={c[2]}, \"\n",
    "          f\"total={total}, balance_std={balance:.1f}\")\n",
    "    \n",
    "# Save the indices of the top 10 balanced sites for depth 0 for later use\n",
    "top_balanced_indices_depth0 = [x[0] for x in top_sorted_by_balance[:15]]\n",
    "print(\"Top balanced site indices at depth 0:\", top_balanced_indices_depth0)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "depth = 0                      # 0=surface, 1=rootzone\n",
    "min_days_per_period = 1000\n",
    "\n",
    "# Reuse your existing periods (same as earlier cell)\n",
    "periods = [\n",
    "    (\"P1\", datetime(2000, 6, 1).date(), datetime(2007, 6, 1).date()),   # [start, end)\n",
    "    (\"P2\", datetime(2007, 6, 1).date(), datetime(2015, 4, 1).date()),   # [start, end)\n",
    "    (\"P3\", datetime(2015, 4, 1).date(), datetime(2024, 6, 30).date()),  # inclusive end\n",
    "]\n",
    "\n",
    "qualified = []   # list of (site_idx, site_id, P1, P2, P3, total)\n",
    "\n",
    "for j in range(n_sites):\n",
    "    days = unique_days[depth][j]\n",
    "    if days is None or len(days) == 0:\n",
    "        continue\n",
    "    days = np.asarray(days)  # ensure numpy array of datetime.date\n",
    "\n",
    "    # Count using same inclusivity as your previous code\n",
    "    counts = np.array([\n",
    "        np.count_nonzero((days >= p[1]) & (days < p[2])) if i < 2\n",
    "        else np.count_nonzero((days >= p[1]) & (days <= p[2]))  # P3 inclusive end\n",
    "        for i, p in enumerate(periods)\n",
    "    ], dtype=int)\n",
    "\n",
    "    if np.all(counts >= min_days_per_period):\n",
    "        sid = ids[j] if j < len(ids) else str(j)\n",
    "        qualified.append((j, sid, counts[0], counts[1], counts[2], int(counts.sum())))\n",
    "\n",
    "print(f\"Sites with â‰¥{min_days_per_period} days in EACH period at depth {depth}: {len(qualified)}\")\n",
    "\n",
    "# Pretty print a few (or all)\n",
    "for rank, (j, sid, c1, c2, c3, tot) in enumerate(qualified[:50], start=1):\n",
    "    print(f\"{rank:2d}. site {j:4d} ({sid}): P1={c1}, P2={c2}, P3={c3}, total={tot}\")\n",
    "\n",
    "# Extract just the site indices from qualified list\n",
    "qualified_indices = [q[0] for q in qualified]    \n",
    "\n",
    "top_balanced_indices_depth0 = qualified_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(top_balanced_indices_depth0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the file\n",
    "data = mat73.loadmat(\"../test_data/M21C_land_sweeper/Evaluation/InSitu/output/LS_OLv8_M36_SCAN_SM_1d_c1234smv_25yr_stats_by_range.mat\")\n",
    "stats_OL = data[\"StatsByRange\"]\n",
    "\n",
    "# Print available keys and array shapes\n",
    "print(\"Available statistics in stats_by_range.mat:\\n\")\n",
    "for k, v in stats_OL.items():\n",
    "    if k in [\"names\", \"masks\", \"tvec\"]:\n",
    "        continue  # housekeeping fields\n",
    "    try:\n",
    "        print(f\"{k:10s} shape={v.shape}\")\n",
    "    except AttributeError:\n",
    "        print(f\"{k:10s} (not an array, type={type(v)})\")\n",
    "\n",
    "# Print the range names\n",
    "print(\"\\nDefined ranges:\", [str(n) for n in stats_OL[\"names\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = mat73.loadmat(\"../test_data/M21C_land_sweeper/Evaluation/InSitu/output/LS_DAv8_M36_SCAN_SM_1d_c1234smv_25yr_stats_by_range.mat\")\n",
    "stats_DA = data[\"StatsByRange\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot for both stats_OL and stats_DA using the same top_indices\n",
    "datasets = {'OL': stats_OL, 'DA': stats_DA}\n",
    "depth = 0  # Layer 0\n",
    "n_ranges = stats_OL['R'].shape[3]  # Number of time ranges\n",
    "expt_labels = [\"LS_OL\", \"LS_DA\"]\n",
    "range_names = [\"2000/6/1 - 2024/5/31\", \"2000/6/1 - 2007/5/31\", \"2007/6/1 - 2015/3/31\" ,\"2015/4/1 - 2024/5/31\"]\n",
    "\n",
    "n_sites = stats_OL['R'].shape[0]\n",
    "common_mask = np.zeros(n_sites, dtype=bool)\n",
    "\n",
    "# Start from your balanced site list, if you want to keep that constraint\n",
    "base_mask = np.zeros(n_sites, dtype=bool)\n",
    "base_mask[top_balanced_indices_depth0] = True\n",
    "\n",
    "# Require: site is non-NaN for ALL ranges in BOTH datasets (and all entries along the 3rd axis)\n",
    "# If the 3rd axis has size >1 (e.g., per-network/per-stat dim), we require no NaNs anywhere in it.\n",
    "common_mask[:] = base_mask\n",
    "for stats in datasets.values():\n",
    "    arr = stats['R'][:, depth, :, :]            # shape: (site, depth=0, K, n_ranges)\n",
    "    valid_here = np.all(~np.isnan(arr), axis=(1, 2))  # non-NaN across K and all ranges\n",
    "    common_mask &= valid_here\n",
    "\n",
    "# Turn mask into indices\n",
    "common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Safety check: if too strict (empty), relax requirement across the 3rd axis to \"any\"\n",
    "if common_idx.size == 0:\n",
    "    for stats in datasets.values():\n",
    "        arr = stats['anomR'][:, depth, :, :]               # (site, K, n_ranges)\n",
    "        valid_here = np.all(np.any(~np.isnan(arr), axis=1), axis=1)  # any over K, all ranges\n",
    "        common_mask = base_mask & valid_here if 'common_mask' not in locals() else (common_mask & valid_here)\n",
    "    common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Prepare figure\n",
    "fig, axs = plt.subplots(1, n_ranges, figsize=(20, 6), sharey=True)\n",
    "fig.suptitle('R Mean and Confidence Intervals for sites with good data coverage for surface', fontsize=title_fontsize)\n",
    "\n",
    "for range_idx in range(n_ranges):\n",
    "    # range_name = stats_OL['names'][range_idx]\n",
    "    range_name = range_names[range_idx]\n",
    "\n",
    "    bar_width = 0.6  # Width of the bars\n",
    "    x = np.arange(len(expt_labels))  # X positions for the bars\n",
    "\n",
    "    # Initialize lists to store means and confidence intervals\n",
    "    means = []\n",
    "    ci_lows = []\n",
    "    ci_ups = []\n",
    "\n",
    "     # n is the SAME for all experiments/ranges by construction\n",
    "    num_sites_sr = int(common_idx.size)  \n",
    "\n",
    "    for label, stats in datasets.items():\n",
    "        R = stats['R']\n",
    "        RLO = stats['RLO']\n",
    "        RUP = stats['RUP']\n",
    "\n",
    "        # Calculate mean and confidence intervals\n",
    "        R_mean = np.around(np.nanmean(R[common_idx, depth, :, range_idx]), decimals=2)\n",
    "        R_CI_LO = np.around(np.nanmean(RLO[common_idx, depth, :, range_idx]) / np.sqrt(num_sites_sr), decimals=4)\n",
    "        R_CI_UP = np.around(np.nanmean(RUP[common_idx, depth, :, range_idx]) / np.sqrt(num_sites_sr), decimals=4)\n",
    "\n",
    "        means.append(R_mean)\n",
    "        ci_lows.append(R_CI_LO)\n",
    "        ci_ups.append(R_CI_UP)\n",
    "\n",
    "    # Plot bar chart with error bars\n",
    "    # Combine the absolute values of confidence intervals for yerr\n",
    "    yerr = np.array([np.abs(ci_lows), np.abs(ci_ups)])\n",
    "    axs[range_idx].bar(x, means, yerr=yerr, capsize=5, width=bar_width, color=[plt.cm.tab10(i % 10) for i in range(len(means))], label=expt_labels)\n",
    "    axs[range_idx].set_title(f'{range_name} (n = {num_sites_sr})', fontsize=title_fontsize)\n",
    "    axs[range_idx].set_xticks(x)\n",
    "    axs[range_idx].set_xticklabels(expt_labels, fontsize=y_tick_label_fontsize)\n",
    "    axs[range_idx].set_ylabel('R Mean', fontsize=label_fontsize)\n",
    "    axs[range_idx].set_ylim(0.6, 0.75)\n",
    "    axs[range_idx].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import cartopy.crs as ccrs\n",
    "import cartopy.feature as cfeature\n",
    "\n",
    "# Ensure required variables are available\n",
    "assert 'stats_OL' in globals(), \"stats_OL not loaded\"\n",
    "assert 'stats_DA' in globals(), \"stats_DA not loaded\"\n",
    "assert 'mat' in globals(), \"mat not loaded\"\n",
    "\n",
    "# Extract latitudes and longitudes for common_idx\n",
    "latitudes = mat['INSITU_lat'].ravel()[common_idx]\n",
    "longitudes = mat['INSITU_lon'].ravel()[common_idx]\n",
    "\n",
    "# Extract R values for OL and DA for common_idx\n",
    "R_OL = stats_OL['R'][common_idx, 0, 0, :]  # Shape: (n_sites, n_ranges)\n",
    "R_DA = stats_DA['R'][common_idx, 0, 0, :]  # Shape: (n_sites, n_ranges)\n",
    "\n",
    "# Calculate DA R - OL R for common_idx\n",
    "R_diff = R_DA - R_OL  # Shape: (n_sites, n_ranges)\n",
    "\n",
    "# Define range names for titles\n",
    "range_names = [\"2000/6/1 - 2024/5/31\", \"2000/6/1 - 2007/5/31\", \"2007/6/1 - 2015/3/31\", \"2015/4/1 - 2024/5/31\"]\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "fig.suptitle(r'Surface SM: DA $R$ - OL $R$ (Long time series SCAN sites)', fontsize=20, y=0.92)\n",
    "\n",
    "# Create a single colorbar axis at the bottom\n",
    "cbar_ax = fig.add_axes([0.25, 0.2, 0.5, 0.02])  # [left, bottom, width, height]\n",
    "\n",
    "# Initialize a variable to store the scatter plot for the colorbar\n",
    "sc = None\n",
    "\n",
    "# Define discrete color levels\n",
    "levels = np.linspace(-0.05, 0.05, 10)\n",
    "cmap = plt.cm.get_cmap('coolwarm_r', len(levels) - 1)  # Discrete colormap\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    if i >= R_diff.shape[1]:\n",
    "        ax.axis('off')  # Turn off unused subplots\n",
    "        continue\n",
    "\n",
    "    # Add map features\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES, edgecolor='black')  # Add state boundaries\n",
    "\n",
    "    # Set extent to show all of CONUS\n",
    "    ax.set_extent([-125, -66.5, 24, 49], crs=ccrs.PlateCarree())\n",
    "\n",
    "    # Plot the data for common_idx\n",
    "    sc = ax.scatter(longitudes, latitudes, c=R_diff[:, i], cmap=cmap, s=50, edgecolor='k', \n",
    "                    transform=ccrs.PlateCarree(), vmin=levels[0], vmax=levels[-1])\n",
    "    ax.set_title(range_names[i], fontsize=16)\n",
    "\n",
    "# Add a single discrete colorbar\n",
    "norm = plt.Normalize(vmin=levels[0], vmax=levels[-1])\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label('DA R - OL R', fontsize=14)\n",
    "cbar.set_ticks(levels)\n",
    "cbar.ax.set_xticklabels([f\"{lvl:.2f}\" for lvl in levels])\n",
    "\n",
    "plt.tight_layout(rect=[0.1, 0.15, 1, 1])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---- Build a common site mask across all datasets and time ranges (depth=0) ----\n",
    "depth = 0\n",
    "datasets = {'OL': stats_OL, 'DA': stats_DA}\n",
    "\n",
    "n_sites = stats_OL['anomR'].shape[0]\n",
    "common_mask = np.zeros(n_sites, dtype=bool)\n",
    "\n",
    "# Start from your balanced site list, if you want to keep that constraint\n",
    "base_mask = np.zeros(n_sites, dtype=bool)\n",
    "base_mask[top_balanced_indices_depth0] = True\n",
    "\n",
    "# Require: site is non-NaN for ALL ranges in BOTH datasets (and all entries along the 3rd axis)\n",
    "# If the 3rd axis has size >1 (e.g., per-network/per-stat dim), we require no NaNs anywhere in it.\n",
    "common_mask[:] = base_mask\n",
    "for stats in datasets.values():\n",
    "    arr = stats['anomR'][:, depth, :, :]            # shape: (site, depth=0, K, n_ranges)\n",
    "    valid_here = np.all(~np.isnan(arr), axis=(1, 2))  # non-NaN across K and all ranges\n",
    "    common_mask &= valid_here\n",
    "\n",
    "# Turn mask into indices\n",
    "common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Safety check: if too strict (empty), relax requirement across the 3rd axis to \"any\"\n",
    "if common_idx.size == 0:\n",
    "    for stats in datasets.values():\n",
    "        arr = stats['anomR'][:, depth, :, :]               # (site, K, n_ranges)\n",
    "        valid_here = np.all(np.any(~np.isnan(arr), axis=1), axis=1)  # any over K, all ranges\n",
    "        common_mask = base_mask & valid_here if 'common_mask' not in locals() else (common_mask & valid_here)\n",
    "    common_idx = np.where(common_mask)[0]\n",
    "\n",
    "fig, axs = plt.subplots(1, n_ranges, figsize=(20, 6), sharey=True)\n",
    "fig.suptitle('anomR Mean and Confidence Intervals for sites with good data coverage for surface', fontsize=title_fontsize)\n",
    "\n",
    "for range_idx in range(n_ranges):\n",
    "    range_name = range_names[range_idx]\n",
    "    x = np.arange(len(expt_labels))\n",
    "    bar_width = 0.6\n",
    "\n",
    "    means, ci_lows, ci_ups = [], [], []\n",
    "\n",
    "    # n is identical across experiments/ranges by construction\n",
    "    num_sites_sr = int(common_idx.size)\n",
    "\n",
    "    for label, stats in datasets.items():\n",
    "        anomR   = stats['anomR'][common_idx, depth, :, range_idx]\n",
    "        anomRLO = stats['anomRLO'][common_idx, depth, :, range_idx]\n",
    "        anomRUP = stats['anomRUP'][common_idx, depth, :, range_idx]\n",
    "\n",
    "        # Collapse the extra dim if needed\n",
    "        vals = np.nanmean(anomR, axis=1) if anomR.ndim == 2 else anomR.ravel()\n",
    "        lo   = np.nanmean(anomRLO, axis=1) if anomRLO.ndim == 2 else anomRLO.ravel()\n",
    "        up   = np.nanmean(anomRUP, axis=1) if anomRUP.ndim == 2 else anomRUP.ravel()\n",
    "\n",
    "        # Mean\n",
    "        anomR_mean = float(np.nanmean(vals))\n",
    "\n",
    "        # Error bars (keep your existing approach; or replace with SEM if you prefer)\n",
    "        anomR_CI_LO = float(np.nanmean(lo) / np.sqrt(max(num_sites_sr, 1)))\n",
    "        anomR_CI_UP = float(np.nanmean(up) / np.sqrt(max(num_sites_sr, 1)))\n",
    "\n",
    "        means.append(anomR_mean)\n",
    "        ci_lows.append(anomR_CI_LO)\n",
    "        ci_ups.append(anomR_CI_UP)\n",
    "\n",
    "    yerr = np.array([np.abs(ci_lows), np.abs(ci_ups)])\n",
    "    axs[range_idx].bar(\n",
    "        x, means, yerr=yerr, capsize=5, width=bar_width,\n",
    "        color=[plt.cm.tab10(i % 10) for i in range(len(means))]\n",
    "    )\n",
    "    axs[range_idx].set_title(f'{range_name} (n = {num_sites_sr})', fontsize=title_fontsize)\n",
    "    axs[range_idx].set_xticks(x)\n",
    "    axs[range_idx].set_xticklabels(expt_labels, fontsize=y_tick_label_fontsize)\n",
    "    axs[range_idx].set_ylabel('anomR Mean', fontsize=label_fontsize)\n",
    "    axs[range_idx].set_ylim(0.5, 0.7)\n",
    "    axs[range_idx].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract anomR values for OL and DA for common_idx\n",
    "anomR_OL = stats_OL['anomR'][common_idx, 0, 0, :]  # Shape: (n_sites, n_ranges)\n",
    "anomR_DA = stats_DA['anomR'][common_idx, 0, 0, :]  # Shape: (n_sites, n_ranges)\n",
    "\n",
    "# Extract latitudes and longitudes for common_idx\n",
    "latitudes = mat['INSITU_lat'].ravel()[common_idx]\n",
    "longitudes = mat['INSITU_lon'].ravel()[common_idx]\n",
    "\n",
    "# Calculate DA anomR - OL anomR for common_idx\n",
    "anomR_diff = anomR_DA - anomR_OL  # Shape: (n_sites, n_ranges)\n",
    "\n",
    "# Define range names for titles\n",
    "range_names = [\"2000/6/1 - 2024/5/31\", \"2000/6/1 - 2007/5/31\", \"2007/6/1 - 2015/3/31\", \"2015/4/1 - 2024/5/31\"]\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "fig.suptitle(r'Surface SM: DA $anomR$ - OL $anomR$ (Long time series SCAN sites)', fontsize=20, y=0.92)\n",
    "\n",
    "# Create a single colorbar axis at the bottom\n",
    "cbar_ax = fig.add_axes([0.25, 0.2, 0.5, 0.02])  # [left, bottom, width, height]\n",
    "\n",
    "# Initialize a variable to store the scatter plot for the colorbar\n",
    "sc = None\n",
    "\n",
    "# Define discrete color levels\n",
    "levels = np.linspace(-0.1, 0.1, 10)\n",
    "cmap = plt.cm.get_cmap('coolwarm_r', len(levels) - 1)  # Discrete colormap\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    if i >= anomR_diff.shape[1]:\n",
    "        ax.axis('off')  # Turn off unused subplots\n",
    "        continue\n",
    "\n",
    "    # Add map features\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES, edgecolor='black')  # Add state boundaries\n",
    "\n",
    "    # Set extent to show all of CONUS\n",
    "    ax.set_extent([-125, -66.5, 24, 49], crs=ccrs.PlateCarree())\n",
    "\n",
    "    # Plot the data for common_idx\n",
    "    sc = ax.scatter(longitudes, latitudes, c=anomR_diff[:, i], cmap=cmap, s=50, edgecolor='k', \n",
    "                    transform=ccrs.PlateCarree(), vmin=levels[0], vmax=levels[-1])\n",
    "    ax.set_title(range_names[i], fontsize=16)\n",
    "\n",
    "# Add a single discrete colorbar\n",
    "norm = plt.Normalize(vmin=levels[0], vmax=levels[-1])\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label('DA anomR - OL anomR', fontsize=14)\n",
    "cbar.set_ticks(levels)\n",
    "cbar.ax.set_xticklabels([f\"{lvl:.2f}\" for lvl in levels])\n",
    "\n",
    "plt.tight_layout(rect=[0.1, 0.15, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot for both stats_OL and stats_DA using the same top_indices\n",
    "datasets = {'OL': stats_OL, 'DA': stats_DA}\n",
    "depth = 0  # Layer 0\n",
    "n_ranges = stats_OL['ubRMSE'].shape[3]  # Number of time ranges\n",
    "expt_labels = [\"LS_OL\", \"LS_DA\"]\n",
    "\n",
    "n_sites = stats_OL['ubRMSE'].shape[0]\n",
    "common_mask = np.zeros(n_sites, dtype=bool)\n",
    "\n",
    "# Start from your balanced site list, if you want to keep that constraint\n",
    "base_mask = np.zeros(n_sites, dtype=bool)\n",
    "base_mask[top_balanced_indices_depth0] = True\n",
    "\n",
    "# Require: site is non-NaN for ALL ranges in BOTH datasets (and all entries along the 3rd axis)\n",
    "# If the 3rd axis has size >1 (e.g., per-network/per-stat dim), we require no NaNs anywhere in it.\n",
    "common_mask[:] = base_mask\n",
    "for stats in datasets.values():\n",
    "    arr = stats['ubRMSE'][:, depth, :, :]            # shape: (site, depth=0, K, n_ranges)\n",
    "    valid_here = np.all(~np.isnan(arr), axis=(1, 2))  # non-NaN across K and all ranges\n",
    "    common_mask &= valid_here\n",
    "\n",
    "# Turn mask into indices\n",
    "common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Safety check: if too strict (empty), relax requirement across the 3rd axis to \"any\"\n",
    "if common_idx.size == 0:\n",
    "    for stats in datasets.values():\n",
    "        arr = stats['ubRMSE'][:, depth, :, :]               # (site, K, n_ranges)\n",
    "        valid_here = np.all(np.any(~np.isnan(arr), axis=1), axis=1)  # any over K, all ranges\n",
    "        common_mask = base_mask & valid_here if 'common_mask' not in locals() else (common_mask & valid_here)\n",
    "    common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Prepare figure\n",
    "fig, axs = plt.subplots(1, n_ranges, figsize=(20, 6), sharey=True)\n",
    "fig.suptitle('ubRMSE Mean and Confidence Intervals for sites with good data coverage for surface', fontsize=title_fontsize)\n",
    "\n",
    "for range_idx in range(n_ranges):\n",
    "    #range_name = stats_OL['names'][range_idx]\n",
    "    range_name = range_names[range_idx]\n",
    "    bar_width = 0.6  # Width of the bars\n",
    "    x = np.arange(len(expt_labels))  # X positions for the bars\n",
    "\n",
    "    # Initialize lists to store means and confidence intervals\n",
    "    means = []\n",
    "    ci_lows = []\n",
    "    ci_ups = []\n",
    "\n",
    "    # n is identical across experiments/ranges by construction\n",
    "    num_sites_sr = int(common_idx.size)\n",
    "\n",
    "    for label, stats in datasets.items():\n",
    "        ubRMSE = stats['ubRMSE']\n",
    "        ubRMSELO = stats['ubRMSELO']\n",
    "        ubRMSEUP = stats['ubRMSEUP']\n",
    "\n",
    "        # Calculate mean and confidence intervals\n",
    "        ubRMSE_mean = np.around(np.nanmean(ubRMSE[common_idx, depth, :, range_idx]), decimals=3)\n",
    "        ubRMSE_CI_LO = np.around(np.nanmean(ubRMSELO[common_idx, depth, :, range_idx]) / np.sqrt(num_sites_sr), decimals=4)\n",
    "        ubRMSE_CI_UP = np.around(np.nanmean(ubRMSEUP[common_idx, depth, :, range_idx]) / np.sqrt(num_sites_sr), decimals=4)\n",
    "\n",
    "        means.append(ubRMSE_mean)\n",
    "        ci_lows.append(ubRMSE_CI_LO)\n",
    "        ci_ups.append(ubRMSE_CI_UP)\n",
    "\n",
    "    # Plot bar chart with error bars\n",
    "    # Combine the absolute values of confidence intervals for yerr\n",
    "    yerr = np.array([np.abs(ci_lows), np.abs(ci_ups)])\n",
    "    axs[range_idx].bar(x, means, yerr=yerr, capsize=5, width=bar_width, color=[plt.cm.tab10(i % 10) for i in range(len(means))], label=expt_labels)\n",
    "    axs[range_idx].set_title(f'{range_name} (n = {num_sites_sr})', fontsize=title_fontsize)\n",
    "    axs[range_idx].set_xticks(x)\n",
    "    axs[range_idx].set_xticklabels(expt_labels, fontsize=y_tick_label_fontsize)\n",
    "    axs[range_idx].set_ylabel('ubRMSE Mean (m3/m3)', fontsize=label_fontsize)\n",
    "    axs[range_idx].set_ylim(0.05, 0.07)\n",
    "    axs[range_idx].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract ubRMSE values for OL and DA for common_idx\n",
    "ubRMSE_OL = stats_OL['ubRMSE'][common_idx, 0, 0, :]  # Shape: (n_sites, n_ranges)\n",
    "ubRMSE_DA = stats_DA['ubRMSE'][common_idx, 0, 0, :]  # Shape: (n_sites, n_ranges)\n",
    "\n",
    "# Extract latitudes and longitudes for common_idx\n",
    "latitudes = mat['INSITU_lat'].ravel()[common_idx]\n",
    "longitudes = mat['INSITU_lon'].ravel()[common_idx]\n",
    "\n",
    "# Calculate DA ubRMSE - OL ubRMSE for common_idx\n",
    "ubRMSE_diff = ubRMSE_DA - ubRMSE_OL  # Shape: (n_sites, n_ranges)\n",
    "\n",
    "# Define range names for titles\n",
    "range_names = [\"2000/6/1 - 2024/5/31\", \"2000/6/1 - 2007/5/31\", \"2007/6/1 - 2015/3/31\", \"2015/4/1 - 2024/5/31\"]\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(16, 12), subplot_kw={'projection': ccrs.PlateCarree()})\n",
    "fig.suptitle(r'Surface SM: DA $ubRMSE$ - OL $ubRMSE$ (Long time series SCAN sites)', fontsize=20, y=0.92)\n",
    "\n",
    "# Create a single colorbar axis at the bottom\n",
    "cbar_ax = fig.add_axes([0.25, 0.2, 0.5, 0.02])  # [left, bottom, width, height]\n",
    "\n",
    "# Initialize a variable to store the scatter plot for the colorbar\n",
    "sc = None\n",
    "\n",
    "# Define discrete color levels\n",
    "levels = np.linspace(-0.01, 0.01, 10)\n",
    "cmap = plt.cm.get_cmap('coolwarm', len(levels) - 1)  # Discrete colormap with reversed colors\n",
    "\n",
    "for i, ax in enumerate(axs.flat):\n",
    "    if i >= ubRMSE_diff.shape[1]:\n",
    "        ax.axis('off')  # Turn off unused subplots\n",
    "        continue\n",
    "\n",
    "    # Add map features\n",
    "    ax.add_feature(cfeature.COASTLINE)\n",
    "    ax.add_feature(cfeature.BORDERS, linestyle=':')\n",
    "    ax.add_feature(cfeature.STATES, edgecolor='black')  # Add state boundaries\n",
    "\n",
    "    # Set extent to show all of CONUS\n",
    "    ax.set_extent([-125, -66.5, 24, 49], crs=ccrs.PlateCarree())\n",
    "\n",
    "    # Plot the data for common_idx\n",
    "    sc = ax.scatter(longitudes, latitudes, c=ubRMSE_diff[:, i], cmap=cmap, s=50, edgecolor='k', \n",
    "                    transform=ccrs.PlateCarree(), vmin=levels[0], vmax=levels[-1])\n",
    "    ax.set_title(range_names[i], fontsize=16)\n",
    "\n",
    "# Add a single discrete colorbar\n",
    "norm = plt.Normalize(vmin=levels[0], vmax=levels[-1])\n",
    "cbar = fig.colorbar(plt.cm.ScalarMappable(norm=norm, cmap=cmap), cax=cbar_ax, orientation='horizontal')\n",
    "cbar.set_label('DA ubRMSE - OL ubRMSE', fontsize=14)\n",
    "cbar.set_ticks(levels)\n",
    "cbar.ax.set_xticklabels([f\"{lvl:.3f}\" for lvl in levels])\n",
    "\n",
    "plt.tight_layout(rect=[0.1, 0.15, 1, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Initialize an empty list to store the data\n",
    "table_data = []\n",
    "\n",
    "# Loop through each site in top_balanced_indices_depth0\n",
    "for site_idx in top_balanced_indices_depth0:\n",
    "    # Loop through each time period\n",
    "    for range_idx, range_name in enumerate(range_names):\n",
    "        # Extract statistics for OL\n",
    "        R_OL = stats_OL['R'][site_idx, 0, 0, range_idx]\n",
    "        anomR_OL = stats_OL['anomR'][site_idx, 0, 0, range_idx]\n",
    "        ubRMSE_OL = stats_OL['ubRMSE'][site_idx, 0, 0, range_idx]\n",
    "\n",
    "        # Extract statistics for DA\n",
    "        R_DA = stats_DA['R'][site_idx, 0, 0, range_idx]\n",
    "        anomR_DA = stats_DA['anomR'][site_idx, 0, 0, range_idx]\n",
    "        ubRMSE_DA = stats_DA['ubRMSE'][site_idx, 0, 0, range_idx]\n",
    "\n",
    "        # Append the data to the list\n",
    "        table_data.append({\n",
    "            'Site': site_idx,\n",
    "            'Time Period': range_name,\n",
    "            'R_OL': R_OL,\n",
    "            'anomR_OL': anomR_OL,\n",
    "            'ubRMSE_OL': ubRMSE_OL,\n",
    "            'R_DA': R_DA,\n",
    "            'anomR_DA': anomR_DA,\n",
    "            'ubRMSE_DA': ubRMSE_DA\n",
    "        })\n",
    "\n",
    "# Convert the list to a DataFrame\n",
    "results_table = pd.DataFrame(table_data)\n",
    "\n",
    "# Display the table\n",
    "print(results_table)\n",
    "\n",
    "# Optionally, save the table to a CSV file\n",
    "results_table.to_csv('statistics_table.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare figure properties\n",
    "time_periods = [\"2000/6/1 - 2024/5/31\", \"2000/6/1 - 2007/5/31\", \"2007/6/1 - 2015/3/31\", \"2015/4/1 - 2024/5/31\"]\n",
    "colors = ['blue', 'orange']\n",
    "\n",
    "# Loop through each time period\n",
    "for period_idx, period_name in enumerate(time_periods):\n",
    "    # Extract R values for OL and DA for the current time period\n",
    "    R_OL_values = [entry['R_OL'] for entry in table_data if entry['Time Period'] == period_name]\n",
    "    R_DA_values = [entry['R_DA'] for entry in table_data if entry['Time Period'] == period_name]\n",
    "    site_indices = [entry['Site'] for entry in table_data if entry['Time Period'] == period_name]\n",
    "\n",
    "    # Create the bar chart\n",
    "    x = np.arange(len(site_indices))  # X positions for the bars\n",
    "    bar_width = 0.4\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(x - bar_width / 2, R_OL_values, bar_width, label='OL', color=colors[0])\n",
    "    plt.bar(x + bar_width / 2, R_DA_values, bar_width, label='DA', color=colors[1])\n",
    "\n",
    "    # Add labels, title, and legend\n",
    "    plt.xlabel('Site Index', fontsize=14)\n",
    "    plt.ylabel('R Value', fontsize=14)\n",
    "    plt.title(f'R Values for OL and DA ({period_name})', fontsize=16)\n",
    "    plt.xticks(x, site_indices, rotation=45, fontsize=12)\n",
    "    plt.ylim(0, 1)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Config ---\n",
    "depth = 1                      # 0=surface, 1=rootzone\n",
    "min_days_per_period = 500\n",
    "\n",
    "# Reuse your existing periods (same as earlier cell)\n",
    "periods = [\n",
    "    (\"P1\", datetime(2000, 6, 1).date(), datetime(2007, 6, 1).date()),   # [start, end)\n",
    "    (\"P2\", datetime(2007, 6, 1).date(), datetime(2015, 4, 1).date()),   # [start, end)\n",
    "    (\"P3\", datetime(2015, 4, 1).date(), datetime(2024, 6, 30).date()),  # inclusive end\n",
    "]\n",
    "\n",
    "qualified = []   # list of (site_idx, site_id, P1, P2, P3, total)\n",
    "\n",
    "for j in range(n_sites):\n",
    "    days = unique_days[depth][j]\n",
    "    if days is None or len(days) == 0:\n",
    "        continue\n",
    "    days = np.asarray(days)  # ensure numpy array of datetime.date\n",
    "\n",
    "    # Count using same inclusivity as your previous code\n",
    "    counts = np.array([\n",
    "        np.count_nonzero((days >= p[1]) & (days < p[2])) if i < 2\n",
    "        else np.count_nonzero((days >= p[1]) & (days <= p[2]))  # P3 inclusive end\n",
    "        for i, p in enumerate(periods)\n",
    "    ], dtype=int)\n",
    "\n",
    "    if np.all(counts >= min_days_per_period):\n",
    "        sid = ids[j] if j < len(ids) else str(j)\n",
    "        qualified.append((j, sid, counts[0], counts[1], counts[2], int(counts.sum())))\n",
    "\n",
    "print(f\"Sites with â‰¥{min_days_per_period} days in EACH period at depth {depth}: {len(qualified)}\")\n",
    "\n",
    "# Pretty print a few (or all)\n",
    "for rank, (j, sid, c1, c2, c3, tot) in enumerate(qualified[:50], start=1):\n",
    "    print(f\"{rank:2d}. site {j:4d} ({sid}): P1={c1}, P2={c2}, P3={c3}, total={tot}\")\n",
    "\n",
    "# Extract just the site indices from qualified list\n",
    "qualified_indices = [q[0] for q in qualified]    \n",
    "\n",
    "top_balanced_indices_depth1 = qualified_indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot for both stats_OL and stats_DA using the same top_indices\n",
    "datasets = {'OL': stats_OL, 'DA': stats_DA}\n",
    "depth = 1  # Layer 1\n",
    "n_ranges = stats_OL['R'].shape[3]  # Number of time ranges\n",
    "expt_labels = [\"LS_OL\", \"LS_DA\"]\n",
    "range_names = [\"2000/6/1 - 2024/5/31\", \"2000/6/1 - 2007/5/31\", \"2007/6/1 - 2015/3/31\" ,\"2015/4/1 - 2024/5/31\"]\n",
    "\n",
    "n_sites = stats_OL['R'].shape[0]\n",
    "common_mask = np.zeros(n_sites, dtype=bool)\n",
    "\n",
    "# Start from your balanced site list, if you want to keep that constraint\n",
    "base_mask = np.zeros(n_sites, dtype=bool)\n",
    "base_mask[top_balanced_indices_depth1] = True\n",
    "\n",
    "# Require: site is non-NaN for ALL ranges in BOTH datasets (and all entries along the 3rd axis)\n",
    "# If the 3rd axis has size >1 (e.g., per-network/per-stat dim), we require no NaNs anywhere in it.\n",
    "common_mask[:] = base_mask\n",
    "for stats in datasets.values():\n",
    "    arr = stats['R'][:, depth, :, :]            # shape: (site, depth=0, K, n_ranges)\n",
    "    valid_here = np.all(~np.isnan(arr), axis=(1, 2))  # non-NaN across K and all ranges\n",
    "    common_mask &= valid_here\n",
    "\n",
    "# Turn mask into indices\n",
    "common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Safety check: if too strict (empty), relax requirement across the 3rd axis to \"any\"\n",
    "if common_idx.size == 0:\n",
    "    for stats in datasets.values():\n",
    "        arr = stats['R'][:, depth, :, :]               # (site, K, n_ranges)\n",
    "        valid_here = np.all(np.any(~np.isnan(arr), axis=1), axis=1)  # any over K, all ranges\n",
    "        common_mask = base_mask & valid_here if 'common_mask' not in locals() else (common_mask & valid_here)\n",
    "    common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Prepare figure\n",
    "fig, axs = plt.subplots(1, n_ranges, figsize=(20, 6), sharey=True)\n",
    "fig.suptitle('R Mean and Confidence Intervals for sites with good data coverage for rootzone', fontsize=title_fontsize)\n",
    "\n",
    "for range_idx in range(n_ranges):\n",
    "    # range_name = stats_OL['names'][range_idx]\n",
    "    range_name = range_names[range_idx]\n",
    "\n",
    "    bar_width = 0.6  # Width of the bars\n",
    "    x = np.arange(len(expt_labels))  # X positions for the bars\n",
    "\n",
    "    # Initialize lists to store means and confidence intervals\n",
    "    means = []\n",
    "    ci_lows = []\n",
    "    ci_ups = []\n",
    "\n",
    "    # n is the SAME for all experiments/ranges by construction\n",
    "    num_sites_sr = int(common_idx.size)\n",
    "    print(f\"Range {range_name}: n_sites_sr = {num_sites_sr}\")\n",
    "\n",
    "    for label, stats in datasets.items():\n",
    "        R = stats['R']\n",
    "        RLO = stats['RLO']\n",
    "        RUP = stats['RUP']\n",
    "\n",
    "        # Calculate mean and confidence intervals\n",
    "        R_mean = np.around(np.nanmean(R[common_idx, depth, :, range_idx]), decimals=2)\n",
    "        R_CI_LO = np.around(np.nanmean(RLO[common_idx, depth, :, range_idx]) / np.sqrt(num_sites_sr), decimals=4)\n",
    "        R_CI_UP = np.around(np.nanmean(RUP[common_idx, depth, :, range_idx]) / np.sqrt(num_sites_sr), decimals=4)\n",
    "\n",
    "        means.append(R_mean)\n",
    "        ci_lows.append(R_CI_LO)\n",
    "        ci_ups.append(R_CI_UP)\n",
    "\n",
    "    # Plot bar chart with error bars\n",
    "    # Combine the absolute values of confidence intervals for yerr\n",
    "    yerr = np.array([np.abs(ci_lows), np.abs(ci_ups)])\n",
    "    axs[range_idx].bar(x, means, yerr=yerr, capsize=5, width=bar_width, color=[plt.cm.tab10(i % 10) for i in range(len(means))], label=expt_labels)\n",
    "    axs[range_idx].set_title(f'{range_name} (n = {num_sites_sr})', fontsize=title_fontsize)\n",
    "    axs[range_idx].set_xticks(x)\n",
    "    axs[range_idx].set_xticklabels(expt_labels, fontsize=y_tick_label_fontsize)\n",
    "    axs[range_idx].set_ylabel('R Mean', fontsize=label_fontsize)\n",
    "    axs[range_idx].set_ylim(0.5, 0.8)\n",
    "    axs[range_idx].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot for both stats_OL and stats_DA using the same top_indices\n",
    "datasets = {'OL': stats_OL, 'DA': stats_DA}\n",
    "depth = 1  # Layer 0\n",
    "n_ranges = stats_OL['anomR'].shape[3]  # Number of time ranges\n",
    "expt_labels = [\"LS_OL\", \"LS_DA\"]\n",
    "\n",
    "n_sites = stats_OL['anomR'].shape[0]\n",
    "common_mask = np.zeros(n_sites, dtype=bool)\n",
    "\n",
    "# Start from your balanced site list, if you want to keep that constraint\n",
    "base_mask = np.zeros(n_sites, dtype=bool)\n",
    "base_mask[top_balanced_indices_depth1] = True\n",
    "\n",
    "# Require: site is non-NaN for ALL ranges in BOTH datasets (and all entries along the 3rd axis)\n",
    "# If the 3rd axis has size >1 (e.g., per-network/per-stat dim), we require no NaNs anywhere in it.\n",
    "common_mask[:] = base_mask\n",
    "for stats in datasets.values():\n",
    "    arr = stats['anomR'][:, depth, :, :]            # shape: (site, depth=0, K, n_ranges)\n",
    "    valid_here = np.all(~np.isnan(arr), axis=(1, 2))  # non-NaN across K and all ranges\n",
    "    common_mask &= valid_here\n",
    "\n",
    "# Turn mask into indices\n",
    "common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Safety check: if too strict (empty), relax requirement across the 3rd axis to \"any\"\n",
    "if common_idx.size == 0:\n",
    "    for stats in datasets.values():\n",
    "        arr = stats['anomR'][:, depth, :, :]               # (site, K, n_ranges)\n",
    "        valid_here = np.all(np.any(~np.isnan(arr), axis=1), axis=1)  # any over K, all ranges\n",
    "        common_mask = base_mask & valid_here if 'common_mask' not in locals() else (common_mask & valid_here)\n",
    "    common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Prepare figure\n",
    "fig, axs = plt.subplots(1, n_ranges, figsize=(20, 6), sharey=True)\n",
    "fig.suptitle('anomR Mean and Confidence Intervals for sites with good data coverage for rootzone', fontsize=title_fontsize)\n",
    "\n",
    "for range_idx in range(n_ranges):\n",
    "    #range_name = stats_OL['names'][range_idx]\n",
    "    range_name = range_names[range_idx]\n",
    "    bar_width = 0.6  # Width of the bars\n",
    "    x = np.arange(len(expt_labels))  # X positions for the bars\n",
    "\n",
    "    # Initialize lists to store means and confidence intervals\n",
    "    means = []\n",
    "    ci_lows = []\n",
    "    ci_ups = []\n",
    "\n",
    "    # n is the SAME for all experiments/ranges by construction\n",
    "    num_sites_sr = int(common_idx.size)  \n",
    "\n",
    "    for label, stats in datasets.items():\n",
    "        anomR = stats['anomR']\n",
    "        anomRLO = stats['anomRLO']\n",
    "        anomRUP = stats['anomRUP']\n",
    "\n",
    "        # Calculate mean and confidence intervals\n",
    "        anomR_mean = np.around(np.nanmean(anomR[common_idx, depth, :, range_idx]), decimals=2)\n",
    "        anomR_CI_LO = np.around(np.nanmean(anomRLO[common_idx, depth, :, range_idx]) / np.sqrt(num_sites_sr), decimals=4)\n",
    "        anomR_CI_UP = np.around(np.nanmean(anomRUP[common_idx, depth, :, range_idx]) / np.sqrt(num_sites_sr), decimals=4)\n",
    "\n",
    "        means.append(anomR_mean)\n",
    "        ci_lows.append(anomR_CI_LO)\n",
    "        ci_ups.append(anomR_CI_UP)\n",
    "\n",
    "    # Plot bar chart with error bars\n",
    "    # Combine the absolute values of confidence intervals for yerr\n",
    "    yerr = np.array([np.abs(ci_lows), np.abs(ci_ups)])\n",
    "    axs[range_idx].bar(x, means, yerr=yerr, capsize=5, width=bar_width, color=[plt.cm.tab10(i % 10) for i in range(len(means))], label=expt_labels)\n",
    "    axs[range_idx].set_title(f'{range_name} (n = {num_sites_sr})', fontsize=title_fontsize)\n",
    "    axs[range_idx].set_xticks(x)\n",
    "    axs[range_idx].set_xticklabels(expt_labels, fontsize=y_tick_label_fontsize)\n",
    "    axs[range_idx].set_ylabel('anomR Mean', fontsize=label_fontsize)\n",
    "    axs[range_idx].set_ylim(0.4, 0.8)\n",
    "    axs[range_idx].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and plot for both stats_OL and stats_DA using the same top_indices\n",
    "datasets = {'OL': stats_OL, 'DA': stats_DA}\n",
    "depth = 1  # Layer 0\n",
    "n_ranges = stats_OL['ubRMSE'].shape[3]  # Number of time ranges\n",
    "expt_labels = [\"LS_OL\", \"LS_DA\"]\n",
    "\n",
    "n_sites = stats_OL['ubRMSE'].shape[0]\n",
    "common_mask = np.zeros(n_sites, dtype=bool)\n",
    "\n",
    "# Start from your balanced site list, if you want to keep that constraint\n",
    "base_mask = np.zeros(n_sites, dtype=bool)\n",
    "base_mask[top_balanced_indices_depth1] = True\n",
    "\n",
    "# Require: site is non-NaN for ALL ranges in BOTH datasets (and all entries along the 3rd axis)\n",
    "# If the 3rd axis has size >1 (e.g., per-network/per-stat dim), we require no NaNs anywhere in it.\n",
    "common_mask[:] = base_mask\n",
    "for stats in datasets.values():\n",
    "    arr = stats['ubRMSE'][:, depth, :, :]            # shape: (site, depth=0, K, n_ranges)\n",
    "    valid_here = np.all(~np.isnan(arr), axis=(1, 2))  # non-NaN across K and all ranges\n",
    "    common_mask &= valid_here\n",
    "\n",
    "# Turn mask into indices\n",
    "common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Safety check: if too strict (empty), relax requirement across the 3rd axis to \"any\"\n",
    "if common_idx.size == 0:\n",
    "    for stats in datasets.values():\n",
    "        arr = stats['ubRMSE'][:, depth, :, :]               # (site, K, n_ranges)\n",
    "        valid_here = np.all(np.any(~np.isnan(arr), axis=1), axis=1)  # any over K, all ranges\n",
    "        common_mask = base_mask & valid_here if 'common_mask' not in locals() else (common_mask & valid_here)\n",
    "    common_idx = np.where(common_mask)[0]\n",
    "\n",
    "# Prepare figure\n",
    "fig, axs = plt.subplots(1, n_ranges, figsize=(20, 6), sharey=True)\n",
    "fig.suptitle('ubRMSE Mean and Confidence Intervals for sites with good data coverage for rootzone', fontsize=title_fontsize)\n",
    "\n",
    "for range_idx in range(n_ranges):\n",
    "    #range_name = stats_OL['names'][range_idx]\n",
    "    range_name = range_names[range_idx]\n",
    "    bar_width = 0.6  # Width of the bars\n",
    "    x = np.arange(len(expt_labels))  # X positions for the bars\n",
    "\n",
    "    # Initialize lists to store means and confidence intervals\n",
    "    means = []\n",
    "    ci_lows = []\n",
    "    ci_ups = []\n",
    "\n",
    "    # n is the SAME for all experiments/ranges by construction\n",
    "    num_sites_sr = int(common_idx.size)  \n",
    "\n",
    "    for label, stats in datasets.items():\n",
    "        ubRMSE = stats['ubRMSE']\n",
    "        ubRMSELO = stats['ubRMSELO']\n",
    "        ubRMSEUP = stats['ubRMSEUP']\n",
    "\n",
    "        # Calculate mean and confidence intervals\n",
    "        ubRMSE_mean = np.around(np.nanmean(ubRMSE[common_idx, depth, :, range_idx]), decimals=3)\n",
    "        ubRMSE_CI_LO = np.around(np.nanmean(ubRMSELO[common_idx, depth, :, range_idx]) / np.sqrt(num_sites_sr), decimals=4)\n",
    "        ubRMSE_CI_UP = np.around(np.nanmean(ubRMSEUP[common_idx, depth, :, range_idx]) / np.sqrt(num_sites_sr), decimals=4)\n",
    "\n",
    "        means.append(ubRMSE_mean)\n",
    "        ci_lows.append(ubRMSE_CI_LO)\n",
    "        ci_ups.append(ubRMSE_CI_UP)\n",
    "\n",
    "    # Plot bar chart with error bars\n",
    "    # Combine the absolute values of confidence intervals for yerr\n",
    "    yerr = np.array([np.abs(ci_lows), np.abs(ci_ups)])\n",
    "    axs[range_idx].bar(x, means, yerr=yerr, capsize=5, width=bar_width, color=[plt.cm.tab10(i % 10) for i in range(len(means))], label=expt_labels)\n",
    "    axs[range_idx].set_title(f'{range_name} (n = {num_sites_sr})', fontsize=title_fontsize)\n",
    "    axs[range_idx].set_xticks(x)\n",
    "    axs[range_idx].set_xticklabels(expt_labels, fontsize=y_tick_label_fontsize)\n",
    "    axs[range_idx].set_ylabel('ubRMSE Mean (m3/m3)', fontsize=label_fontsize)\n",
    "    axs[range_idx].set_ylim(0.02, 0.05)\n",
    "    axs[range_idx].grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m_rs_file = '../test_data/M21C_land_sweeper/Evaluation/InSitu/output/LS_DAv8_M36_SCAN_SM_1d_c1234smv_25yr_raw_timeseries.mat'\n",
    "mat_contents = sio.loadmat(m_rs_file)\n",
    "\n",
    "LDAS_sm_DA = mat_contents.get('LDAS_sm_org')\n",
    "\n",
    "# Ensure required variables are available\n",
    "assert 'INSITU_sm' in globals(), \"INSITU_sm not loaded\"\n",
    "assert 'LDAS_sm_OL' in globals(), \"LDAS_sm_org not loaded\"\n",
    "assert 'timestamps_arr' in globals(), \"timestamps_arr not available\"\n",
    "assert 'top_balanced_indices_depth0' in globals(), \"top_balanced_indices_depth0 not available\"\n",
    "assert 'stats_OL' in globals(), \"stats_OL not loaded\"\n",
    "assert 'stats_DA' in globals(), \"stats_DA not loaded\"\n",
    "\n",
    "# Loop through each site in top_balanced_indices_depth0\n",
    "for site_idx in top_balanced_indices_depth0:\n",
    "    # Extract observed and modeled soil moisture for the site\n",
    "    observed_sm = INSITU_sm[:, 0, site_idx]  # Observed soil moisture (depth 0)\n",
    "    modeled_sm_OL = LDAS_sm_OL[:, 0, site_idx]  # Modeled soil moisture (depth 0)\n",
    "    modeled_sm_DA = LDAS_sm_DA[:, 0, site_idx]  # Modeled soil moisture (depth 0)\n",
    "\n",
    "    # Extract latitude and longitude\n",
    "    lat = mat['INSITU_lat'][site_idx, 0]\n",
    "    lon = mat['INSITU_lon'][site_idx, 0]\n",
    "\n",
    "    # Extract R, anomR, and ubRMSE values for the site\n",
    "    R_value = stats_OL['R'][site_idx, 0, 0, 0]  # R for the first range\n",
    "    anomR_value = stats_OL['anomR'][site_idx, 0, 0, 0]  # anomR for the first range\n",
    "    ubRMSE_value = stats_OL['ubRMSE'][site_idx, 0, 0, 0]  # ubRMSE for the first range\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(timestamps_arr, observed_sm, label='Obs SM', alpha=0.7, linewidth=1)\n",
    "    plt.plot(timestamps_arr, modeled_sm_OL, label='OL SM', alpha=0.7, linewidth=1)\n",
    "    plt.plot(timestamps_arr, modeled_sm_DA, ':', label='DA SM', alpha=0.7, linewidth=1)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Soil Moisture (mÂ³/mÂ³)')\n",
    "    plt.title(f\"Site {site_idx} (Lat: {lat:.2f}, Lon: {lon:.2f})\\n\"\n",
    "              f\"R: {R_value:.2f}, anomR: {anomR_value:.2f}, ubRMSE: {ubRMSE_value:.3f}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.ndimage import uniform_filter1d\n",
    "\n",
    "# Loop through each site in top_balanced_indices_depth0\n",
    "for site_idx in top_balanced_indices_depth0:\n",
    "    # Extract observed and modeled soil moisture for the site\n",
    "    observed_sm = INSITU_sm[:, 0, site_idx]  # Observed soil moisture (depth 0)\n",
    "    modeled_sm_OL = LDAS_sm_OL[:, 0, site_idx]  # Modeled soil moisture (depth 0)\n",
    "    modeled_sm_DA = LDAS_sm_DA[:, 0, site_idx]  # Modeled soil moisture (depth 0)\n",
    "\n",
    "    # Apply a 28-day smoother (window size = 28) without interpolating NaN values\n",
    "    # Replace NaN values with the mean of the non-NaN values for smoothing\n",
    "    observed_sm_filled = np.nan_to_num(observed_sm, nan=np.nanmean(observed_sm))\n",
    "\n",
    "    # Apply the uniform filter\n",
    "    observed_sm_smooth = uniform_filter1d(observed_sm_filled, size=28, mode='nearest', origin=0)\n",
    "    modeled_sm_OL_smooth = uniform_filter1d(modeled_sm_OL, size=28, mode='nearest', origin=0)\n",
    "    modeled_sm_DA_smooth = uniform_filter1d(modeled_sm_DA, size=28, mode='nearest', origin=0)\n",
    "\n",
    "    # Extract latitude and longitude\n",
    "    lat = mat['INSITU_lat'][site_idx, 0]\n",
    "    lon = mat['INSITU_lon'][site_idx, 0]\n",
    "\n",
    "    # Extract R, anomR, and ubRMSE values for the site\n",
    "    R_value = stats_OL['R'][site_idx, 0, 0, 0]  # R for the first range\n",
    "    anomR_value = stats_OL['anomR'][site_idx, 0, 0, 0]  # anomR for the first range\n",
    "    ubRMSE_value = stats_OL['ubRMSE'][site_idx, 0, 0, 0]  # ubRMSE for the first range\n",
    "\n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    # plt.plot(timestamps_arr, observed_sm_smooth, label='Obs SM (28-day smooth)', alpha=0.7, linewidth=1)\n",
    "    plt.plot(timestamps_arr, modeled_sm_OL_smooth, label='OL SM (28-day smooth)', alpha=0.7, linewidth=1)\n",
    "    plt.plot(timestamps_arr, modeled_sm_DA_smooth, ':', label='DA SM (28-day smooth)', alpha=0.7, linewidth=1)\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Soil Moisture (mÂ³/mÂ³)')\n",
    "    plt.title(f\"Site {site_idx} (Lat: {lat:.2f}, Lon: {lon:.2f})\\n\"\n",
    "              f\"R: {R_value:.2f}, anomR: {anomR_value:.2f}, ubRMSE: {ubRMSE_value:.3f}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%Y-%m-%d'))\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "# Ensure required variables are available\n",
    "assert 'timestamps_arr' in globals(), \"timestamps_arr not available\"\n",
    "assert 'LDAS_sm_OL' in globals(), \"LDAS_sm_OL not loaded\"\n",
    "assert 'LDAS_sm_DA' in globals(), \"LDAS_sm_DA not loaded\"\n",
    "\n",
    "# Convert timestamps to a DataFrame for easier grouping by month\n",
    "timestamps_df = DataFrame({'timestamp': timestamps_arr})\n",
    "timestamps_df['year_month'] = timestamps_df['timestamp'].dt.to_period('M')\n",
    "\n",
    "# Prepare storage for monthly mean anomalies\n",
    "monthly_anom_OL = []\n",
    "monthly_anom_DA = []\n",
    "\n",
    "# Loop through each site and depth\n",
    "for depth in range(LDAS_sm_OL.shape[1]):\n",
    "    for site in range(LDAS_sm_OL.shape[2]):\n",
    "        # Extract time series for the current depth and site\n",
    "        ts_OL = LDAS_sm_OL[:, depth, site]\n",
    "        ts_DA = LDAS_sm_DA[:, depth, site]\n",
    "\n",
    "        # Combine with timestamps for grouping\n",
    "        df_OL = DataFrame({'value': ts_OL, 'year_month': timestamps_df['year_month']})\n",
    "        df_DA = DataFrame({'value': ts_DA, 'year_month': timestamps_df['year_month']})\n",
    "\n",
    "        # Calculate monthly means\n",
    "        monthly_mean_OL = df_OL.groupby('year_month')['value'].mean()\n",
    "        monthly_mean_DA = df_DA.groupby('year_month')['value'].mean()\n",
    "\n",
    "        # Calculate anomalies (subtracting the overall mean for each site and depth)\n",
    "        overall_mean_OL = monthly_mean_OL.mean()\n",
    "        overall_mean_DA = monthly_mean_DA.mean()\n",
    "        monthly_anom_OL.append(monthly_mean_OL - overall_mean_OL)\n",
    "        monthly_anom_DA.append(monthly_mean_DA - overall_mean_DA)\n",
    "\n",
    "# Convert lists of anomalies to DataFrames for easier plotting\n",
    "monthly_anom_OL_df = DataFrame(monthly_anom_OL).T\n",
    "monthly_anom_DA_df = DataFrame(monthly_anom_DA).T\n",
    "monthly_anom_OL_df.index = monthly_mean_OL.index\n",
    "monthly_anom_DA_df.index = monthly_mean_DA.index\n",
    "\n",
    "# Ensure the DataFrames have proper column indices\n",
    "monthly_anom_OL_df.columns = range(len(monthly_anom_OL))\n",
    "monthly_anom_DA_df.columns = range(len(monthly_anom_DA))\n",
    "\n",
    "# Example: Plot monthly mean anomalies for a specific site and depth\n",
    "depth_idx = 0  # Change as needed\n",
    "\n",
    "# Loop through each site in top_balanced_indices_depth0\n",
    "for site_idx in top_balanced_indices_depth0:\n",
    "\n",
    "    plt.figure(figsize=(24, 6))\n",
    "    plt.bar(monthly_anom_OL_df.index.to_timestamp(), monthly_anom_OL_df[site_idx], label='OL Monthly Anomaly', alpha=0.7, width=20, align='center')\n",
    "    plt.bar(monthly_anom_DA_df.index.to_timestamp(), monthly_anom_DA_df[site_idx], label='DA Monthly Anomaly', alpha=0.7, width=20, align='center')\n",
    "    plt.xlabel('Time')\n",
    "    plt.ylabel('Monthly Anomaly (mÂ³/mÂ³)')\n",
    "    plt.title(f\"Monthly Mean Anomalies for Site {site_idx}, Depth {depth_idx}\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    ax = plt.gca()\n",
    "    ax.xaxis.set_major_locator(AutoDateLocator())\n",
    "    ax.xaxis.set_major_formatter(DateFormatter('%Y-%m'))\n",
    "    plt.xticks(rotation=30, ha='right')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "xr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
